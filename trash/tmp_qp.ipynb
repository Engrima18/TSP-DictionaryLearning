{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p=0.162, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(137, 96)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = G.get_cycles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7534"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p_edges=0.162, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1, dtype=float)\n",
    "Lu = np.matmul(B2, np.transpose(B2), dtype=float)\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = \"\"\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 30 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "n_sim = 10\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 100)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_type = \"separated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [29:16<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [24:32<00:00,  2.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [22:50<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [23:03<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [23:02<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [23:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [22:50<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [22:50<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [23:01<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [23:09<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dictionary_type = \"separated\"\n",
    "\n",
    "D_true = np.zeros((n, n * s, n_sim))\n",
    "D_true_coll = np.zeros((n, n, s, n_sim))\n",
    "Y_train = np.zeros((n, m_train, n_sim))\n",
    "Y_test = np.zeros((n, m_test, n_sim))\n",
    "epsilon_true = np.zeros(n_sim)\n",
    "c_true = np.zeros(n_sim)\n",
    "X_train = np.zeros((n * s, m_train, n_sim))\n",
    "X_test = np.zeros((n * s, m_test, n_sim))\n",
    "n_search = 3000\n",
    "\n",
    "for sim in range(n_sim):\n",
    "    best_sparsity = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for i in tqdm(range(n_search)):\n",
    "        try:\n",
    "            D_try, h, Y_train_try, Y_test_try, epsilon_try, c_try, X_train_try, X_test_try = create_ground_truth(Lu,\n",
    "                                                                                    Ld,\n",
    "                                                                                    m_train,\n",
    "                                                                                    m_test, \n",
    "                                                                                    s=s, \n",
    "                                                                                    K=k, \n",
    "                                                                                    K0=K0_max, \n",
    "                                                                                    dictionary_type=dictionary_type, \n",
    "                                                                                    sparsity_mode=sparsity_mode)\n",
    "            \n",
    "            max_possible_sparsity, acc = verify_dic(D_try, Y_train_try, X_train_try, K0_max, .7)\n",
    "            if max_possible_sparsity > best_sparsity:\n",
    "                best_sparsity = max_possible_sparsity\n",
    "                best_acc = acc\n",
    "                D_true[:, :, sim] = D_try\n",
    "                Y_train[:, :, sim] = Y_train_try\n",
    "                Y_test[:, :, sim] = Y_test_try\n",
    "                epsilon_true[sim] = epsilon_try\n",
    "                c_true[sim] = c_try\n",
    "                X_train[:, :, sim] = X_train_try\n",
    "                X_test[:, :, sim] = X_test_try\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dictionary creation: {e}\")\n",
    "    if verbose:\n",
    "        print(f\"...Done! # Best Sparsity: {best_sparsity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- find the dimensions of X_train and divide X_train in blocks like D_coll (to find X_p for each dictionary D_p)\n",
    "- check the dimensionality of L_k to understand the multiplication of L_k by X_p (or X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'initialize_dic' on <module 'tsplearn.model_train' from 'c:\\\\Users\\\\engri\\\\Desktop\\\\tesi\\\\TSP-DictionaryLearning\\\\tsplearn\\\\model_train.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdill\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mresults\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mjoint\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mipynb_env.db\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\engri\\anaconda3\\lib\\site-packages\\dill\\session.py:512\u001b[0m, in \u001b[0;36mload_session\u001b[1;34m(filename, main, **kwds)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_session\u001b[39m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, main\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds):\n\u001b[0;32m    511\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_session() has been renamed load_module().\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m)\n\u001b[1;32m--> 512\u001b[0m     load_module(filename, module\u001b[38;5;241m=\u001b[39mmain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\engri\\anaconda3\\lib\\site-packages\\dill\\session.py:494\u001b[0m, in \u001b[0;36mload_module\u001b[1;34m(filename, module, **kwds)\u001b[0m\n\u001b[0;32m    491\u001b[0m         runtime_main \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__runtime__.\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m main\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    492\u001b[0m         sys\u001b[38;5;241m.\u001b[39mmodules[runtime_main] \u001b[38;5;241m=\u001b[39m main\n\u001b[1;32m--> 494\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# if newly opened file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engri\\anaconda3\\lib\\site-packages\\dill\\_dill.py:442\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m): \u001b[38;5;66;03m#NOTE: if settings change, need to update attributes\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(obj)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__module__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_main_module, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    444\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ignore:\n\u001b[0;32m    445\u001b[0m             \u001b[38;5;66;03m# point obj class to main\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engri\\anaconda3\\lib\\site-packages\\dill\\_dill.py:432\u001b[0m, in \u001b[0;36mUnpickler.find_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;66;03m#XXX: special case: NoneType missing\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdill.dill\u001b[39m\u001b[38;5;124m'\u001b[39m: module \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdill._dill\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 432\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStockUnpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Can't get attribute 'initialize_dic' on <module 'tsplearn.model_train' from 'c:\\\\Users\\\\engri\\\\Desktop\\\\tesi\\\\TSP-DictionaryLearning\\\\tsplearn\\\\model_train.py'>"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "dill.load_session(path+'\\\\results\\\\joint\\\\ipynb_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.36323668733983"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLARABEL', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLPK', 'GLPK_MI', 'MOSEK', 'OSQP', 'SCIPY', 'SCS', 'SDPA']\n"
     ]
    }
   ],
   "source": [
    "print(cp.installed_solvers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def compute_vandermonde(L):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B\n",
    "\n",
    "def topological_dictionary_learn_qp(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, \n",
    "         optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            L = Lu + Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermonde(L)\n",
    "            B = cp.Constant(B.real)\n",
    "\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            L = Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermonde(L)\n",
    "            B = cp.Constant(B.real)\n",
    "        elif dictionary_type==\"separated\":\n",
    "\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "            Bu = compute_vandermonde(Lu)\n",
    "            Bd = compute_vandermonde(Ld)\n",
    "            B = cp.hstack([Bu.real, Bd[:, 1:].real])\n",
    "\n",
    "        # Init the the sparse representation \n",
    "        Y = cp.Constant(Y_train)\n",
    "        I = np.eye(n)\n",
    "        I_2 = cp.Constant(np.eye(s*(2*K+1)))\n",
    "        I_s = cp.Constant(np.eye(s))\n",
    "        i_s = cp.Constant(np.ones((s,1)))\n",
    "        X_train = X0\n",
    "\n",
    "        def aux_matrix(Lu,Ld,X_train,K,N):\n",
    "\n",
    "            LL = [np.eye(N)]\n",
    "            LL_tmp = []\n",
    "            for i in range(1, K + 1):\n",
    "                LL.append(la.matrix_power(Lu, i))\n",
    "                LL_tmp.append(la.matrix_power(Ld, i))\n",
    "\n",
    "            LL = LL + LL_tmp\n",
    "            LL = np.array(LL)\n",
    "            P = np.array([LL@X_train[(i*n): ((i+1)*n), :] for i in range(s)])\n",
    "\n",
    "            return P\n",
    "        \n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # QP Step\n",
    "            \n",
    "            # # Define the objective function\n",
    "            # if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "            #     # Init the variables\n",
    "            #     break\n",
    "\n",
    "            # else:\n",
    "            \n",
    "\n",
    "            P = aux_matrix(Lu,Ld,X_train,K,n)\n",
    "\n",
    "            # Init variables and parameters\n",
    "            h = cp.Variable((s*(2*k+1), 1))\n",
    "            l = cp.Constant(np.zeros((s*(2*k+1), 1)))\n",
    "            Q = cp.Constant(np.zeros((s*(2*k+1), s*(2*k+1))))\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(m_train):\n",
    "                    Pij = cp.Constant(P[:,:,i,j].flatten().reshape(-1,1))\n",
    "                    l = l + (Y[i,j]*Pij.T)\n",
    "                    Q = Q + Pij@Pij.T\n",
    "\n",
    "            Q = Q + cp.multiply(lambda_, I_2)\n",
    "\n",
    "            # Check convexity\n",
    "            # assert np.all(np.linalg.eigvals(Q.value) >= 0), f'element {(i,j)} not positive semi-definite'\n",
    "\n",
    "            # Quadratic term\n",
    "            term2 = cp.quad_form(h, Q, assume_PSD = True)\n",
    "            # Linear term\n",
    "            term1 = l@h\n",
    "            term1 = cp.multiply(-2, term1)[0]\n",
    "            \n",
    "            obj = cp.Minimize(term2+term1)\n",
    "\n",
    "            # Define the constraints\n",
    "            cons1 = cp.kron(I_s,B)@h\n",
    "            cons2 = cp.kron(i_s.T,B)@h  #### maybe i_s.T is not changing anything since i_s has only one dimension (while i_s should be a column)\n",
    "            constraints = [cons1 >= 0] + \\\n",
    "                            [cons1 <=c] + \\\n",
    "                            [cons2 >= (c-epsilon)] + \\\n",
    "                            [cons2 <= (c+epsilon)]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            \n",
    "            def split_coeffs(h,s ,k):\n",
    "                h = h.value.flatten()\n",
    "                hH = h[np.arange(0, (s*(2*k+1)), (2*k+1))].reshape((s,1))\n",
    "                hS = h[np.hstack([[i,i+1] for i in range(1, (s*(2*k+1)), (2*k+1))])].reshape((s,k))\n",
    "                hI = h[np.hstack([[i,i+1] for i in range((k+1), (s*(2*k+1)), (2*k+1))])].reshape((s,k))\n",
    "                return h, hH, hS, hI\n",
    "                            \n",
    "            # Update the dictionary\n",
    "            D_coll = []\n",
    "            h, hH, hS, hI = split_coeffs(h, s, K)\n",
    "            # h = h.value.flatten()\n",
    "            # hH = h[:s].reshape((s,1))\n",
    "            # hS = h[s:((s*K)+s)].reshape((s,k))\n",
    "            # hI = h[((s*K)+s):].reshape((s,k))\n",
    "\n",
    "            for i in range(0,s):\n",
    "                # tmp =  np.zeros((n, n))\n",
    "                # for j in range(0,K):\n",
    "                #     tmp += ((Luk[j, :, :])*hS[(i*(s-1)+j)]) + ((Ldk[j, :, :])*hI[(i*(s-1)+j)])\n",
    "                # tmp += (I*hH[i])\n",
    "                hu = hS[i].reshape(k,1,1)\n",
    "                hd = hI[i].reshape(k,1,1)\n",
    "                hid = hH[i]\n",
    "                tmp = np.sum(hu*Luk + hd*Ldk, axis=0) + hid*I\n",
    "                D_coll.append(tmp)\n",
    "\n",
    "            D = np.hstack(tuple(D_coll))\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalize\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # print(\"-\"*100)\n",
    "            # print(f'Iter: {iter_}')\n",
    "            # print()\n",
    "            # print(f'hH.shape: {hH.shape}')\n",
    "            # print(f'hH: {hH}')\n",
    "            # print()\n",
    "            # print(f'hS.shape: {hS.shape}')\n",
    "            # print(f'hS: {hS}')\n",
    "            # print()\n",
    "            # print(f'hI.shape: {hI.shape}')\n",
    "            # print(f'hI: {hI}')\n",
    "            # print()\n",
    "            # print(f'test error: {error_test_norm}')\n",
    "            # print()\n",
    "            # print(\"-\"*100)\n",
    "\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI, hS, hH])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, K)\n",
    "        elif dictionary_type==\"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "\n",
    "        # Init the dictionary and the sparse representation \n",
    "        D_coll = [cp.Constant(D0[:,(n*i):(n*(i+1))]) for i in range(s)]\n",
    "        Y = cp.Constant(Y_train)\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            # Init constants and parameters\n",
    "            D_coll = [cp.Constant(np.zeros((n, n))) for i in range(s)] \n",
    "            Dsum = cp.Constant(np.zeros((n, n)))\n",
    "            X = cp.Constant(X_train)\n",
    "            I = cp.Constant(np.eye(n))\n",
    "            \n",
    "            # Define the objective function\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                # Init the variables\n",
    "                h = cp.Variable((s, K))\n",
    "                hI = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += (cp.Constant(Lk[j, :, :]) * h[i,j])\n",
    "                    tmp += (I*hI[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "            else:\n",
    "                # Init the variables\n",
    "                hI = cp.Variable((s, K))\n",
    "                hS = cp.Variable((s, K))\n",
    "                hH = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += ((cp.Constant(Luk[j, :, :])*hS[i,j]) + (cp.Constant(Ldk[j, :, :])*hI[i,j]))\n",
    "                    tmp += (I*hH[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                \n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            # Define the constraints\n",
    "            constraints = [D_coll[i] >> 0 for i in range(s)] + \\\n",
    "                            [(cp.multiply(c, I) - D_coll[i]) >> 0 for i in range(s)] + \\\n",
    "                            [(Dsum - cp.multiply((c - epsilon), I)) >> 0, (cp.multiply((c + epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalization\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            \n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else [hS.value, hI.value, hH.value]\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            # print(\"-\"*100)\n",
    "            # print(f'Iter: {iter_}')\n",
    "            # print()\n",
    "            # print(f'hH.shape: {hH.shape}')\n",
    "            # print(f'hH: {hH.value}')\n",
    "            # print()\n",
    "            # print(f'hS.shape: {hS.shape}')\n",
    "            # print(f'hS: {hS.value}')\n",
    "            # print()\n",
    "            # print(f'hI.shape: {hI.shape}')\n",
    "            # print(f'hI: {hI.value}')\n",
    "            # print()\n",
    "            # print(f'test error: {error_test_norm}')\n",
    "            # print()\n",
    "            # print(\"-\"*100)\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [12:17, 738.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 5 Testing Separated Hodge Laplacian... Done! Test Error: 0.027142746245361005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [24:52, 747.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 9 Testing Separated Hodge Laplacian... Done! Test Error: 0.008353883504784776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [44:57, 956.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 13 Testing Separated Hodge Laplacian... Done! Test Error: 0.005569306278951018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:22:09, 1459.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 17 Testing Separated Hodge Laplacian... Done! Test Error: 0.0020380647954793213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:24:11, 1262.76s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[139], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization Failed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dict_types\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     dict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m0\u001b[39m][sim,k0_index], dict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m1\u001b[39m][sim,k0_index], _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopological_dictionary_learn_qp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_sim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sparsity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Testing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... Done! Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m1\u001b[39m][sim,k0_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[138], line 142\u001b[0m, in \u001b[0;36mtopological_dictionary_learn_qp\u001b[1;34m(Y_train, Y_test, K, n, s, D0, X0, Lu, Ld, dictionary_type, c, epsilon, K0, lambda_, max_iter, patience, tol, verbose)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(m_train):\n\u001b[0;32m    141\u001b[0m         Pij \u001b[38;5;241m=\u001b[39m cp\u001b[38;5;241m.\u001b[39mConstant(P[:,:,i,j]\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 142\u001b[0m         l \u001b[38;5;241m=\u001b[39m l \u001b[38;5;241m+\u001b[39m (\u001b[43mY\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mPij\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m)\n\u001b[0;32m    143\u001b[0m         Q \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m+\u001b[39m Pij\u001b[38;5;129m@Pij\u001b[39m\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    145\u001b[0m Q \u001b[38;5;241m=\u001b[39m Q \u001b[38;5;241m+\u001b[39m cp\u001b[38;5;241m.\u001b[39mmultiply(lambda_, I_2)\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:50\u001b[0m, in \u001b[0;36m_cast_other.<locals>.cast_op\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A wrapped binary operator that can handle non-Expression arguments.\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     49\u001b[0m other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcast_to_const(other)\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:601\u001b[0m, in \u001b[0;36mExpression.__mul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Expression : The product of two expressions.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m () \u001b[38;5;129;01mor\u001b[39;00m other\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n\u001b[0;32m    597\u001b[0m     \u001b[38;5;66;03m# Use one argument to apply a scaling to the remaining argument.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m     \u001b[38;5;66;03m# We accomplish this with elementwise multiplication, which\u001b[39;00m\n\u001b[0;32m    599\u001b[0m     \u001b[38;5;66;03m# casts the scalar argument to match the size of the remaining\u001b[39;00m\n\u001b[0;32m    600\u001b[0m     \u001b[38;5;66;03m# argument.\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcvxtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melmul_expr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m other\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    603\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_scalar() \u001b[38;5;129;01mor\u001b[39;00m other\u001b[38;5;241m.\u001b[39mis_scalar()):\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;66;03m# If matmul was intended, this gives a dimension mismatch. We\u001b[39;00m\n\u001b[0;32m    605\u001b[0m     \u001b[38;5;66;03m# interpret the ``is_scalar`` results as implying that the user\u001b[39;00m\n\u001b[0;32m    606\u001b[0m     \u001b[38;5;66;03m# simply wants to apply a scaling.\u001b[39;00m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cvxtypes\u001b[38;5;241m.\u001b[39melmul_expr()(\u001b[38;5;28mself\u001b[39m, other)\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\binary_operators.py:243\u001b[0m, in \u001b[0;36mmultiply.__init__\u001b[1;34m(self, lh_expr, rh_expr)\u001b[0m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lh_expr, rh_expr) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 243\u001b[0m     lh_expr, rh_expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlh_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrh_expr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28msuper\u001b[39m(multiply, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(lh_expr, rh_expr)\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\expressions\\expression.py:544\u001b[0m, in \u001b[0;36mExpression.broadcast\u001b[1;34m(lh_expr, rh_expr)\u001b[0m\n\u001b[0;32m    542\u001b[0m rh_expr \u001b[38;5;241m=\u001b[39m Expression\u001b[38;5;241m.\u001b[39mcast_to_const(rh_expr)\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lh_expr\u001b[38;5;241m.\u001b[39mis_scalar() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rh_expr\u001b[38;5;241m.\u001b[39mis_scalar():\n\u001b[1;32m--> 544\u001b[0m     lh_expr \u001b[38;5;241m=\u001b[39m \u001b[43mcvxtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpromote\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlh_expr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrh_expr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m rh_expr\u001b[38;5;241m.\u001b[39mis_scalar() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lh_expr\u001b[38;5;241m.\u001b[39mis_scalar():\n\u001b[0;32m    546\u001b[0m     rh_expr \u001b[38;5;241m=\u001b[39m cvxtypes\u001b[38;5;241m.\u001b[39mpromote()(rh_expr, lh_expr\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py:47\u001b[0m, in \u001b[0;36mpromote\u001b[1;34m(expr, shape)\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m expr\u001b[38;5;241m.\u001b[39mis_scalar():\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOnly scalars may be promoted.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPromote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\affine\\promote.py:65\u001b[0m, in \u001b[0;36mPromote.__init__\u001b[1;34m(self, expr, shape)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, expr, shape: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpromoted_shape \u001b[38;5;241m=\u001b[39m shape\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPromote\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexpr\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\cvxpy\\atoms\\atom.py:41\u001b[0m, in \u001b[0;36mAtom.__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     38\u001b[0m _allow_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# args are the expressions passed into the Atom constructor.\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m lu\u001b[38;5;241m.\u001b[39mget_id()\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# Throws error if args is empty.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "min_error_fou_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "dict_errors = {\n",
    "    \"sep\": (min_error_sep_train,min_error_sep_test)\n",
    "    }\n",
    "\n",
    "\n",
    "dict_types = {\n",
    "    \"sep\": (\"Separated Hodge Laplacian\",\"separated\")\n",
    "    }\n",
    "\n",
    "for sim in range(7,8):\n",
    "    c = c_true[sim]  \n",
    "    epsilon = epsilon_true[sim] \n",
    "    for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "        discard = 1\n",
    "        while discard == 1:\n",
    "            \n",
    "            try:\n",
    "                D0, X0, discard = initialize_dic(Lu, Ld, s, k, Y_train[:, :, sim], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "            except:\n",
    "                print(\"Initialization Failed!\")\n",
    "\n",
    "        for d in dict_types.items():\n",
    "            # try:\n",
    "            dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index], _, _, _ = topological_dictionary_learn_qp(Y_train[:,:,sim], Y_test[:,:,sim],\n",
    "                                                                                                                        k, n, s, D0, X0, Lu, Ld, d[1][1],\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)\n",
    "            if verbose:\n",
    "                print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Done! Test Error: {dict_errors[d[0]][1][sim,k0_index]}\")\n",
    "            \n",
    "\n",
    "            # except:\n",
    "            #     print(f'Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Diverged!')\n",
    "            #     try:\n",
    "            #         dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index] = (dict_errors[d[0]][0][sim-1,k0_index]\n",
    "            #                                                                               , dict_errors[d[0]][1][sim-1,k0_index])\n",
    "            #     except:\n",
    "            #         dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index] = (dict_errors[d[0]][0][sim+1,k0_index]\n",
    "            #                                                                               , dict_errors[d[0]][1][sim+1,k0_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 1\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[1.15372766]\n",
      " [1.15373349]\n",
      " [1.15373651]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[ 7.27607721e-06 -9.07298979e-07]\n",
      " [-4.31118761e-08  4.28954552e-08]\n",
      " [-7.25088570e-06  8.65884701e-07]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[ 3.54178653e-06 -2.96201087e-07]\n",
      " [-1.75885822e-06  1.58447083e-07]\n",
      " [-1.79305934e-06  1.38381558e-07]]\n",
      "\n",
      "test error: 0.3914354995113346\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 2\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[0.99830229]\n",
      " [2.14122785]\n",
      " [0.32426987]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[ 0.13532262 -0.01456644]\n",
      " [-0.24156463  0.01609855]\n",
      " [ 0.10528705 -0.00144463]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[ 0.08978463  0.00303407]\n",
      " [-0.30094174  0.01770366]\n",
      " [ 0.19773727 -0.00396834]]\n",
      "\n",
      "test error: 0.10501045947315957\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 3\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 2.95304371e+00]\n",
      " [ 5.08468679e-01]\n",
      " [-2.01993894e-11]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.3084107   0.00668534]\n",
      " [ 0.17967704 -0.00867683]\n",
      " [ 0.12860689  0.00200427]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.40015842  0.02709712]\n",
      " [ 0.17646791 -0.00925044]\n",
      " [ 0.21737701 -0.00159624]]\n",
      "\n",
      "test error: 0.06277691317615688\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 4\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.34698143e+00]\n",
      " [ 1.34363201e-01]\n",
      " [-7.03288232e-13]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.32913424  0.00190675]\n",
      " [ 0.24867697 -0.01072967]\n",
      " [ 0.07395137  0.00934814]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.46980096  0.0273623 ]\n",
      " [ 0.27802691 -0.01346705]\n",
      " [ 0.15306381  0.00466834]]\n",
      "\n",
      "test error: 0.05384792722615046\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 5\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.46914239e+00]\n",
      " [ 5.24653921e-13]\n",
      " [-1.63688977e-12]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.2786357  -0.00674826]\n",
      " [ 0.23136939 -0.00623537]\n",
      " [ 0.04455634  0.01321464]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.41644613  0.01306024]\n",
      " [ 0.29498231 -0.01204575]\n",
      " [ 0.10299412  0.00951081]]\n",
      "\n",
      "test error: 0.04771217340006417\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 6\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.51326864e+00]\n",
      " [ 1.45573507e-13]\n",
      " [-5.28179415e-13]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.21710347 -0.01535698]\n",
      " [ 0.1595131   0.00349087]\n",
      " [ 0.03325556  0.01470922]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.37440304  0.0083442 ]\n",
      " [ 0.25055008 -0.00662683]\n",
      " [ 0.06873111  0.01277268]]\n",
      "\n",
      "test error: 0.04539849126884591\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 7\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.65996984e+00]\n",
      " [-7.01229449e-13]\n",
      " [-9.97354024e-13]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.22248195 -0.01613779]\n",
      " [ 0.10847918  0.01012833]\n",
      " [ 0.02682139  0.01552144]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.37473968  0.00754797]\n",
      " [ 0.19599229 -0.00040784]\n",
      " [ 0.04651997  0.01484992]]\n",
      "\n",
      "test error: 0.04387535474569733\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 8\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.73470416e+00]\n",
      " [-2.71097827e-12]\n",
      " [-7.26463739e-12]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.21611113 -0.01784939]\n",
      " [ 0.07391422  0.0147883 ]\n",
      " [ 0.0230569   0.01598226]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.35933861  0.00597708]\n",
      " [ 0.15890381  0.0036283 ]\n",
      " [ 0.03230429  0.01617973]]\n",
      "\n",
      "test error: 0.04442297832298514\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 9\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.75408450e+00]\n",
      " [-2.29303964e-11]\n",
      " [-3.27138570e-12]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.22566048 -0.01416098]\n",
      " [ 0.05452525  0.01717459]\n",
      " [ 0.0219884   0.01596566]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.33482482  0.00403424]\n",
      " [ 0.13158952  0.00658864]\n",
      " [ 0.02319125  0.01698933]]\n",
      "\n",
      "test error: 0.044187054562993044\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 10\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[3.69923645e+00]\n",
      " [1.44686713e-12]\n",
      " [1.54302510e-11]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.20753134 -0.01273277]\n",
      " [ 0.04286519  0.0184182 ]\n",
      " [ 0.02137066  0.01587008]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.30001521  0.0017366 ]\n",
      " [ 0.12214734  0.00739735]\n",
      " [ 0.01966729  0.01715086]]\n",
      "\n",
      "test error: 0.04377124075675823\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 11\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[3.61095493e+00]\n",
      " [4.38789449e-12]\n",
      " [2.62554039e-11]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.17115673 -0.01341342]\n",
      " [ 0.03812477  0.01875827]\n",
      " [ 0.02017903  0.01587304]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.24930184 -0.00156388]\n",
      " [ 0.11252417  0.00822445]\n",
      " [ 0.01709782  0.0172361 ]]\n",
      "\n",
      "test error: 0.04414795557134316\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 12\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[3.59657035e+00]\n",
      " [2.18148673e-11]\n",
      " [3.92668841e-11]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.17267986 -0.00816008]\n",
      " [ 0.03520649  0.01872805]\n",
      " [ 0.01748766  0.016019  ]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.23180437 -0.00279264]\n",
      " [ 0.10589522  0.00878047]\n",
      " [ 0.01291353  0.01748923]]\n",
      "\n",
      "test error: 0.04620947621473377\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 13\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[ 3.53829928e+00]\n",
      " [-5.97949982e-12]\n",
      " [ 2.11327541e-11]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.10435056 -0.0068227 ]\n",
      " [ 0.035189    0.0183712 ]\n",
      " [ 0.01583191  0.01607638]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.18896911 -0.00567165]\n",
      " [ 0.09759389  0.0094977 ]\n",
      " [ 0.00975602  0.01762943]]\n",
      "\n",
      "test error: 0.04593353199816448\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 14\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[3.47484237e+00]\n",
      " [1.81123304e-12]\n",
      " [3.61654396e-12]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[-0.03417668 -0.00558536]\n",
      " [ 0.03318962  0.01837962]\n",
      " [ 0.0142124   0.01609344]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.12792056 -0.00988449]\n",
      " [ 0.09009426  0.01013587]\n",
      " [ 0.00610488  0.01782566]]\n",
      "\n",
      "test error: 0.04511071282476882\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [01:06, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Iter: 15\n",
      "\n",
      "hH.shape: (3, 1)\n",
      "hH: [[3.46151239e+00]\n",
      " [6.91194137e-11]\n",
      " [3.18301130e-11]]\n",
      "\n",
      "hS.shape: (3, 2)\n",
      "hS: [[ 0.09181927 -0.01792377]\n",
      " [ 0.03262248  0.0181425 ]\n",
      " [ 0.01449794  0.01575414]]\n",
      "\n",
      "hI.shape: (3, 2)\n",
      "hI: [[-0.09327719 -0.01240216]\n",
      " [ 0.08410956  0.0106526 ]\n",
      " [ 0.00285411  0.01800001]]\n",
      "\n",
      "test error: 0.04506777519476743\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Simulation: 10/10 Sparsity: 5 Testing Separated Hodge Laplacian... Done! Test Error: 0.04377124075675823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_error_fou_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "dict_errors = {\n",
    "    \"sep\": (min_error_sep_train,min_error_sep_test)\n",
    "    }\n",
    "\n",
    "\n",
    "dict_types = {\n",
    "    \"sep\": (\"Separated Hodge Laplacian\",\"separated\")\n",
    "    }\n",
    "\n",
    "for sim in range(7,8):\n",
    "    c = c_true[sim]  \n",
    "    epsilon = epsilon_true[sim] \n",
    "    for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "        discard = 1\n",
    "        while discard == 1:\n",
    "            \n",
    "            try:\n",
    "                D0, X0, discard = initialize_dic(Lu, Ld, s, k, Y_train[:, :, sim], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "            except:\n",
    "                print(\"Initialization Failed!\")\n",
    "\n",
    "        for d in dict_types.items():\n",
    "            # try:\n",
    "            dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index], _, _, _ = topological_dictionary_learn(Y_train[:,:,sim], Y_test[:,:,sim],\n",
    "                                                                                                                        k, n, s, D0, X0, Lu, Ld, d[1][1],\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)\n",
    "            if verbose:\n",
    "                print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Done! Test Error: {dict_errors[d[0]][1][sim,k0_index]}\")\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aux_matrix(Lu,Ld,X_train,K,N):\n",
    "\n",
    "    LL = [np.eye(N)]\n",
    "    LL_tmp = []\n",
    "    for i in range(1, K + 1):\n",
    "        LL.append(la.matrix_power(Lu, i))\n",
    "        LL_tmp.append(la.matrix_power(Ld, i))\n",
    "\n",
    "    LL = LL + LL_tmp\n",
    "    LL = np.array(LL)\n",
    "    P = np.array([LL@X_train[(i*n): ((i+1)*n), :] for i in range(s)])\n",
    "\n",
    "    return P, LL\n",
    "\n",
    "P, LL = aux_matrix(Lu,Ld,X_train[:,:,7],k,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5, 100, 150)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 150)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:,:,7].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mall(LL[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39meye(n))\n\u001b[0;32m      3\u001b[0m I_X3 \u001b[38;5;241m=\u001b[39m LL[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m@\u001b[39m X_train[:,:,\u001b[38;5;241m7\u001b[39m][\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mn:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn,:]\n\u001b[0;32m      4\u001b[0m Lu1_X3 \u001b[38;5;241m=\u001b[39m LL[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m@\u001b[39m X_train[:,:,\u001b[38;5;241m7\u001b[39m][\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mn:\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mn,:]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "np.all(LL[0] == np.eye(n))\n",
    "\n",
    "I_X3 = LL[0] @ X_train[:,:,7][2*n:3*n,:]\n",
    "Lu1_X3 = LL[1] @ X_train[:,:,7][2*n:3*n,:]\n",
    "Lu1_X3[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 5)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pij_not = P[:,:,4,0]\n",
    "Pij_not.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  5, 10])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hH = h[np.arange(0, (s*(2*k+1)), (2*k+1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  6,  7, 11, 12])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hS = h[np.hstack([[i,i+1] for i in range(1, (s*(2*k+1)), (2*k+1))])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  4,  8,  9, 13, 14])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hI = h[np.hstack([[i,i+1] for i in range((k+1), (s*(2*k+1)), (2*k+1))])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.6917315 ],\n",
       "       [ 2.76692599],\n",
       "       [16.60155596],\n",
       "       [ 1.383463  ],\n",
       "       [ 8.99250948],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [ 0.        ],\n",
       "       [-0.23084475],\n",
       "       [ 0.        ],\n",
       "       [-0.61687947],\n",
       "       [-4.39417932],\n",
       "       [ 0.61687947],\n",
       "       [ 4.39417932]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Pij = P[:,:,4,0].flatten().reshape(-1,1)\n",
    "Pij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 15)\n"
     ]
    }
   ],
   "source": [
    "P_reshaped = P.reshape(s, (2*k+1), -1)\n",
    "\n",
    "result = np.tensordot(P, P, axes=([2, 3], [2, 3])).reshape(s*(2*k+1), s*(2*k+1))\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_reshaped = P.reshape(s*(2*k+1), n*m_train)  # Reshape P to a 2D array where each column is a flattened P[:,:,i,j]\n",
    "\n",
    "# Step 2: Compute the sum of outer products\n",
    "Q_aiut = np.einsum('ji,ik->jk', P_reshaped, P_reshaped.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_reshaped = P.reshape(s, (2*k+1), n*m_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 15)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q_aiut.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False,  True, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q==Q_aiut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.zeros((s*(2*k+1), 1))\n",
    "Q = np.zeros((s*(2*k+1), s*(2*k+1)))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(m_train):\n",
    "        Pij = P[:,:,i,j].flatten().reshape(-1,1)\n",
    "        Q = Q + Pij@Pij.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 15000)\n"
     ]
    }
   ],
   "source": [
    "P_flattened = P.reshape(s*(2*k+1), m_train*n)\n",
    "\n",
    "# Step 2: Compute the outer product for each vector with itself and sum over all M*N\n",
    "# Using einsum for vectorized outer products and summation\n",
    "result = np.einsum('ij,ik->jk', P_flattened, P_flattened)\n",
    "\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(Q.value==result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
