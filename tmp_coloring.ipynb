{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "prob_T=0.5\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p_edges=0.162, p_triangles=prob_T, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "T = int(np.ceil(nu*(1-prob_T)))\n",
    "\n",
    "# Laplacians\n",
    "Lu, Ld, L = G.get_laplacians(sub_size=100)\n",
    "Lu_full = G.get_laplacians(sub_size=100, full=True)\n",
    "M =  L.shape[0]\n",
    "\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type=\"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "P = 3 # Number of Kernels (Sub-dictionaries)\n",
    "J = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(M*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "n_search = 1500\n",
    "n_sim = 10\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 100 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:09<00:00, 11.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:10<00:00, 11.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:04<00:00, 12.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:07<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:08<00:00, 11.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:08<00:00, 11.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:08<00:00, 11.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:07<00:00, 11.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:08<00:00, 11.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:04<00:00, 12.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D_true, Y_train, Y_test, X_train, X_test, epsilon_true, c_true = generate_data(dictionary_type=dictionary_type,\n",
    "                                                                                Lu=Lu,\n",
    "                                                                                Ld=Ld,\n",
    "                                                                                M=M,\n",
    "                                                                                P=P,\n",
    "                                                                                J=J,\n",
    "                                                                                n_sim=n_sim,\n",
    "                                                                                m_test=m_test,\n",
    "                                                                                m_train=m_train,\n",
    "                                                                                K0_max=K0_max,\n",
    "                                                                                n_search=n_search,\n",
    "                                                                                sparsity_mode=sparsity_mode,\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "\n",
    "name = f'top_data_T{int(prob_T*100)}'\n",
    "save_var = {\"D_true\" : D_true,\n",
    "            \"Y_train\" : Y_train,\n",
    "            \"Y_test\" : Y_test,\n",
    "            \"X_train\" : X_train,\n",
    "            \"X_test\" : X_test,\n",
    "            \"epsilon_true\" : epsilon_true,\n",
    "            \"c_true\" : c_true}\n",
    "\n",
    "PATH = os.getcwd()\n",
    "DIR_PATH = f'{PATH}\\\\synthetic_data'\n",
    "FILENAME = f'{DIR_PATH}\\\\{name}.pkl'\n",
    "\n",
    "if not os.path.exists(DIR_PATH):\n",
    "    os.makedirs(DIR_PATH)\n",
    "    \n",
    "with open(FILENAME, 'wb') as f: \n",
    "    pickle.dump(save_var, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "name = f'top_data_T{int(prob_T*100)}'\n",
    "PATH = os.getcwd()\n",
    "DIR_PATH = f'{PATH}\\\\synthetic_data'\n",
    "FILENAME = f'{DIR_PATH}\\\\{name}.pkl'\n",
    "\n",
    "with open(FILENAME, 'rb') as f: \n",
    "    load_data = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "D_true = load_data['D_true']\n",
    "Y_train = load_data['Y_train']\n",
    "Y_test = load_data['Y_test']\n",
    "X_train = load_data['X_train']\n",
    "X_test = load_data['X_test']\n",
    "epsilon_true =  load_data['epsilon_true']\n",
    "c_true = load_data['c_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 J: int, \n",
    "                                 M: int, \n",
    "                                 P: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        J (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        M (int): Number of data points (number of nodes in the data graph).\n",
    "        P (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-s.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "    history = []\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            Lk, _, _ = compute_Lj_and_lambdaj(Lu + Ld, J)\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lj_and_lambdaj(Ld, J)\n",
    "        elif dictionary_type==\"separated\":\n",
    "            Luk, _, _ = compute_Lj_and_lambdaj(Lu, J, separated=True)\n",
    "            Ldk, _, _ = compute_Lj_and_lambdaj(Ld, J, separated=True)\n",
    "\n",
    "        # Init the dictionary and the sparse representation \n",
    "        D_coll = [cp.Constant(D0[:,(M*i):(M*(i+1))]) for i in range(P)]\n",
    "        Y = cp.Constant(Y_train)\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            # Init constants and parameters\n",
    "            D_coll = [cp.Constant(np.zeros((M, M))) for i in range(P)] \n",
    "            Dsum = cp.Constant(np.zeros((M, M)))\n",
    "            X = cp.Constant(X_train)\n",
    "            I = cp.Constant(np.eye(M))\n",
    "            \n",
    "            # Define the objective function\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                # Init the variables\n",
    "                h = cp.Variable((P, J))\n",
    "                hI = cp.Variable((P, 1))\n",
    "                for i in range(0,P):\n",
    "                    tmp =  cp.Constant(np.zeros((M, M)))\n",
    "                    for j in range(0,J):\n",
    "                        tmp += (cp.Constant(Lk[j, :, :]) * h[i,j])\n",
    "                    tmp += (I*hI[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(P)])\n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "            else:\n",
    "                # Init the variables\n",
    "                hI = cp.Variable((P, J))\n",
    "                hS = cp.Variable((P, J))\n",
    "                hH = cp.Variable((P, 1))\n",
    "                for i in range(0,P):\n",
    "                    tmp =  cp.Constant(np.zeros((M, M)))\n",
    "                    for j in range(0,J):\n",
    "                        tmp += ((cp.Constant(Luk[j, :, :])*hS[i,j]) + (cp.Constant(Ldk[j, :, :])*hI[i,j]))\n",
    "                    tmp += (I*hH[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(P)])\n",
    "                \n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            # Define the constraints\n",
    "            constraints = [D_coll[i] >> 0 for i in range(P)] + \\\n",
    "                            [(cp.multiply(c, I) - D_coll[i]) >> 0 for i in range(P)] + \\\n",
    "                            [(Dsum - cp.multiply((c - epsilon), I)) >> 0, (cp.multiply((c + epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalization\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            \n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else [hS.value, hI.value, hH.value]\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                history.append(min_error_test_norm)\n",
    "\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            # print(\"-\"*100)\n",
    "            # print(f'Iter: {iter_}')\n",
    "            # print()\n",
    "            # print(f'hH.shape: {hH.shape}')\n",
    "            # print(f'hH: {hH.value}')\n",
    "            # print()\n",
    "            # print(f'hS.shape: {hS.shape}')\n",
    "            # print(f'hS: {hS.value}')\n",
    "            # print()\n",
    "            # print(f'hI.shape: {hI.shape}')\n",
    "            # print(f'hI: {hI.value}')\n",
    "            # print()\n",
    "            # print(f'test error: {error_test_norm}')\n",
    "            # print()\n",
    "            # print(\"-\"*100)\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt, Ldk, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c_true[s]  \n",
    "epsilon = epsilon_true[s] \n",
    "k0 = K0_coll[1]\n",
    "\n",
    "D0, X0, _ = initialize_dic(Lu, Ld, P, J, Y_train[:, :, s], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "\n",
    "min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt = topological_dictionary_learn(Y_train[:,:,s], Y_test[:,:,s],\n",
    "                                                                                                                        J, M, P, D0, X0, Lu_full, Ld, \"separated\",\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.02422352,  0.04879447],\n",
       "        [ 0.11675123,  0.06745261],\n",
       "        [-0.24232236, -0.0491332 ]]),\n",
       " array([[-0.02038475,  0.02637722],\n",
       "        [ 0.11983777,  0.01001707],\n",
       "        [-0.15448104,  0.08269612]]),\n",
       " array([[1.01799162e-02],\n",
       "        [2.81445719e-10],\n",
       "        [6.88631764e+00]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_transform(D, K0, Y_test, Y_train=None):\n",
    "\n",
    "    # OMP Step\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)\n",
    "    Domp = D @ W\n",
    "    X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "    # Normalization\n",
    "    X_test = W @ X_test\n",
    "\n",
    "    # Same for the training set\n",
    "    if Y_train != None:\n",
    "        X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "        X_train = W @ X_train\n",
    "\n",
    "        return X_test, X_train\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "def nmse(D, X, Y, m):\n",
    "    return (1/m)* np.sum(la.norm(Y - (D @ X), axis=0)**2 /la.norm(Y, axis=0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global B2, h_opt, J\n",
    "\n",
    "Ldk,_,_= compute_Lj_and_lambdaj(Ld, J)\n",
    "\n",
    "\n",
    "def indicator_matrix(row):\n",
    "    tmp = row.sigma.copy()\n",
    "    tmp[row.idx] = 0\n",
    "    return np.diag(tmp)\n",
    "\n",
    "def compute_Luk(row, b2, J):\n",
    "    Lu = b2 @ row.sigma @ b2.T\n",
    "    Luk = np.array([la.matrix_power(Lu, i) for i in range(1, J + 1)])\n",
    "    return Luk\n",
    "\n",
    "\n",
    "def learn_upper_laplacian(Y_train: np.ndarray,\n",
    "                          Y_test: np.ndarray, \n",
    "                          J: int, \n",
    "                          M: int, \n",
    "                          P: int,\n",
    "                          Lu: np.ndarray, \n",
    "                          Ld: np.ndarray,\n",
    "                          dictionary_type: str, \n",
    "                          c: float, \n",
    "                          epsilon: float, \n",
    "                          K0: int,\n",
    "                          B2: np.ndarray,\n",
    "                          filter: np.ndarray = 1,\n",
    "                          current_min: float = None,\n",
    "                          lambda_: float = 1e-3, \n",
    "                          max_iter: int = 10, \n",
    "                          patience: int = 10,\n",
    "                          tol: float = 1e-7, \n",
    "                          verbose: bool = False):\n",
    "    \n",
    "    # assert np.all(current_min!=B2), \"You must provide the edge-triangle incidence matrix B2.\"\n",
    "\n",
    "    # Check if we are executing the first recursive iteration\n",
    "    if current_min == None:\n",
    "        current_min = np.inf\n",
    "        T = B2.shape[1]\n",
    "        filter = np.ones(T)\n",
    "\n",
    "\n",
    "    D0, X0, _ = initialize_dic(Lu=Lu,\n",
    "                               Ld=Ld,\n",
    "                               P=P,\n",
    "                               J=J,\n",
    "                               Y_train=Y_train,\n",
    "                               K0=K0,\n",
    "                               dictionary_type=dictionary_type,\n",
    "                               c=c,\n",
    "                               epsilon=epsilon,\n",
    "                               only=\"only_X\")\n",
    "\n",
    "    _, min_error_test, h_opt, X_opt_test, _, D_opt, Ldk = topological_dictionary_learn(Y_train=Y_train,\n",
    "                                                                                    Y_test=Y_test,\n",
    "                                                                                    J=J,\n",
    "                                                                                    M=M,\n",
    "                                                                                    P=P,\n",
    "                                                                                    D0=D0,\n",
    "                                                                                    X0=X0,\n",
    "                                                                                    Lu=Lu,\n",
    "                                                                                    Ld=Ld,\n",
    "                                                                                    dictionary_type=dictionary_type,\n",
    "                                                                                    c=c,\n",
    "                                                                                    epsilon=epsilon,\n",
    "                                                                                    K0=K0,\n",
    "                                                                                    lambda_=lambda_,\n",
    "                                                                                    max_iter=max_iter,\n",
    "                                                                                    patience=patience,\n",
    "                                                                                    tol=tol)\n",
    "    \n",
    "    search_space = np.where(filter == 1)    \n",
    "    sigmas = pd.DataFrame({\"idx\": search_space[0]})\n",
    "\n",
    "    sigmas[\"sigma\"] = sigmas.idx.apply(lambda x: filter)\n",
    "    sigmas[\"sigma\"] = sigmas.apply(lambda x: indicator_matrix(x), axis=1)\n",
    "    sigmas[\"Luk\"] = sigmas.apply(lambda x: compute_Luk(x, B2, J), axis=1)\n",
    "    sigmas[\"D\"] = sigmas.apply(lambda x: generate_dictionary(h_opt, P, x.Luk, Ldk), axis=1)\n",
    "    sigmas[\"X\"] = sigmas.D.apply(lambda x: sparse_transform(x, k0, Y_test))\n",
    "    sigmas[\"NMSE\"] = sigmas.apply(lambda x: nmse(x.D, x.X, Y_test, m_test), axis=1)\n",
    "    \n",
    "    candidate_error = sigmas.NMSE.min()\n",
    "    idx_min = sigmas.NMSE.idxmin()\n",
    "\n",
    "    if candidate_error < min_error_test:\n",
    "        S = sigmas.sigma[idx_min]\n",
    "        Lu_new = B2 @ S @ B2.T\n",
    "        current_min = candidate_error\n",
    "        filter = np.diagonal(S)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Removing 1 triangle from topology... \\n ... New min test error: {current_min} !')\n",
    "\n",
    "        return learn_upper_laplacian(Y_train=Y_train,\n",
    "                                    Y_test=Y_test,\n",
    "                                    J =J,\n",
    "                                    M=M,\n",
    "                                    P=P,\n",
    "                                    Lu=Lu_new,\n",
    "                                    Ld=Ld,\n",
    "                                    dictionary_type=dictionary_type,\n",
    "                                    c=c,\n",
    "                                    epsilon=epsilon,\n",
    "                                    K0=K0,\n",
    "                                    filter=filter,\n",
    "                                    current_min=current_min,\n",
    "                                    B2=B2,\n",
    "                                    lambda_=lambda_,\n",
    "                                    max_iter=max_iter,\n",
    "                                    patience=patience,\n",
    "                                    tol=tol,\n",
    "                                    verbose=verbose)\n",
    "\n",
    "    return min_error_test, Lu, h_opt, X_opt_test, D_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.04186229999386083 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.037959970716465834 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.03593584261643158 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.03802877319023118 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.03668390676973623 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.03150897873451867 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.030608754937751094 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.029971369070535627 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.030931449054453383 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.029547819523563935 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.029355113856979 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.03524009425655902 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.02541745778819825 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.025275726068930304 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.024018262746917204 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.022840089790986035 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.02282100999346717 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.020733645667331702 !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n",
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.0197741822607052 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.018925167985305115 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.018922021073218723 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.018480321168142345 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.018272353535300358 !\n",
      "Removing 1 triangle from topology... \n",
      " ... New min test error: 0.019180962179248542 !\n"
     ]
    }
   ],
   "source": [
    "min_error_test, Lu, h_opt, X_opt_test, D_opt = learn_upper_laplacian(B2=B2,\n",
    "                                                        Y_train=Y_train[:,:,s],\n",
    "                                                        Y_test=Y_test[:,:,s],\n",
    "                                                        J=J,\n",
    "                                                        M=M,\n",
    "                                                        P=P,\n",
    "                                                        Lu=Lu_full,\n",
    "                                                        Ld=Ld,\n",
    "                                                        dictionary_type=dictionary_type,\n",
    "                                                        c=c,\n",
    "                                                        epsilon=epsilon,\n",
    "                                                        K0=k0,\n",
    "                                                        lambda_=lambda_,\n",
    "                                                        max_iter=max_iter,\n",
    "                                                        patience=patience,\n",
    "                                                        tol=tol,\n",
    "                                                        verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.sigma[T-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.NMSE.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05141407890933827"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_error_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04186229999386083"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.NMSE.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Luk_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 100)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       ...,\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True],\n",
       "       [ True,  True,  True, ...,  True,  True,  True]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.D[T] == D_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 80)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.X[T-3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
