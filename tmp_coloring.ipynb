{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "prob_T=0.8\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p_edges=0.162, p_triangles=prob_T, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "T = int(np.ceil(nu*(1-prob_T)))\n",
    "\n",
    "# Laplacians\n",
    "Lu, Ld, L = G.get_laplacians(sub_size=100)\n",
    "n =  L.shape[0]\n",
    "\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type=\"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "n_search = 3000\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = \"\"\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 30 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "n_sim = 10\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'D' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m D_true, Y_train, Y_test, X_train, X_test, epsilon_true, c_true \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mn_sim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mm_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mm_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mK0_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mn_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43msparsity_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\engri\\Desktop\\tesi\\TSP-DictionaryLearning\\tsplearn\\data_gen.py:311\u001b[0m, in \u001b[0;36mgenerate_data\u001b[1;34m(dictionary_type, Lu, Ld, n, s, k, n_sim, m_test, m_train, K0_max, n_search, sparsity_mode, verbose)\u001b[0m\n\u001b[0;32m    307\u001b[0m best_sparsity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(n_search)):\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m--> 311\u001b[0m     D_try, h, Y_train_try, Y_test_try, epsilon_try, c_try, X_train_try, X_test_try \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ground_truth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43mm_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43mm_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43mK\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43mK0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK0_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43mdictionary_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43msparsity_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparsity_mode\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m                                                                                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     max_possible_sparsity, _ \u001b[38;5;241m=\u001b[39m verify_dic(D_try, \n\u001b[0;32m    323\u001b[0m                                             Y_train_try, \n\u001b[0;32m    324\u001b[0m                                             X_train_try, \n\u001b[0;32m    325\u001b[0m                                             K0_max, \u001b[38;5;241m.7\u001b[39m)\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m max_possible_sparsity \u001b[38;5;241m>\u001b[39m best_sparsity:\n",
      "File \u001b[1;32mc:\\Users\\engri\\Desktop\\tesi\\TSP-DictionaryLearning\\tsplearn\\data_gen.py:186\u001b[0m, in \u001b[0;36mcreate_ground_truth\u001b[1;34m(Lu, Ld, m_train, m_test, s, K, K0, dictionary_type, sparsity_mode)\u001b[0m\n\u001b[0;32m    183\u001b[0m     h, c, epsilon, _, _ \u001b[38;5;241m=\u001b[39m generate_coeffs(lambda_max_u_k, lambda_min_u_k, lambda_max_d_k, lambda_min_d_k, s\u001b[38;5;241m=\u001b[39ms)\n\u001b[0;32m    184\u001b[0m     D \u001b[38;5;241m=\u001b[39m generate_dictionary(h, s, Luk, Ldk)\n\u001b[1;32m--> 186\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[43mD\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    188\u001b[0m \u001b[38;5;66;03m# Signal Generation\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_column_vec\u001b[39m(row,n, s):\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'D' referenced before assignment"
     ]
    }
   ],
   "source": [
    "D_true, Y_train, Y_test, X_train, X_test, epsilon_true, c_true = generate_data(dictionary_type,\n",
    "                                                                                Lu,\n",
    "                                                                                Ld,\n",
    "                                                                                n,\n",
    "                                                                                s,\n",
    "                                                                                k,\n",
    "                                                                                n_sim,\n",
    "                                                                                m_test,\n",
    "                                                                                m_train,\n",
    "                                                                                K0_max,\n",
    "                                                                                n_search,\n",
    "                                                                                sparsity_mode,\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "dill.load_session(path+'\\\\results\\\\joint\\\\ipynb_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, K)\n",
    "        elif dictionary_type==\"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "\n",
    "        # Init the dictionary and the sparse representation \n",
    "        D_coll = [cp.Constant(D0[:,(n*i):(n*(i+1))]) for i in range(s)]\n",
    "        Y = cp.Constant(Y_train)\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            # Init constants and parameters\n",
    "            D_coll = [cp.Constant(np.zeros((n, n))) for i in range(s)] \n",
    "            Dsum = cp.Constant(np.zeros((n, n)))\n",
    "            X = cp.Constant(X_train)\n",
    "            I = cp.Constant(np.eye(n))\n",
    "            \n",
    "            # Define the objective function\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                # Init the variables\n",
    "                h = cp.Variable((s, K))\n",
    "                hI = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += (cp.Constant(Lk[j, :, :]) * h[i,j])\n",
    "                    tmp += (I*hI[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "            else:\n",
    "                # Init the variables\n",
    "                hI = cp.Variable((s, K))\n",
    "                hS = cp.Variable((s, K))\n",
    "                hH = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += ((cp.Constant(Luk[j, :, :])*hS[i,j]) + (cp.Constant(Ldk[j, :, :])*hI[i,j]))\n",
    "                    tmp += (I*hH[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                \n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            # Define the constraints\n",
    "            constraints = [D_coll[i] >> 0 for i in range(s)] + \\\n",
    "                            [(cp.multiply(c, I) - D_coll[i]) >> 0 for i in range(s)] + \\\n",
    "                            [(Dsum - cp.multiply((c - epsilon), I)) >> 0, (cp.multiply((c + epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalization\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            \n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else [hS.value, hI.value, hH.value]\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            # print(\"-\"*100)\n",
    "            # print(f'Iter: {iter_}')\n",
    "            # print()\n",
    "            # print(f'hH.shape: {hH.shape}')\n",
    "            # print(f'hH: {hH.value}')\n",
    "            # print()\n",
    "            # print(f'hS.shape: {hS.shape}')\n",
    "            # print(f'hS: {hS.value}')\n",
    "            # print()\n",
    "            # print(f'hI.shape: {hI.shape}')\n",
    "            # print(f'hI: {hI.value}')\n",
    "            # print()\n",
    "            # print(f'test error: {error_test_norm}')\n",
    "            # print()\n",
    "            # print(\"-\"*100)\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.49493901, 9.97980896, 4.36082596, 5.68226973, 9.7411766 ,\n",
       "       2.96568358, 9.01249083, 8.3587013 , 9.22004083, 4.87029145])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\engri\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:420: RuntimeWarning: Orthogonal matching pursuit ended prematurely due to linear dependence in the dictionary. The requested precision might not have been met.\n",
      "  out = _cholesky_omp(\n"
     ]
    }
   ],
   "source": [
    "c = c_true[7]  \n",
    "epsilon = epsilon_true[7] \n",
    "k0 = K0_coll[1]\n",
    "\n",
    "D0, X0, _ = initialize_dic(Lu, Ld, s, k, Y_train[:, :, 7], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "\n",
    "min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt = topological_dictionary_learn(Y_train[:,:,7], Y_test[:,:,7],\n",
    "                                                                                                                        k, n, s, D0, X0, Lu, Ld, \"separated\",\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.06606493,  0.01967646],\n",
       "        [-0.14494447, -0.03232593],\n",
       "        [ 0.07207933,  0.01575976]]),\n",
       " array([[ 0.07860698,  0.02082813],\n",
       "        [-0.21199992, -0.00436066],\n",
       "        [ 0.12997382,  0.00831749]]),\n",
       " array([[1.41647626e-12],\n",
       "        [6.17015881e+00],\n",
       "        [1.85799240e-11]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  b2\n",
       "0  [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas = pd.DataFrame({\"b2\": [B2]})\n",
    "sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_transform(D, K0, Y_test, Y_train=None):\n",
    "\n",
    "    # OMP Step\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)\n",
    "    Domp = D @ W\n",
    "    X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "    # Normalization\n",
    "    X_test = W @ X_test\n",
    "\n",
    "    # Same for the training set\n",
    "    if Y_train!=None:\n",
    "        X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "        X_train = W @ X_train\n",
    "\n",
    "        return X_test, X_train\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmse(D, X, Y, m):\n",
    "    return (1/m)* np.sum(la.norm(Y - (D @ X), axis=0)**2 /la.norm(Y, axis=0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global B2, h_opt, k\n",
    "\n",
    "Ldk,_,_= compute_Lk_and_lambdak(Ld, k)\n",
    "\n",
    "\n",
    "def indicator_matrix(row):\n",
    "    row.sigma[row.idx] = 0\n",
    "    return np.diag(row.sigma)\n",
    "\n",
    "def compute_Luk(row, b2, k):\n",
    "    Lu = b2 @ row.sigma @ b2.T\n",
    "    Luk = np.array([la.matrix_power(Lu, i) for i in range(1, k + 1)])\n",
    "    return Luk\n",
    "\n",
    "T = B2.shape[1]\n",
    "sigmas = pd.DataFrame({\"idx\": np.arange(T)})\n",
    "\n",
    "sigmas[\"sigma\"] = sigmas.idx.apply(lambda x: np.ones(T))\n",
    "sigmas[\"sigma\"] = sigmas.apply(lambda x: indicator_matrix(x), axis=1)\n",
    "sigmas[\"Luk\"] = sigmas.apply(lambda x: compute_Luk(x, B2, k), axis=1)\n",
    "sigmas[\"D\"] = sigmas.apply(lambda x: generate_dictionary(h_opt, 1, x.Luk, Ldk), axis=1)\n",
    "sigmas[\"X\"] = sigmas.D.apply(lambda x: sparse_transform(x, k0, Y_test[:,:,7]))\n",
    "sigmas[\"NMSE\"] = sigmas.apply(lambda x: nmse(x.D, x.X, Y_test[:,:,7], m_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas[\"NMSE\"] = sigmas.apply(lambda x: nmse(x.D, x.X, Y_test[:,:,7], m_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0072992054231631855"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_error_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008666916305215688"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.NMSE.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 80)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.X[T-3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
