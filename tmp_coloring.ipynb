{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "prob_T=0.8\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p_edges=0.162, p_triangles=prob_T, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "T = int(np.ceil(nu*(1-prob_T)))\n",
    "\n",
    "# Laplacians\n",
    "Lu, Ld, L = G.get_laplacians(sub_size=100)\n",
    "M =  L.shape[0]\n",
    "\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type=\"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "P = 3 # Number of Kernels (Sub-dictionaries)\n",
    "J = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(M*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "n_search = 1500\n",
    "n_sim = 10\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 100 \n",
    "patience = 5 \n",
    "tol = 1e-s # tolerance for Patience\n",
    "lambda_ = 1e-s # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:14<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:14<00:00, 11.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:14<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:15<00:00, 11.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:10<00:00, 11.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [02:03<00:00, 12.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [01:22<00:00, 18.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [01:12<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [01:13<00:00, 20.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [01:13<00:00, 20.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "D_true, Y_train, Y_test, X_train, X_test, epsilon_true, c_true = generate_data(dictionary_type=dictionary_type,\n",
    "                                                                                Lu=Lu,\n",
    "                                                                                Ld=Ld,\n",
    "                                                                                M=M,\n",
    "                                                                                P=P,\n",
    "                                                                                J=J,\n",
    "                                                                                n_sim=n_sim,\n",
    "                                                                                m_test=m_test,\n",
    "                                                                                m_train=m_train,\n",
    "                                                                                K0_max=K0_max,\n",
    "                                                                                n_search=n_search,\n",
    "                                                                                sparsity_mode=sparsity_mode,\n",
    "                                                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'top_data_T80'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'top_data_T{int(prob_T*100)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle \n",
    "\n",
    "name = f'top_data_T{int(prob_T*100)}'\n",
    "save_var = {\"D_true\" : D_true,\n",
    "            \"Y_train\" : Y_train,\n",
    "            \"Y_test\" : Y_test,\n",
    "            \"X_train\" : X_train,\n",
    "            \"X_test\" : X_test,\n",
    "            \"epsilon_true\" : epsilon_true,\n",
    "            \"c_true\" : c_true}\n",
    "\n",
    "PATH = os.getcwd()\n",
    "DIR_PATH = f'{PATH}\\\\synthetic_data'\n",
    "FILENAME = f'{DIR_PATH}\\\\{name}.pkl'\n",
    "\n",
    "if not os.path.exists(DIR_PATH):\n",
    "    os.makedirs(DIR_PATH)\n",
    "    \n",
    "with open(FILENAME, 'wb') as f: \n",
    "    pickle.dump(save_var, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "dill.load_session(path+'\\\\results\\\\joint\\\\ipynb_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 J: int, \n",
    "                                 M: int, \n",
    "                                 P: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        J (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        M (int): Number of data points (number of nodes in the data graph).\n",
    "        P (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-s.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, J)\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, J)\n",
    "        elif dictionary_type==\"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, J, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, J, separated=True)\n",
    "\n",
    "        # Init the dictionary and the sparse representation \n",
    "        D_coll = [cp.Constant(D0[:,(M*i):(M*(i+1))]) for i in range(P)]\n",
    "        Y = cp.Constant(Y_train)\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            # Init constants and parameters\n",
    "            D_coll = [cp.Constant(np.zeros((M, M))) for i in range(P)] \n",
    "            Dsum = cp.Constant(np.zeros((M, M)))\n",
    "            X = cp.Constant(X_train)\n",
    "            I = cp.Constant(np.eye(M))\n",
    "            \n",
    "            # Define the objective function\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                # Init the variables\n",
    "                h = cp.Variable((P, J))\n",
    "                hI = cp.Variable((P, 1))\n",
    "                for i in range(0,P):\n",
    "                    tmp =  cp.Constant(np.zeros((M, M)))\n",
    "                    for j in range(0,J):\n",
    "                        tmp += (cp.Constant(Lk[j, :, :]) * h[i,j])\n",
    "                    tmp += (I*hI[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(P)])\n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "            else:\n",
    "                # Init the variables\n",
    "                hI = cp.Variable((P, J))\n",
    "                hS = cp.Variable((P, J))\n",
    "                hH = cp.Variable((P, 1))\n",
    "                for i in range(0,P):\n",
    "                    tmp =  cp.Constant(np.zeros((M, M)))\n",
    "                    for j in range(0,J):\n",
    "                        tmp += ((cp.Constant(Luk[j, :, :])*hS[i,j]) + (cp.Constant(Ldk[j, :, :])*hI[i,j]))\n",
    "                    tmp += (I*hH[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(P)])\n",
    "                \n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            # Define the constraints\n",
    "            constraints = [D_coll[i] >> 0 for i in range(P)] + \\\n",
    "                            [(cp.multiply(c, I) - D_coll[i]) >> 0 for i in range(P)] + \\\n",
    "                            [(Dsum - cp.multiply((c - epsilon), I)) >> 0, (cp.multiply((c + epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalization\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            \n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else [hS.value, hI.value, hH.value]\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            # print(\"-\"*100)\n",
    "            # print(f'Iter: {iter_}')\n",
    "            # print()\n",
    "            # print(f'hH.shape: {hH.shape}')\n",
    "            # print(f'hH: {hH.value}')\n",
    "            # print()\n",
    "            # print(f'hS.shape: {hS.shape}')\n",
    "            # print(f'hS: {hS.value}')\n",
    "            # print()\n",
    "            # print(f'hI.shape: {hI.shape}')\n",
    "            # print(f'hI: {hI.value}')\n",
    "            # print()\n",
    "            # print(f'test error: {error_test_norm}')\n",
    "            # print()\n",
    "            # print(\"-\"*100)\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = c_true[s]  \n",
    "epsilon = epsilon_true[s] \n",
    "k0 = K0_coll[1]\n",
    "\n",
    "D0, X0, _ = initialize_dic(Lu, Ld, P, J, Y_train[:, :, s], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "\n",
    "min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train, D_opt = topological_dictionary_learn(Y_train[:,:,s], Y_test[:,:,s],\n",
    "                                                                                                                        J, M, P, D0, X0, Lu, Ld, \"separated\",\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.00663524,  0.00051291],\n",
       "        [ 0.00106015,  0.00128251],\n",
       "        [ 0.00435605,  0.00100671]]),\n",
       " array([[-0.00205789,  0.00094752],\n",
       "        [ 0.00015903,  0.00053683],\n",
       "        [-0.0001982 ,  0.00022728]]),\n",
       " array([[0.14418037],\n",
       "        [0.00123116],\n",
       "        [0.00391908]])]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  b2\n",
       "0  [[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas = pd.DataFrame({\"b2\": [B2]})\n",
    "sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_transform(D, K0, Y_test, Y_train=None):\n",
    "\n",
    "    # OMP Step\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)\n",
    "    Domp = D @ W\n",
    "    X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "    # Normalization\n",
    "    X_test = W @ X_test\n",
    "\n",
    "    # Same for the training set\n",
    "    if Y_train!=None:\n",
    "        X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "        X_train = W @ X_train\n",
    "\n",
    "        return X_test, X_train\n",
    "    \n",
    "    return X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nmse(D, X, Y, m):\n",
    "    return (1/m)* np.sum(la.norm(Y - (D @ X), axis=0)**2 /la.norm(Y, axis=0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global B2, h_opt, J\n",
    "\n",
    "Ldk,_,_= compute_Lk_and_lambdak(Ld, J)\n",
    "\n",
    "\n",
    "def indicator_matrix(row):\n",
    "    row.sigma[row.idx] = 0\n",
    "    return np.diag(row.sigma)\n",
    "\n",
    "def compute_Luk(row, b2, J):\n",
    "    Lu = b2 @ row.sigma @ b2.T\n",
    "    Luk = np.array([la.matrix_power(Lu, i) for i in range(1, J + 1)])\n",
    "    return Luk\n",
    "\n",
    "T = B2.shape[1]\n",
    "sigmas = pd.DataFrame({\"idx\": np.arange(T)})\n",
    "\n",
    "sigmas[\"sigma\"] = sigmas.idx.apply(lambda x: np.ones(T))\n",
    "sigmas[\"sigma\"] = sigmas.apply(lambda x: indicator_matrix(x), axis=1)\n",
    "sigmas[\"Luk\"] = sigmas.apply(lambda x: compute_Luk(x, B2, J), axis=1)\n",
    "sigmas[\"D\"] = sigmas.apply(lambda x: generate_dictionary(h_opt, 1, x.Luk, Ldk), axis=1)\n",
    "sigmas[\"X\"] = sigmas.D.apply(lambda x: sparse_transform(x, k0, Y_test[:,:,s]))\n",
    "sigmas[\"NMSE\"] = sigmas.apply(lambda x: nmse(x.D, x.X, Y_test[:,:,s], m_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041085929310523"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_error_test_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.213406\n",
       "1     0.213309\n",
       "2     0.213480\n",
       "3     0.213293\n",
       "4     0.213197\n",
       "        ...   \n",
       "57    0.213279\n",
       "58    0.213148\n",
       "59    0.213424\n",
       "60    0.213142\n",
       "61    0.212977\n",
       "Name: NMSE, Length: 62, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.NMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 80)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmas.X[T-3].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
