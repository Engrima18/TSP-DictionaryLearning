{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_ground_truth.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "# __all__ = ['create_ground_truth']\n",
    "\n",
    "def compute_Lk_and_lambdak(L, K, separated=False):\n",
    "    lambdas, _ = eigs(L)\n",
    "    lambdas[np.abs(lambdas) < np.finfo(float).eps] = 0\n",
    "    lambda_max = np.max(lambdas).real\n",
    "    lambda_min = np.min(lambdas).real\n",
    "    Lk = np.array([la.matrix_power(L, i) for i in range(1, K + 1)])\n",
    "    # for the \"separated\" implementation we need a different dimensionality\n",
    "    if separated:\n",
    "        lambda_max_k = lambda_max ** np.arange(1, K + 1)\n",
    "        lambda_min_k = lambda_min ** np.arange(1, K + 1)\n",
    "    else:\n",
    "        lambda_max_k = lambda_max ** np.array(list(np.arange(1, K + 1))+[0])\n",
    "        lambda_min_k = lambda_min ** np.array(list(np.arange(1, K + 1))+[0])\n",
    "    return Lk, lambda_max_k, lambda_min_k\n",
    "\n",
    "def generate_coeffs(*arrays, s, mult=10):\n",
    "    \"\"\" \n",
    "    Select ad hoc parameters for synthetic data generation, randomly over\n",
    "    an interval dependent on the max eigenvalues of the Laplacian(s)\n",
    "    \"\"\"\n",
    "\n",
    "    # if passing four arguments (two for upper and two for lower laplacian eigevals)\n",
    "    # it means that you are using dictionary_type=\"separated\"\n",
    "    if len(arrays)==2:\n",
    "        lambda_max_k, lambda_min_k = arrays\n",
    "        K = lambda_max_k.shape[0]\n",
    "        h = mult / np.max(lambda_max_k) * np.random.rand(s, K)\n",
    "        # For later sanity check in optimization phase \n",
    "        tmp_max_vec = h @ lambda_max_k # parallelize the code with simple matrix multiplications\n",
    "        tmp_min_vec = h @ lambda_min_k\n",
    "        c = np.max(tmp_max_vec)\n",
    "        tmp_sum_max = np.sum(tmp_max_vec)\n",
    "        tmp_sum_min = np.sum(tmp_min_vec)\n",
    "\n",
    "        Delta_min = c - tmp_sum_min\n",
    "        Delta_max = tmp_sum_max - c\n",
    "        epsilon = (Delta_max - Delta_min) * np.random.rand() + Delta_min\n",
    "\n",
    "    elif len(arrays)==4:\n",
    "        lambda_max_u_k, lambda_min_u_k, lambda_max_d_k, lambda_min_d_k = arrays\n",
    "        K = lambda_max_u_k.shape[0]\n",
    "        # np.random.seed(10)\n",
    "        hI = mult / np.max(lambda_max_d_k) * np.random.rand(s, K)\n",
    "        hS = mult / np.max(lambda_max_u_k) * np.random.rand(s, K)\n",
    "        hH = mult / np.min([np.max(lambda_max_u_k), np.max(lambda_max_d_k)]) * np.random.rand(s, 1)\n",
    "        h = [hS, hI, hH]\n",
    "        # For later sanity check in optimization phase\n",
    "        tmp_max_vec_S = (hS @ lambda_max_u_k).reshape(s,1)\n",
    "        tmp_min_vec_S = (hS @ lambda_min_u_k).reshape(s,1)\n",
    "        tmp_max_vec_I = (hI @ lambda_max_d_k).reshape(s,1)\n",
    "        tmp_min_vec_I = (hI @ lambda_min_d_k).reshape(s,1)\n",
    "        c = np.max(tmp_max_vec_I + tmp_max_vec_S + hH)\n",
    "        tmp_sum_min = np.sum(tmp_min_vec_I + tmp_min_vec_S + hH)\n",
    "        tmp_sum_max = np.sum(tmp_max_vec_I + tmp_max_vec_S + hH)\n",
    "        Delta_min = c - tmp_sum_min\n",
    "        Delta_max = tmp_sum_max - c\n",
    "        epsilon = np.max([Delta_min, Delta_max])\n",
    "    else:\n",
    "        raise ValueError(\"Function accepts either 2 or 4 arrays! In case of 4 arrays are provided,\\\n",
    "                        the first 2 refer to upper laplacian and the other two to lower laplacian.\")\n",
    "    return h, c, epsilon, tmp_sum_min, tmp_sum_max\n",
    "\n",
    "def generate_dictionary(h, s, *matrices):\n",
    "    D = []\n",
    "    # Always check if upper and lower Laplacians are separately provided\n",
    "    if len(matrices)==1:\n",
    "        Lk = matrices[0]\n",
    "        n = Lk.shape[-1]\n",
    "        k = Lk.shape[0]\n",
    "        # iterate over each kernel dimension\n",
    "        for i in range(0,s):\n",
    "            # Replicate formula (8) of the paper for the i-th sub-dictionary\n",
    "            tmp = np.sum(h[i]*Lk, axis=0) + h[i,-1]*np.eye(n,n)\n",
    "            D.append(tmp)\n",
    "    elif len(matrices)==2:\n",
    "        Luk , Ldk = matrices\n",
    "        n = Luk.shape[-1]\n",
    "        k = Luk.shape[0]\n",
    "        # iterate over each kernel dimension\n",
    "        for i in range(0,s):\n",
    "            # Replicate formula (8) of the paper for the i-th sub-dictionary\n",
    "            hu = h[0][i].reshape(k,1,1)\n",
    "            hd = h[1][i].reshape(k,1,1)\n",
    "            hid = h[2][i]\n",
    "            tmp = np.sum(hu*Luk + hd*Ldk, axis=0) + hid*np.eye(n,n)\n",
    "            D.append(tmp)\n",
    "    else:\n",
    "        raise ValueError(\"Function accepts one vector and either 1 or 2 matrices.\")\n",
    "    D = np.array(D).reshape(n, n*s)\n",
    "    return D\n",
    "\n",
    "def create_ground_truth(Lu, Ld, m_train, m_test, s, K, K0, dictionary_type, sparsity_mode):\n",
    "\n",
    "    # Joint Dictionary Model\n",
    "    if dictionary_type == \"joint\":\n",
    "        Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        h, c, epsilon, _, _ = generate_coeffs(lambda_max_k, lambda_min_k, s=s)\n",
    "        D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "    # Edge Laplacian Dictionary Model\n",
    "    elif dictionary_type == \"edge_laplacian\":\n",
    "        Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Ld, K)\n",
    "        h, c, epsilon, _, _ = generate_coeffs(lambda_max_k, lambda_min_k, s=s)\n",
    "        D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "    # Separated Dictionary Model\n",
    "    elif dictionary_type == \"separated\":\n",
    "        Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "        Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "        h, c, epsilon, _, _ = generate_coeffs(lambda_max_u_k, lambda_min_u_k, lambda_max_d_k, lambda_min_d_k, s=s)\n",
    "        D = generate_dictionary(h, s, Luk, Ldk)\n",
    "\n",
    "    n = D.shape[0]\n",
    "    # Signal Generation\n",
    "    def create_column_vec(row,n):\n",
    "        tmp = np.zeros(n*s)\n",
    "        tmp[row['idxs']]=row['non_zero_coeff']\n",
    "        return tmp\n",
    "    \n",
    "    m_total = m_train + m_test\n",
    "    tmp = pd.DataFrame()\n",
    "    # Determine the sparsity for each column based on sparsity_mode\n",
    "    if sparsity_mode == \"max\":\n",
    "        tmp_K0 = np.random.choice(np.arange(1,K0+1), size=(m_total), replace=True)\n",
    "    else:\n",
    "        tmp_K0 = np.full((m_total,), K0)\n",
    "    # sparsity coefficient for each column\n",
    "    tmp['K0'] = tmp_K0\n",
    "    # for each column get K0 indexes\n",
    "    tmp['idxs'] = tmp.K0.apply(lambda x: np.random.choice(n*s, x, replace=False))\n",
    "    # for each of the K0 row indexes in each column, sample K0 values\n",
    "    tmp['non_zero_coeff'] = tmp.K0.apply(lambda x: np.random.randn(x))\n",
    "    # create the column vectors with the desired characteristics\n",
    "    tmp['column_vec'] = tmp.apply(lambda x: create_column_vec(x,n=n), axis=1)\n",
    "    # finally derive the sparse signal representation matrix\n",
    "    X = np.column_stack(tmp['column_vec'].values)\n",
    "\n",
    "    all_data = D @ X\n",
    "    X_train = X[:, :m_train]\n",
    "    X_test = X[:, m_train:]\n",
    "    train_Y = all_data[:, :m_train]\n",
    "    test_Y = all_data[:, m_train:]\n",
    "\n",
    "    return D, h, train_Y, test_Y, epsilon, c, X_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_omp_coeff(K0, Domp, col):\n",
    "    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=K0)\n",
    "    omp.fit(Domp, col)\n",
    "    return omp.coef_\n",
    "\n",
    "def initialize_dic(Lu, Ld, s, K, Y_train, K0, dictionary_type, c, epsilon, only):\n",
    "\n",
    "    n = Lu.shape[0]\n",
    "    D = np.zeros((n, n*s))\n",
    "    X = np.zeros(Y_train.shape)\n",
    "    X = np.tile(X, (s,1))\n",
    "    discard = 0\n",
    "\n",
    "    # maybe is better to create a wrapper\n",
    "    def multiplier_search(*arrays, s=s):\n",
    "        is_okay = 0\n",
    "        mult = 100\n",
    "        tries = 0\n",
    "        while is_okay==0:\n",
    "            is_okay = 1\n",
    "            h, c_try, _, tmp_sum_min, tmp_sum_max = generate_coeffs(arrays, s=s, mult=mult)\n",
    "            if c_try <= c:\n",
    "                is_okay *= 1\n",
    "            if tmp_sum_min > c-epsilon:\n",
    "                is_okay *= 1\n",
    "                incr_mult = 0\n",
    "            else:\n",
    "                is_okay = is_okay*0\n",
    "                incr_mult = 1\n",
    "            if tmp_sum_max < c+epsilon:\n",
    "                is_okay *= 1\n",
    "                decr_mult = 0\n",
    "            else:\n",
    "                is_okay *= 0\n",
    "                decr_mult = 1\n",
    "            if is_okay == 0:\n",
    "                tries += 1\n",
    "            if tries >3:\n",
    "                discard = 1\n",
    "                break\n",
    "            if incr_mult == 1:\n",
    "                mult *= 2\n",
    "            if decr_mult == 1:\n",
    "                mult /= 2\n",
    "        return h, discard\n",
    "\n",
    "    if (only == \"only_D\") or (only == \"all\"):\n",
    "        X = 0\n",
    "        # Joint Dictionary Model\n",
    "        if dictionary_type == \"joint\":\n",
    "            Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "            h, discard = multiplier_search(lambda_max_k, lambda_min_k)\n",
    "            D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "        # Edge Laplacian Dictionary Model\n",
    "        elif dictionary_type == \"edge_laplacian\":\n",
    "            Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Ld, K)\n",
    "            h, discard = multiplier_search(lambda_max_k, lambda_min_k)\n",
    "            D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "        # Separated Dictionary Model\n",
    "        elif dictionary_type == \"separated\":\n",
    "            Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "            h, discard = multiplier_search(lambda_max_d_k, lambda_min_d_k, lambda_max_u_k, lambda_min_u_k)\n",
    "            D = generate_dictionary(h, s, Luk, Ldk)\n",
    "    \n",
    "    if (only == \"only_X\" or only == \"all\"):\n",
    "        \n",
    "        if dictionary_type == \"edge_laplacian\":\n",
    "            L = Ld\n",
    "        else:\n",
    "            L = Lu+Ld\n",
    "\n",
    "        _, Dx = eigs(L)\n",
    "        dd = la.norm(Dx, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        Domp = Dx@W\n",
    "        X = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp.real, x), axis=0, arr=Y_train.real)\n",
    "        X = np.tile(X, (s,1))\n",
    "        \n",
    "    return D, X, discard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "def verify_dic(D, Y_train, X_train_true, K0_max, acc_thresh):\n",
    "    # OMP\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)  # Normalization Step\n",
    "    Domp = D @ W\n",
    "    fin_acc = 0\n",
    "    for K0 in range(1, K0_max+1):\n",
    "        idx = np.sum(np.abs(X_train_true) > 0, axis=0) == K0  # select all column vectors with certain sparsity (K0 non-null elements)\n",
    "        tmp_train = Y_train[:, idx].real\n",
    "        ##########################\n",
    "        if tmp_train.shape[1]==0:\n",
    "            continue\n",
    "        X_true_tmp = X_train_true[:, idx].real\n",
    "        idx_group = np.abs(X_true_tmp) > 0\n",
    "        X_tr = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp.real, x), axis=0, arr=tmp_train)\n",
    "        idx_train = np.abs(X_tr) > 0\n",
    "        acc = np.sum(np.sum(idx_group == idx_train, axis=0) == idx_group.shape[0])/idx_group.shape[1]\n",
    "        if acc < 0.7:\n",
    "            break\n",
    "        else:\n",
    "            fin_acc = acc\n",
    "    # ????????\n",
    "    max_possible_sparsity = K0 - 1\n",
    "    return max_possible_sparsity, fin_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological_dictionary_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "def topological_dictionary_learn(Y_train, Y_test, K, n, s, D0, X0, Lu, Ld, dictionary_type, c, epsilon, K0, lambda_=1e-5, max_iter=10, patience=10, tol=1e-7, verbose=0):\n",
    "    h_opt =0  ##############################################\n",
    "    X_opt_train=0\n",
    "    X_opt_test=0  ###########################################\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        # Joint Dictionary\n",
    "        if dictionary_type == \"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        # Separated Dictionary\n",
    "        elif dictionary_type == \"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "        # Edge Laplacian\n",
    "        elif dictionary_type == \"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, K)\n",
    "\n",
    "        # Init the dictionary and the sparse representation\n",
    "        D0 = D0.reshape(n,n,s)\n",
    "        D_coll = [cp.Constant(D0[:,:,j]) for j in range(s)]\n",
    "        X_train = X0\n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            I = cp.Constant(np.eye(n))\n",
    "\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                h = cp.Variable((s, K))\n",
    "                hI = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += cp.multiply(h[i,j], Lk[j, :, :])\n",
    "                    tmp += cp.multiply(hI[i,:], I)\n",
    "                    D_coll[i] = tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                term1 = cp.norm(Y_train - D @ X_train, 'fro')**2\n",
    "                term2 = lambda_ * cp.norm(h, 'fro')**2\n",
    "                term3 = lambda_ * cp.norm(hI, 'fro')**2\n",
    "                obj = cp.Minimize(term1+term2+term3)\n",
    "            else:\n",
    "                hI = cp.Variable((s, K))\n",
    "                hS = cp.Variable((s, K))\n",
    "                hH = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    hu = hS[i,:]\n",
    "                    hd = hI[i,:]\n",
    "                    hid = hH[i]\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += cp.multiply(hu[j], Luk[j, :, :]) + cp.multiply(hd[j], Ldk[j, :, :])\n",
    "                    tmp += cp.multiply(hid, I)\n",
    "                    D_coll[i] = tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                \n",
    "                term1 = cp.norm(Y_train - D @ X_train, 'fro')**2\n",
    "                term2 = lambda_ * cp.norm(hI, 'fro')**2\n",
    "                term3 = lambda_ * cp.norm(hS, 'fro')**2\n",
    "                term4 = lambda_ * cp.norm(hH, 'fro')**2\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            Dsum = cp.sum(D_coll)\n",
    "            constraints = [D_coll[i] >= 0 * I for i in range(s)] + \\\n",
    "                            [D_coll[i] <= c * I for i in range(s)] + \\\n",
    "                            [Dsum >= cp.multiply((c - epsilon), I), Dsum <= cp.multiply((c + epsilon), I)]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.CLARABEL)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train.real)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test.real)\n",
    "            # Normalize?\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "                print(\"ping!\")\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI.value, hS.value, hH.value])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                print(\"ping!\")\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "\n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = eigs(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train.real)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test.real)\n",
    "        # Normalize?\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "    return h_opt, X_opt_test, X_opt_train, min_error_test_norm, min_error_train_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimostrazione problema numero 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221.78636757869785"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prova\n",
    "\n",
    "import scipy.io\n",
    "mat = scipy.io.loadmat('datiSC.mat')\n",
    "B1 = mat[\"B1\"]\n",
    "B2 = mat[\"B2\"]\n",
    "# Subsampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1, dtype=float)\n",
    "Lu = np.matmul(B2, np.transpose(B2), dtype=float)\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "_ ,U = la.eig(L)\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type_true = \"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s_true = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k_true = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = dictionary_type_true\n",
    "s = s_true\n",
    "k = k_true\n",
    "K0_coll = np.arange(5, 26, 4) # K0_coll = 5:4:25 %4:4:40 %5:3:20\n",
    "lambda_ = 1e-6 # l2 multiplier\n",
    "max_iter = 100 # Maximum number of iterations\n",
    "patience = 5 # Patience\n",
    "tol = 1e-7 # Tolerance for Patience\n",
    "n_sim = 10\n",
    "verbose = 0\n",
    "\n",
    "Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, k, separated=True)\n",
    "Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, k, separated=True)\n",
    "h, c, epsilon, _, _ = generate_coeffs(lambda_max_u_k, lambda_min_u_k, lambda_max_d_k, lambda_min_d_k, s=s)\n",
    "hS = h[0]\n",
    "hI = h[1]\n",
    "hH = h[2]\n",
    "D1 = []\n",
    "D2 = []\n",
    "n = Luk.shape[-1]\n",
    "k = Luk.shape[0]\n",
    "# print(\"My try\")\n",
    "# # iterate over each kernel dimension\n",
    "# for i in range(0,s):\n",
    "#     # Replicate formula (8) of the paper for the i-th sub-dictionary\n",
    "#     hu = h[0][i].reshape(k,1,1)\n",
    "#     hd = h[1][i].reshape(k,1,1)\n",
    "#     hid = h[2][i]\n",
    "#     tmp = np.sum(hu*Luk + hd*Ldk, axis=0) + hid*np.eye(n,n)\n",
    "#     D2.append(tmp)\n",
    "#     print(np.sum(tmp))\n",
    "# print(np.sum(D2))\n",
    "# print()\n",
    "# D2 = np.array(D2).reshape(n, n*s)\n",
    "# print(\"My try with explicit loop\")\n",
    "# for i in range(s):\n",
    "#     tmp = np.zeros((n,n))\n",
    "#     print(f's: {i}')\n",
    "#     # Replicate formula (8) of the paper for the i-th sub-dictionary\n",
    "#     for j in range(k):\n",
    "#         print(f'k: {j}')\n",
    "#         print(hI[i,j])\n",
    "#         tmp = tmp + hI[i,j]*Ldk[j,:,:] + hS[i,j]*Luk[j,:,:]\n",
    "#         print(np.sum(tmp))\n",
    "#     tmp = tmp + hH[i,0]*np.eye(n,n)\n",
    "#     print(f'Fuori! {np.sum(tmp)}')\n",
    "#     D1.append(tmp)\n",
    "\n",
    "# D1 = np.array(D1).reshape(n, n*s)\n",
    "# print(np.sum(D1))\n",
    "# D = generate_dictionary(h, s, Luk, Ldk)\n",
    "\n",
    "np.random.seed(10)\n",
    "# Joint Dictionary Model\n",
    "if dictionary_type == \"joint\":\n",
    "    Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Lu + Ld, k)\n",
    "    h, c, epsilon, _, _ = generate_coeffs(lambda_max_k, lambda_min_k, s=s)\n",
    "    D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "# Edge Laplacian Dictionary Model\n",
    "elif dictionary_type == \"edge_laplacian\":\n",
    "    Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Ld, k)\n",
    "    h, c, epsilon, _, _ = generate_coeffs(lambda_max_k, lambda_min_k, s=s)\n",
    "    D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "# Separated Dictionary Model\n",
    "elif dictionary_type == \"separated\":\n",
    "    Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, k, separated=True)\n",
    "    Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, k, separated=True)\n",
    "    h, c, epsilon, _, _ = generate_coeffs(lambda_max_u_k, lambda_min_u_k, lambda_max_d_k, lambda_min_d_k, s=s)\n",
    "    D = generate_dictionary(h, s, Luk, Ldk)\n",
    "\n",
    "n = D.shape[0]\n",
    "\n",
    "# Signal Generation\n",
    "def create_column_vec(row,n):\n",
    "    tmp = np.zeros(n*s)\n",
    "    tmp[row['idxs']]=row['non_zero_coeff']\n",
    "    return tmp\n",
    "\n",
    "def custom_rand(row,n,s):\n",
    "    np.random.seed(1218)\n",
    "    res = np.random.choice(n*s, row, replace=False)\n",
    "    return res\n",
    "\n",
    "def custom_randn(row):\n",
    "    np.random.seed(1218)\n",
    "    res = np.random.randn(row)\n",
    "    return res\n",
    "\n",
    "m_total = m_train + m_test\n",
    "tmp = pd.DataFrame()\n",
    "# Determine the sparsity for each column based on sparsity_mode\n",
    "if sparsity_mode == \"max\":\n",
    "    np.random.seed(1218)\n",
    "    tmp_K0 = np.random.choice(np.arange(1,K0_max+1), size=(m_total), replace=True)\n",
    "else:\n",
    "    tmp_K0 = np.full((m_total,), K0_max)\n",
    "# sparsity coefficient for each column\n",
    "tmp['K0'] = tmp_K0\n",
    "# for each column get K0 indexes\n",
    "tmp['idxs'] = tmp.K0.apply(lambda x: custom_rand(x,n,s))\n",
    "# for each of the K0 row indexes in each column, sample K0 values\n",
    "tmp['non_zero_coeff'] = tmp.K0.apply(lambda x: custom_randn(x))\n",
    "# create the column vectors with the desired characteristics\n",
    "tmp['column_vec'] = tmp.apply(lambda x: create_column_vec(x,n=n), axis=1)\n",
    "# finally derive the sparse signal representation matrix\n",
    "X = np.column_stack(tmp['column_vec'].values)\n",
    "\n",
    "all_data = D @ X\n",
    "X_train = X[:, :m_train]\n",
    "X_test = X[:, m_train:]\n",
    "train_Y = all_data[:, :m_train]\n",
    "test_Y = all_data[:, m_train:]\n",
    "\n",
    "\n",
    "X1 = np.zeros((n*s, m_total))\n",
    "for j in range(m_total):\n",
    "    np.random.seed(1218)\n",
    "    tmp_idx = np.random.choice(n*s, tmp_K0[j], replace=False)\n",
    "    np.random.seed(1218)\n",
    "    non_zero_coeff = np.random.randn(tmp_K0[j])\n",
    "    X1[tmp_idx, j] = non_zero_coeff\n",
    "\n",
    "\n",
    "# D, h, Y_train, Y_test, epsilon, c, X_train, X_test = create_ground_truth(Lu,\n",
    "#                                                                          Ld,\n",
    "#                                                                          m_train,\n",
    "#                                                                          m_test, \n",
    "#                                                                          s, \n",
    "#                                                                          k, \n",
    "#                                                                          K0_max, \n",
    "#                                                                          dictionary_type, \n",
    "#                                                                          sparsity_mode)\n",
    "\n",
    "\n",
    "# # OMP\n",
    "# dd = la.norm(D, axis=0)\n",
    "# W = np.diag(1. / dd)  # Normalization Step\n",
    "# Domp = D @ W\n",
    "# fin_acc = 0\n",
    "# for K0 in range(1, K0_max+1):\n",
    "#     idx = np.sum(np.abs(X_train) > 0, axis=0) == K0  # select all column vectors with certain sparsity (K0 non-null elements)\n",
    "#     tmp_train = Y_train[:, idx].real\n",
    "#     ##########################\n",
    "#     if tmp_train.shape[1]==0:\n",
    "#         continue\n",
    "#     X_true_tmp = X_train[:, idx].real\n",
    "#     idx_group = np.abs(X_true_tmp) > 0\n",
    "#     X_tr = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp.real, x), axis=0, arr=tmp_train)\n",
    "#     idx_train = np.abs(X_tr) > 0\n",
    "#     acc = np.sum(np.sum(idx_group == idx_train, axis=0) == idx_group.shape[0])/idx_group.shape[1]\n",
    "#     if acc < 0.7:\n",
    "#         break\n",
    "#     else:\n",
    "#         fin_acc = acc\n",
    "# # ????????\n",
    "# max_possible_sparsity = K0 - 1\n",
    "\n",
    "np.sum(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(X == X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['K0'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_K0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp['idxs'].iloc[(m_total -1)] == tmp_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for   \n",
    "    hI = 10 / np.max(lambda_max_d_k) * np.random.rand(s, k)\n",
    "    hS = 10 / np.max(lambda_max_u_k) * np.random.rand(s, k)\n",
    "    hH = 10 / np.min([np.max(lambda_max_u_k), np.max(lambda_max_d_k)]) * np.random.rand(s, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.59942778165582"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(hI[0,0]*Ldk[0,:,:] + hS[0,0]*Luk[0,:,:]  + hH[0,0]*np.eye(n,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04318829, 0.00116196],\n",
       "       [0.03547965, 0.04192752],\n",
       "       [0.02791273, 0.01258696]])"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "10 / np.max(lambda_max_d_k) * np.random.rand(s, k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
