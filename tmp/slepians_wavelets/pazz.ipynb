{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from topolearn import *\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from lib import *\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from sklearn.preprocessing import normalize \n",
    "from collections import defaultdict\n",
    "import numpy.linalg as la\n",
    "\n",
    "\n",
    "\n",
    "def compute_error(Y,X):\n",
    "    num_col = Y.shape[1]\n",
    "    error = 0\n",
    "    for col in range(num_col):\n",
    "        error = error + np.linalg.norm(Y[:,col]-X[:,col])**2/np.linalg.norm(Y[:,col])**2\n",
    "    return (1/num_col)*error\n",
    "\n",
    "def nmse(D, X, Y, m):\n",
    "    return (1 / m) * np.sum(la.norm(Y - (D @ X), axis=0) ** 2 / la.norm(Y, axis=0) ** 2)\n",
    "\n",
    "#~~~~~~~~~~~~~~~#\n",
    "# data loading  #\n",
    "#~~~~~~~~~~~~~~~#\n",
    "\n",
    "\n",
    "mat = scipy.io.loadmat('C:\\\\Users\\\\engri\\\\Desktop\\\\tesi\\\\TSP-DictionaryLearning\\\\real_data\\\\data_real.mat')\n",
    "# edge_signals = np.array(mat[\"signal_edge\"].T, dtype=float)\n",
    "# valid_signal = np.where(np.sum(edge_signals, axis=1)!=0)\n",
    "# edge_signals = edge_signals[valid_signal]\n",
    "flow = mat[\"signal_edge\"][:,~np.all(mat[\"signal_edge\"] == 0, axis = 0)]\n",
    "\n",
    "# flow = flow/np.max(flow)\n",
    "# flow = flow[:,10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89, 26)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Coverage: 1\n",
      "Harmonic Component is Present!\n",
      "Saving Directory not Valid!\n",
      "Slepians Rank: 1\n",
      "Dictionary Dimension: (89, 337)\n",
      "Complete Coverage: 1\n",
      "Harmonic Component is Present!\n",
      "Saving Directory not Valid!\n",
      "Slepians Rank: 1\n",
      "Dictionary Dimension: (89, 657)\n",
      "Complete Coverage: 1\n",
      "Harmonic Component is Present!\n",
      "Saving Directory not Valid!\n",
      "Slepians Rank: 1\n",
      "Dictionary Dimension: (89, 1012)\n",
      "defaultdict(<class 'list'>, {'slepians 2': [0.19231625071006472, 0.1166996740022137, 0.0777999663220373, 0.05347708775561865, 0.03712373968055475, 0.026004664369903948], 'slepians 4': [0.1918585488845948, 0.11147942380765813, 0.06246827266434132, 0.03604342086818035, 0.019991805606688926, 0.01141867415204933], 'slepians None': [0.18329278240044805, 0.08427877006293673, 0.040088485789576044, 0.02174060920787337, 0.012342665069467799, 0.00663445313254727], 'fourier': [0.5951997110399401, 0.45669814073853754, 0.342856761093417, 0.2594853541169462, 0.20550531947575137, 0.1635147997154655], 'sep': [0.236961283827108, 0.09334317935136097, 0.044835451851288226, 0.02583641819228394, 0.016845020352066975, 0.010651577909453711]})\n",
      "    slepians 2  slepians 4  slepians None   fourier       sep\n",
      "5     0.192316    0.191859       0.183293  0.595200  0.236961\n",
      "9     0.116700    0.111479       0.084279  0.456698  0.093343\n",
      "13    0.077800    0.062468       0.040088  0.342857  0.044835\n",
      "17    0.053477    0.036043       0.021741  0.259485  0.025836\n",
      "21    0.037124    0.019992       0.012343  0.205505  0.016845\n",
      "25    0.026005    0.011419       0.006634  0.163515  0.010652\n"
     ]
    }
   ],
   "source": [
    "\n",
    "B1 = mat[\"B1\"]\n",
    "B2 = mat[\"B2\"]\n",
    "N0 = B1.shape[0]\n",
    "N1 = B1.shape[1]\n",
    "N2 = B2.shape[1]\n",
    "#B1, B2 = scale_incidence_matrices(B1, B2)\n",
    "#sgn_change = np.diag(np.sign(np.random.randn(N1)))\n",
    "#B1 = B1@sgn_change\n",
    "#B2 = sgn_change@B2\n",
    "L, L1, L2 = hodge_laplacians(B1, B2)\n",
    "\n",
    "\n",
    "w = np.linalg.eigvalsh(L)\n",
    "w1 = np.linalg.eigvalsh(L1)\n",
    "w2 = np.linalg.eigvalsh(L2)\n",
    "\n",
    "L_line = L.copy()\n",
    "L_line = -(np.abs(L_line))\n",
    "np.fill_diagonal(L_line, 0)\n",
    "L_line -= np.diag(np.sum(L_line, axis=0))\n",
    "w_line = np.linalg.eigvalsh(L_line)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# Wavelet, Fourier and Slepians making #\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "W_fourier = FourierBasis(B1, B2)\n",
    "W_joint = JointHodgelet(B1, B2,\n",
    "                        *log_wavelet_kernels_gen(3, 4, np.log(np.max(w))))\n",
    "W_sep = SeparateHodgelet(B1, B2,\n",
    "                         *log_wavelet_kernels_gen(3, 4, np.log(np.max(w1))),\n",
    "                         *log_wavelet_kernels_gen(3, 4, np.log(np.max(w2))))\n",
    "W_lift = LiftedHodgelet(B1, B2,\n",
    "                        *log_wavelet_kernels_gen(3, 4, np.log(np.max(w1))),\n",
    "                        *log_wavelet_kernels_gen(3, 4, np.log(np.max(w2))))\n",
    "W_lift_mixed = MixedLiftedHodgelet(B1, B2,\n",
    "                                   *log_wavelet_kernels_gen(3, 4, np.log(np.max(w1))),\n",
    "                                   *log_wavelet_kernels_gen(3, 4, np.log(np.max(w2))))\n",
    "\n",
    "W_fourier_line = LaplaceFourierBasis(L_line)\n",
    "W_line = JointLaplacelet(L_line,\n",
    "                         *log_wavelet_kernels_gen(3, 4, np.log(np.max(w_line))))\n",
    "\n",
    "\n",
    "option = \"One-shot-diffusion\"#\"One-shot-diffusion\"\n",
    "F_sol,F_irr = get_frequency_mask(B1,B2) # Get frequency bands\n",
    "diff_order_sol= 1\n",
    "diff_order_irr = 1\n",
    "step_prog = 1\n",
    "source_sol = np.ones((N1,))\n",
    "source_irr = np.ones((N1,))\n",
    "S_neigh, complete_coverage = cluster_on_neigh(B1,B2,diff_order_sol,diff_order_irr,source_sol,source_irr,option,step_prog)\n",
    "R = [F_sol, F_irr]\n",
    "S = S_neigh\n",
    "top_K_coll = [2,4, None]\n",
    "spars_level = np.arange(5, 26, 4)\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# orthogonal matching pursuit #\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "error_data = defaultdict(list)    \n",
    "for top_K_slep in top_K_coll:\n",
    "    print(\"Complete Coverage: \"+str(complete_coverage))\n",
    "    W_slepians = SimplicianSlepians(B1, B2, S, R, top_K = top_K_slep, save_dir = res_dir, load_dir = res_dir)\n",
    "    print(\"Slepians Rank: \"+str(W_slepians.full_rank))\n",
    "    print(\"Dictionary Dimension: \"+str(W_slepians.atoms_flat.shape))\n",
    "    slepians_noisy = [W_slepians.omp(flow, k = spars)\n",
    "                            for spars in spars_level]\n",
    "    slepians_error = [compute_error(flow, W_slepians.atoms_flat@spars.coef_.T)\n",
    "                            for spars in slepians_noisy]\n",
    "    error_data[\"slepians \"+str(top_K_slep)]= slepians_error\n",
    "\n",
    "fourier_noisy = [W_fourier.omp(flow, k = spars)\n",
    "                        for spars in spars_level]\n",
    "fourier_error = [compute_error(flow, W_fourier.atoms_flat@spars.coef_.T)\n",
    "                                for spars in fourier_noisy]\n",
    "\n",
    "sep_noisy = [W_sep.omp(flow, k = spars)\n",
    "                        for spars in spars_level]\n",
    "sep_error = [compute_error(flow, W_sep.atoms_flat@spars.coef_.T)\n",
    "                                for spars in sep_noisy]\n",
    "\n",
    "error_data['fourier']=fourier_error\n",
    "error_data['sep']=sep_error\n",
    "\"\"\"   \n",
    "error_data = {**error_data,**complete_coll_error}\n",
    "for key in error_data.keys():\n",
    "    error_data[key] = np.mean(np.array(error_data[key]),axis = 0)\n",
    "\"\"\"\n",
    "print(error_data)\n",
    "error_df = pd.DataFrame(error_data,\n",
    "                            columns=list(error_data.keys()),\n",
    "                            index=spars_level)\n",
    "print(error_df)\n",
    "#error_df.to_csv(f'{res_dir}/error_snr_'+str(snr)+'.csv', float_format='%0.4f', index_label='err', sep = \";\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40, 50, 60, 70]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spars_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "res_dir = \"/Users/Claudio/Desktop/Topological_Slepians/Results\"\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "# setup - data creation function #\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\n",
    "\n",
    "def hexgrid(n=15):\n",
    "\n",
    "    xs = np.linspace(-2,2,n)\n",
    "    ys = np.sqrt(3)*np.linspace(-2,2,n)\n",
    "    \n",
    "    xint = (xs[1]-xs[0])/2\n",
    "    \n",
    "    x1, y1 = np.meshgrid(xs,ys)\n",
    "    x1[::2] += xint\n",
    "    x1 = x1.flatten()\n",
    "    y1 = y1.flatten()\n",
    "    \n",
    "    coords = np.vstack([x1,y1]).T\n",
    "\n",
    "    # sort coords\n",
    "    diagonal_coordinates = np.sum(coords, axis=1)  # y = -x + c, compute c\n",
    "    diagonal_idxs = np.argsort(diagonal_coordinates)  # sort by c: origin comes first, upper-right comes last\n",
    "    coords = coords[diagonal_idxs]  # apply sort to original coordinates\n",
    "    \n",
    "    tri = Delaunay(coords)\n",
    "    simplices = [sorted(f) for f in tri.simplices]\n",
    "    \n",
    "    edges = []\n",
    "    for f in simplices:\n",
    "        [a,b,c] = sorted(f)\n",
    "        edges.append((a,b))\n",
    "        edges.append((b,c))\n",
    "        edges.append((a,c))\n",
    "    edges = sorted(set(edges))\n",
    "    edges_tup = [tuple(e) for e in edges]\n",
    "\n",
    "    edge_idx_dict = {edges_tup[i]: i for i in range(len(edges_tup))}\n",
    "\n",
    "    nodes = list(range(len(coords)))\n",
    "\n",
    "    graph = nx.DiGraph()\n",
    "    graph.add_nodes_from(nodes)\n",
    "    graph.add_edges_from(edges)\n",
    "\n",
    "    return graph, nodes, edges_tup, simplices, coords, edge_idx_dict\n",
    "\n",
    "def incidence_matrices(G, V, E, faces, edge_to_idx):\n",
    "    # redefined from nblib.py\n",
    "    B1 = np.array(nx.incidence_matrix(G, nodelist=V, edgelist=E, oriented=True).todense())\n",
    "    B2 = np.zeros([len(E),len(faces)])\n",
    "\n",
    "    for f_idx, face in enumerate(faces): # face is sorted\n",
    "        edges = [face[:-1], face[1:], [face[0], face[2]]]\n",
    "        e_idxs = [edge_to_idx[tuple(e)] for e in edges]\n",
    "\n",
    "        B2[e_idxs[:-1], f_idx] = 1\n",
    "        B2[e_idxs[-1], f_idx] = -1\n",
    "    return B1, B2\n",
    "\n",
    "def discrete_flow(E, coords):\n",
    "\n",
    "    flow_dict = {}\n",
    "\n",
    "    A = np.array([[0.5, -1/(4*np.sqrt(3))],[1/(4*np.sqrt(3)), 0.5]])\n",
    "\n",
    "    for e in E:\n",
    "        j0,j1 = e\n",
    "        p0 = coords[j0]\n",
    "        p1 = coords[j1]\n",
    "\n",
    "        clipradius = 0.7\n",
    "        if (np.linalg.norm(0.5*(p0+p1)-np.array([np.pi/4,np.pi/4]))>clipradius) and \\\n",
    "           (np.linalg.norm(0.5*(p0+p1)-np.array([-np.pi/4,-np.pi/4]))>clipradius):\n",
    "            flow_dict[e] = 0\n",
    "            continue\n",
    "\n",
    "        d = p1 - p0\n",
    "\n",
    "        q0 = p0 + A @ d\n",
    "        q1 = p0 + A.T @ d\n",
    "\n",
    "        # black magic: F = [cos(x+y), sin(x-y)]\n",
    "        # I integrate perpendicular to the edges between hexagons\n",
    "        f = ((q0[1]-q1[1])/((q1[0]+q1[1])-(q0[0]+q0[1])) * (np.sin(q1[0]+q1[1])-np.sin(q0[0]+q0[1])) -\n",
    "             (q0[0]-q1[0])/((q1[0]-q1[1])-(q0[0]-q0[1])) * (np.cos(q1[0]-q1[1])-np.cos(q0[0]-q0[1])))\n",
    "\n",
    "        flow_dict[e] = f\n",
    "\n",
    "    return flow_dict\n",
    "\n",
    "#~~~~~~~~~~~~~~~#\n",
    "# data building #\n",
    "#~~~~~~~~~~~~~~~#\n",
    "G, V, E, F, coords, edge_idx_dict = hexgrid()\n",
    "N0 = len(V)\n",
    "N1 = len(E)\n",
    "N2 = len(F)\n",
    "B1, B2 = incidence_matrices(G, V, E, F, edge_idx_dict)\n",
    "B1, B2 = scale_incidence_matrices(B1, B2)\n",
    "sgn_change = np.diag(np.sign(np.random.randn(N1)))\n",
    "B1 = B1 @ sgn_change\n",
    "B2 = sgn_change @ B2\n",
    "L, L1, L2 = hodge_laplacians(B1, B2)\n",
    "\n",
    "# Generate Signal\n",
    "# compute a mapping from edges to the reals\n",
    "flow_dict = discrete_flow(E, coords)\n",
    "# then use the indexing to write as a vector in R^E\n",
    "flow2 = np.zeros(N1)\n",
    "for e in E:\n",
    "    flow2[edge_idx_dict[e]] = flow_dict[e]\n",
    "flow2 /= np.linalg.norm(flow2)\n",
    "\n",
    "w = np.linalg.eigvalsh(L)\n",
    "w1 = np.linalg.eigvalsh(L1)\n",
    "w2 = np.linalg.eigvalsh(L2)\n",
    "\n",
    "L_line = L.copy()\n",
    "L_line = -(np.abs(L_line))\n",
    "np.fill_diagonal(L_line, 0)\n",
    "L_line -= np.diag(np.sum(L_line, axis=0))\n",
    "w_line = np.linalg.eigvalsh(L_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mengri\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtesi\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTSP-DictionaryLearning\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mtsplearn\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsplearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "prob_T=0.8\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p_edges=0.162, p_triangles=prob_T, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "T = int(np.ceil(nu*(1-prob_T)))\n",
    "\n",
    "# Laplacians\n",
    "Lu, Ld, L = G.get_laplacians(sub_size=100)\n",
    "Lu_full = G.get_laplacians(sub_size=100, full=True)\n",
    "B2_true = B2@G.mask\n",
    "M =  L.shape[0]\n",
    "\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type=\"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "P = 3 # Number of Kernels (Sub-dictionaries)\n",
    "J = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(M*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "n_search = 3000\n",
    "n_sim = 10\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 100 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True\n",
    "\n",
    "gen_params={'dictionary_type':dictionary_type,\n",
    "        'm_train':m_train,\n",
    "        'm_test':m_test,\n",
    "        'P':P,\n",
    "        'M':M,\n",
    "        'J':J,\n",
    "        'sparsity':sparsity,\n",
    "        'K0_max':K0_max,\n",
    "        'sparsity_mode':sparsity_mode,\n",
    "        'n_search':n_search,\n",
    "        'n_sim':n_sim,\n",
    "        'prob_T':prob_T}\n",
    "\n",
    "load_data = generate_data(Lu, Ld, **gen_params)\n",
    "\n",
    "D_true = load_data['D_true']\n",
    "Y_train = load_data['Y_train']\n",
    "Y_test = load_data['Y_test']\n",
    "X_train = load_data['X_train']\n",
    "X_test = load_data['X_test']\n",
    "epsilon_true =  load_data['epsilon_true']\n",
    "c_true = load_data['c_true']\n",
    "\n",
    "s = 8\n",
    "c = c_true[s]  \n",
    "epsilon = epsilon_true[s] \n",
    "k0 = K0_coll[4]\n",
    "\n",
    "topo_params = {\"K0\":K0_coll[4],\n",
    "               \"J\":J,\n",
    "               \"P\":P,\n",
    "               \"true_prob_T\":prob_T,\n",
    "               \"sub_size\":100,\n",
    "               \"dictionary_type\":dictionary_type,\n",
    "               \"c\":c_true[s],\n",
    "               \"epsilon\":epsilon_true[s],\n",
    "               \"seed\":0,\n",
    "               \"n\":40,\n",
    "               \"p_edges\":0.162\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from typing import Tuple, List, Union, Dict\n",
    "import pickle\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.tsp_generation import *\n",
    "from typing import Tuple, List, Union, Dict\n",
    "import pickle\n",
    "from functools import wraps\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def _indicator_matrix(row):\n",
    "    tmp = row.sigma.copy()\n",
    "    tmp[row.idx] = 0\n",
    "    return np.diag(tmp)\n",
    "\n",
    "def _indicator_matrix_rev(row):\n",
    "    tmp = row.sigma.copy()\n",
    "    tmp[row.idx] = 1\n",
    "    return np.diag(tmp)\n",
    "\n",
    "def _compute_Luj(row, b2, J):\n",
    "    Lu = b2 @ row.sigma @ b2.T\n",
    "    Luj = np.array([la.matrix_power(Lu, i) for i in range(1, J + 1)])\n",
    "    return Luj\n",
    "\n",
    "def _split_coeffs(h ,s ,k, sep=False):\n",
    "    h_tmp = h.value.flatten()\n",
    "    # hH = h_tmp[:s,].reshape((s,1))\n",
    "    # hS = h_tmp[s:s*(k+1),].reshape((s,k))\n",
    "    # hI = h_tmp[s*(k+1):,].reshape((s,k))\n",
    "    if sep:\n",
    "        hH = h_tmp[np.arange(0, (s*(2*k+1)), (2*k+1))].reshape((s,1))\n",
    "        hS = h_tmp[np.hstack([[i,i+1] for i in range(1, (s*(2*k+1)), (2*k+1))])].reshape((s,k))\n",
    "        hI = h_tmp[np.hstack([[i,i+1] for i in range((k+1), (s*(2*k+1)), (2*k+1))])].reshape((s,k))\n",
    "        return [hH, hS, hI]\n",
    "    h = h_tmp[:s*k]\n",
    "    hi = h_tmp[s*k:]\n",
    "    return [h, hi]\n",
    "    \n",
    "def sparse_transform(D, K0, Y_te, Y_tr=None):\n",
    "\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)\n",
    "    Domp = D @ W\n",
    "    X_te = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_te)\n",
    "    # Normalization\n",
    "    X_te = W @ X_te\n",
    "\n",
    "    if np.all(Y_tr == None):\n",
    "\n",
    "        return X_te\n",
    "    \n",
    "    # Same for the training set\n",
    "    X_tr = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_tr)\n",
    "    X_tr = W @ X_tr\n",
    "    \n",
    "    return X_te, X_tr\n",
    "\n",
    "\n",
    "def compute_vandermonde(L, k):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B\n",
    "\n",
    "\n",
    "def nmse(D, X, Y, m):\n",
    "    return (1/m)* np.sum(la.norm(Y - (D @ X), axis=0)**2 /la.norm(Y, axis=0)**2)\n",
    "\n",
    "\n",
    "class TspSolver:\n",
    "\n",
    "    def __init__(self, X_train, X_test, Y_train, Y_test, *args, **kwargs):\n",
    "\n",
    "        params = {\n",
    "                'P': None,      # Number of Kernels (Sub-dictionaries)\n",
    "                'J': None,      # Polynomial order\n",
    "                'K0': None,     # Sparsity level\n",
    "                'dictionary_type': None,\n",
    "                'c': None,      # spectral control parameter \n",
    "                'epsilon': None,# spectral control parameter\n",
    "                'n': 10,        # number of nodes\n",
    "                'sub_size': None,   # Number of sub-sampled nodes\n",
    "                'prob_T': 1.,   # Ratio of colored triangles\n",
    "                'true_prob_T': 1.,   # True ratio of colored triangles\n",
    "                'p_edges': 1.,  # Probability of edge existence\n",
    "                'seed': None\n",
    "                }\n",
    "        \n",
    "        if args:\n",
    "            if len(args) != 1 or not isinstance(args[0], dict):\n",
    "                raise ValueError(\"When using positional arguments, must provide a single dictionary\")\n",
    "            params.update(args[0])\n",
    "\n",
    "        params.update(kwargs)\n",
    "\n",
    "        # Data\n",
    "        self.X_train: np.ndarray = X_train\n",
    "        self.X_test: np.ndarray = X_test\n",
    "        self.Y_train: np.ndarray = Y_train\n",
    "        self.Y_test: np.ndarray = Y_test\n",
    "        self.m_train: int = Y_train.shape[1]\n",
    "        self.m_test: int = Y_test.shape[1]\n",
    "\n",
    "        # Topology and geometry behind data\n",
    "        self.G = EnhancedGraph(n=params['n'],\n",
    "                               p_edges=params['p_edges'], \n",
    "                               p_triangles=params['prob_T'], \n",
    "                               seed=params['seed'])\n",
    "        self.B1: np.ndarray = self.G.get_b1()\n",
    "        self.B2: np.ndarray = self.G.get_b2()\n",
    "\n",
    "        # Sub-sampling if needed to decrease complexity\n",
    "        if params['sub_size'] != None:\n",
    "            self.B1 = self.B1[:, :params['sub_size']]\n",
    "            self.B2 = self.B2[:params['sub_size'], :]\n",
    "            self.B2 = self.B2[:,np.sum(np.abs(self.B2), 0) == 3]\n",
    "        self.nu: int = self.B2.shape[1]\n",
    "        self.nd: int = self.B1.shape[1]\n",
    "        self.true_prob_T = params['true_prob_T']\n",
    "        self.T: int = int(np.ceil(self.nu*(1-params['prob_T'])))\n",
    "\n",
    "        # Laplacians\n",
    "        Lu, Ld, L = self.G.get_laplacians(sub_size=params['sub_size'])\n",
    "        self.Lu: np.ndarray = Lu\n",
    "        self.Ld: np.ndarray = Ld\n",
    "        self.L: np.ndarray = L\n",
    "        self.Lu_full: np.ndarray = G.get_laplacians(sub_size=params['sub_size'], \n",
    "                                                    full=True)\n",
    "        self.M =  L.shape[0]\n",
    "        self.history: List[np.ndarray] = []\n",
    "\n",
    "        # Dictionary, parameters and hyperparameters for compression\n",
    "        self.P = params['P']\n",
    "        self.J = params['J']\n",
    "        self.c = params['c']\n",
    "        self.epsilon = params['epsilon']\n",
    "        self.K0 = params['K0']\n",
    "        self.dictionary_type = params['dictionary_type']\n",
    "        self.D_opt: np.ndarray = np.zeros((self.M, self.M*self.P))\n",
    "\n",
    "        if self.dictionary_type==\"separated\":\n",
    "            hs = np.zeros((self.P,self.J))\n",
    "            hi = np.zeros((self.P,self.J))\n",
    "            hh = np.zeros((self.P,1))\n",
    "            self.h_opt: List[np.ndarray] = [hh,hs,hi]\n",
    "        else:\n",
    "            h = np.zeros((self.P, self.J))\n",
    "            hI = np.zeros((self.P, 1))\n",
    "            self.h_opt: np.ndarray = [h, hI]\n",
    "            \n",
    "        self.X_opt_train: np.ndarray = np.zeros(self.X_train.shape)\n",
    "        self.X_opt_test: np.ndarray = np.zeros(self.X_test.shape)\n",
    "\n",
    "        if self.dictionary_type == \"joint\":\n",
    "            self.Lj, self.lambda_max_j, self.lambda_min_j = compute_Lj_and_lambdaj(self.L, self.J)\n",
    "            self.B = compute_vandermonde(self.L, self.J).real\n",
    "        elif self.dictionary_type == \"edge_laplacian\":\n",
    "            self.Lj, self.lambda_max_j, self.lambda_min_j = compute_Lj_and_lambdaj(self.Ld, self.J)\n",
    "            self.B = compute_vandermonde(self.Ld, self.J).real\n",
    "        elif  self.dictionary_type == 'separated':\n",
    "            self.Luj, self.lambda_max_u_j, self.lambda_min_u_j = compute_Lj_and_lambdaj(self.Lu, self.J, separated=True)\n",
    "            self.Ldj, self.lambda_max_d_j, self.lambda_min_d_j = compute_Lj_and_lambdaj(self.Ld, self.J, separated=True)\n",
    "            self.Bu = compute_vandermonde(self.Lu, self.J).real\n",
    "            self.Bd = compute_vandermonde(self.Ld, self.J)[:, 1:].real\n",
    "            self.B = np.hstack([self.Bu, self.Bd])\n",
    "\n",
    "        self.P_aux: np.ndarray = None\n",
    "\n",
    "        # Init the learning errors\n",
    "        self.min_error_train = 1e20\n",
    "        self.min_error_test = 1e20\n",
    "\n",
    "    # def fit(self) -> Tuple[float, List[np.ndarray], np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    #     min_error_test, _, _, h_opt, X_opt_test, D_opt = self.learn_upper_laplacian()\n",
    "    #     return min_error_test, self.history, params['Lu'], h_opt, X_opt_test, D_opt\n",
    "\n",
    "    def update_Lu(self, Lu_new):\n",
    "        self.Lu = Lu_new\n",
    "        self.Luj, self.lambda_max_u_j, self.lambda_min_u_j = compute_Lj_and_lambdaj(self.Lu, \n",
    "                                                                                    self.J, \n",
    "                                                                                    separated=True)\n",
    "        self.Bu = compute_vandermonde(self.Lu, self.J).real\n",
    "        self.B = np.hstack([self.Bu, self.Bd])\n",
    "\n",
    "    @staticmethod\n",
    "    def _multiplier_search(*arrays, P, c, epsilon):\n",
    "        is_okay = 0\n",
    "        mult = 100\n",
    "        tries = 0\n",
    "        while is_okay==0:\n",
    "            is_okay = 1\n",
    "            h, c_try, _, tmp_sum_min, tmp_sum_max = generate_coeffs(arrays, P=P, mult=mult)\n",
    "            if c_try <= c:\n",
    "                is_okay *= 1\n",
    "            if tmp_sum_min > c-epsilon:\n",
    "                is_okay *= 1\n",
    "                incr_mult = 0\n",
    "            else:\n",
    "                is_okay = is_okay*0\n",
    "                incr_mult = 1\n",
    "            if tmp_sum_max < c+epsilon:\n",
    "                is_okay *= 1\n",
    "                decr_mult = 0\n",
    "            else:\n",
    "                is_okay *= 0\n",
    "                decr_mult = 1\n",
    "            if is_okay == 0:\n",
    "                tries += 1\n",
    "            if tries >3:\n",
    "                discard = 1\n",
    "                break\n",
    "            if incr_mult == 1:\n",
    "                mult *= 2\n",
    "            if decr_mult == 1:\n",
    "                mult /= 2\n",
    "        return h, discard\n",
    "\n",
    "    def init_dict(self,\n",
    "                  h_prior: np.ndarray = None, \n",
    "                  mode: str = \"only_X\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Initialize the dictionary and the signal sparse representation for the alternating\n",
    "        optimization algorithm.\n",
    "\n",
    "        Args:\n",
    "            Lu (np.ndarray): Upper Laplacian matrix\n",
    "            Ld (np.ndarray): Lower Laplacian matrix\n",
    "            P (int): Number of kernels (sub-dictionaries).\n",
    "            J (int): Max order of the polynomial for the single sub-dictionary.\n",
    "            Y_train (np.ndarray): Training data.\n",
    "            K0 (int): Sparsity of the signal representation.\n",
    "            dictionary_type (str): Type of dictionary.\n",
    "            c (float): Boundary constant from the synthetic data generation process.\n",
    "            epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "            only (str): Type of initialization. Can be one of: \"only_X\", \"all\", \"only_D\".\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, bool]: Initialized dictionary, initialized sparse representation, and discard flag value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If no prior info on the dictionary\n",
    "        if np.all(h_prior == None):\n",
    "\n",
    "            if (mode in [\"all\",\"only_D\"]):\n",
    "\n",
    "                discard = 1\n",
    "                while discard==1:\n",
    "\n",
    "                    if dictionary_type != \"separated\":\n",
    "                        h_prior, discard = self._multiplier_search(self.lambda_max_j, \n",
    "                                                              self.lambda_min_j, \n",
    "                                                              P=self.P, \n",
    "                                                              c=self.c, \n",
    "                                                              epsilon=self.epsilon)\n",
    "                        self.D_opt = generate_dictionary(h_prior, \n",
    "                                                         self.P, \n",
    "                                                         self.Lj)\n",
    "\n",
    "                    else:\n",
    "                        h_prior, discard = self._multiplier_search(self.lambda_max_d_j, \n",
    "                                                              self.lambda_min_d_j, \n",
    "                                                              self.lambda_max_u_j, \n",
    "                                                              self.lambda_min_u_j,\n",
    "                                                              P=self.P, \n",
    "                                                              c=self.c, \n",
    "                                                              epsilon=self.epsilon)\n",
    "                        self.D_opt = generate_dictionary(h_prior, \n",
    "                                                         self.P, \n",
    "                                                         self.Luj, \n",
    "                                                         self.Ldj)\n",
    "        \n",
    "            if (mode in [\"all\",\"only_X\"]):\n",
    "                \n",
    "                L = self.Ld if self.dictionary_type == \"edge_laplacian\" else self.L\n",
    "                _, Dx = sla.eig(L)\n",
    "                dd = la.norm(Dx, axis=0)\n",
    "                W = np.diag(1./dd)\n",
    "                Dx = Dx / la.norm(Dx)  \n",
    "                Domp = Dx@W\n",
    "                X = np.apply_along_axis(lambda x: get_omp_coeff(self.K0, Domp.real, x), axis=0, arr=self.Y_train)\n",
    "                X = np.tile(X, (self.P,1))\n",
    "                self.X_opt_train = X\n",
    "\n",
    "        # Otherwise use prior info about the dictionary to initialize the sparse representation\n",
    "        else:\n",
    "            \n",
    "            self.h_opt = h_prior\n",
    "\n",
    "            if dictionary_type == \"separated\":\n",
    "                self.D_opt = generate_dictionary(h_prior, \n",
    "                                                 self.P, \n",
    "                                                 self.Luj, \n",
    "                                                 self.Ldj)\n",
    "                self.X_opt_train = sparse_transform(self.D_opt, \n",
    "                                                    self.K0, \n",
    "                                                    self.Y_train)\n",
    "            else: \n",
    "                self.D_opt = generate_dictionary(h_prior, \n",
    "                                                 self.P, \n",
    "                                                 self.Lj)\n",
    "                self.X_opt_train = sparse_transform(self.D_opt, \n",
    "                                                    self.K0, \n",
    "                                                    self.Y_train)             \n",
    "\n",
    "   \n",
    "    def save_results(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "\n",
    "            outputs = func(self, *args, **kwargs)\n",
    "            func_name = func.__name__\n",
    "\n",
    "            if func_name == \"topological_dictionary_learn\":\n",
    "\n",
    "                path = os.getcwd()\n",
    "                dir_path = os.path.join(path, \n",
    "                                        'results', \n",
    "                                        'dictionary_learning',\n",
    "                                        f'{self.dictionary_type}')\n",
    "                name = f'learn_D_{self.dictionary_type}'\n",
    "                filename = os.path.join(dir_path, f'{name}.pkl')\n",
    "                save_var = {\"min_error_test\": self.min_error_test,\n",
    "                            \"min_error_train\": self.min_error_train,\n",
    "                            \"history\": outputs[2],\n",
    "                            \"h_opt\": self.h_opt,\n",
    "                            \"X_opt_test\": self.X_opt_test,\n",
    "                            \"X_opt_train\": self.X_opt_train,\n",
    "                            \"D_opt\": self.D_opt}\n",
    "                \n",
    "            elif func_name == \"learn_upper_laplacian\":\n",
    "\n",
    "                path = os.getcwd()\n",
    "                dir_path = os.path.join(path, 'results', 'topology_learning')\n",
    "                name = f'learn_T{int(self.true_prob_T*100)}'\n",
    "                filename = os.path.join(dir_path, f'{name}.pkl')\n",
    "                save_var = {\"min_error_test\": self.min_error_test,\n",
    "                            \"min_error_train\": self.min_error_train,\n",
    "                            \"history\": self.history,\n",
    "                            \"Lu_opt\": self.Lu,\n",
    "                            \"h_opt\": self.h_opt,\n",
    "                            \"X_opt_test\": self.X_opt_test,\n",
    "                            \"X_opt_train\": self.X_opt_train,\n",
    "                            \"D_opt\": self.D_opt}\n",
    "\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "\n",
    "            try:\n",
    "                with open(filename, 'wb') as file:\n",
    "                    pickle.dump(save_var, file)\n",
    "            except IOError as e:\n",
    "                print(f\"An error occurred while writing the file: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "            return outputs  \n",
    "        return wrapper\n",
    "\n",
    "    def topological_dictionary_learn(self,\n",
    "                                     lambda_: float = 1e-3, \n",
    "                                     max_iter: int = 10, \n",
    "                                     patience: int = 10,\n",
    "                                     tol: float = 1e-7,\n",
    "                                     step_h: float = 1.,\n",
    "                                     step_x: float = 1.,\n",
    "                                     solver: str =\"MOSEK\", \n",
    "                                     verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
    "        \"\"\"\n",
    "        Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "        The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "        for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "        the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "        Args:\n",
    "            Y_train (np.ndarray): Training data.\n",
    "            Y_test (np.ndarray): Testing data.\n",
    "            J (int): Max order of the polynomial for the single sub-dictionary.\n",
    "            M (int): Number of data points (number of nodes in the data graph).\n",
    "            P (int): Number of kernels (sub-dictionaries).\n",
    "            D0 (np.ndarray): Initial dictionary.\n",
    "            X0 (np.ndarray): Initial sparse representation.\n",
    "            Lu (np.ndarray): Upper Laplacian matrix\n",
    "            Ld (np.ndarray): Lower Laplacian matrix\n",
    "            dictionary_type (str): Type of dictionary.\n",
    "            c (float): Boundary constant from the synthetic data generation process.\n",
    "            epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "            K0 (int): Sparsity of the signal representation.\n",
    "            lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "            max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "            patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "            tol (float, optional): Tolerance value. Defaults to 1e-s.\n",
    "            verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "            minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define hyperparameters\n",
    "        iter_, pat_iter = 1, 0\n",
    "        hist = []\n",
    "\n",
    "        if self.dictionary_type != \"fourier\":\n",
    "\n",
    "            # Init the dictionary and the sparse representation \n",
    "            D_coll = [cp.Constant(self.D_opt[:,(self.M*i):(self.M*(i+1))]) for i in range(self.P)]\n",
    "            Dsum = cp.Constant(np.zeros((self.M, self.M)))\n",
    "            h_opt = self.h_opt\n",
    "            Y = cp.Constant(self.Y_train)\n",
    "            X_tr = self.X_opt_train\n",
    "            X_te = self.X_opt_test\n",
    "            I = cp.Constant(np.eye(self.M))\n",
    "            \n",
    "            while pat_iter < patience and iter_ <= max_iter:\n",
    "                \n",
    "                # SDP Step\n",
    "                X = cp.Constant(X_tr)\n",
    "                if iter_ != 1:\n",
    "                    D_coll = [cp.Constant(D[:,(self.M*i):(self.M*(i+1))]) for i in range(self.P)]\n",
    "                    Dsum = cp.Constant(np.zeros((self.M, self.M)))\n",
    "                \n",
    "                # Define the objective function\n",
    "                if self.dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                    # Init the variables\n",
    "                    h = cp.Variable((self.P, self.J))\n",
    "                    hI = cp.Variable((self.P, 1))\n",
    "                    h.value, hI.value = h_opt\n",
    "                    for i in range(0,self.P):\n",
    "                        tmp =  cp.Constant(np.zeros((self.M, self.M)))\n",
    "                        for j in range(0,self.J):\n",
    "                            tmp += (cp.Constant(self.Lj[j, :, :]) * h[i,j])\n",
    "                        tmp += (I*hI[i])\n",
    "                        D_coll[i] = tmp\n",
    "                        Dsum += tmp\n",
    "                    D = cp.hstack([D_coll[i]for i in range(self.P)])\n",
    "                    term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                    term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                    term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                    obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "                else:\n",
    "                    # Init the variables\n",
    "                    hI = cp.Variable((self.P, self.J))\n",
    "                    hS = cp.Variable((self.P, self.J))\n",
    "                    hH = cp.Variable((self.P, 1))\n",
    "                    hH.value, hS.value, hI.value = h_opt ##################### OCCHIO\n",
    "                    for i in range(0,self.P):\n",
    "                        tmp =  cp.Constant(np.zeros((self.M, self.M)))\n",
    "                        for j in range(0,self.J):\n",
    "                            tmp += ((cp.Constant(self.Luj[j, :, :])*hS[i,j]) + (cp.Constant(self.Ldj[j, :, :])*hI[i,j]))\n",
    "                        tmp += (I*hH[i])\n",
    "                        D_coll[i] = tmp\n",
    "                        Dsum += tmp\n",
    "                    D = cp.hstack([D_coll[i] for i in range(self.P)])\n",
    "        \n",
    "                    term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                    term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                    term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                    term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                    obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "                # Define the constraints\n",
    "                constraints = [D_coll[i] >> 0 for i in range(self.P)] + \\\n",
    "                                [(cp.multiply(self.c, I) - D_coll[i]) >> 0 for i in range(self.P)] + \\\n",
    "                                [(Dsum - cp.multiply((self.c - self.epsilon), I)) >> 0, (cp.multiply((self.c + self.epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "                prob = cp.Problem(obj, constraints)\n",
    "                prob.solve(solver=eval(f'cp.{solver}'), verbose=False)\n",
    "\n",
    "                # Dictionary Update\n",
    "                D = D.value\n",
    "                if self.dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                    h_opt = [h_opt[0] + step_h*(h.value - h_opt[0]),\n",
    "                             h_opt[1] + step_h*(hI.value - h_opt[1])]\n",
    "                else:\n",
    "                    h_opt = [h_opt[0] + step_h*(hH.value-h_opt[0]),\n",
    "                             h_opt[1] + step_h*(hS.value-h_opt[1]), \n",
    "                             h_opt[2] + step_h*(hI.value-h_opt[2])]\n",
    "\n",
    "                # OMP Step\n",
    "                X_te_tmp, X_tr_tmp = sparse_transform(D, self.K0, self.Y_test, self.Y_train)\n",
    "                # Sparse Representation Update\n",
    "                X_tr = X_tr + step_x*(X_tr_tmp - X_tr)\n",
    "                X_te = X_te + step_x*(X_te_tmp - X_te)\n",
    "\n",
    "                # Error Update\n",
    "                error_train = nmse(D, X_tr, self.Y_train, self.m_train)\n",
    "                error_test = nmse(D, X_te, self.Y_test, self.m_test)\n",
    "\n",
    "                hist.append(error_test)\n",
    "                \n",
    "                # Error Storing\n",
    "                if (error_train < self.min_error_train) and (abs(error_train) > np.finfo(float).eps) and (abs(error_train - self.min_error_train) > tol):\n",
    "                    self.X_opt_train = X_tr\n",
    "                    self.min_error_train = error_train\n",
    "\n",
    "                if (error_test < self.min_error_test) and (abs(error_test) > np.finfo(float).eps) and (abs(error_test - self.min_error_test) > tol):\n",
    "                    self.h_opt = h_opt\n",
    "                    self.D_opt = D\n",
    "                    self.X_opt_test = X_te\n",
    "                    self.min_error_test = error_test\n",
    "                    pat_iter = 0\n",
    "\n",
    "                    if verbose == 1:\n",
    "                        print(\"New Best Test Error:\", self.min_error_test)\n",
    "                else:\n",
    "                    pat_iter += 1\n",
    "\n",
    "                iter_ += 1\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # Fourier Dictionary Benchmark\n",
    "            _, self.D_opt = sla.eigh(self.L)\n",
    "            self.X_opt_test, self.X_opt_train = sparse_transform(self.D_opt, self.K0, self.Y_test, self.Y_train)\n",
    "\n",
    "            # Error Updating\n",
    "            self.min_error_train = nmse(self.D_opt, self.X_opt_train, self.Y_train, self.m_train)\n",
    "            self.min_error_test= nmse(self.D_opt, self.X_opt_test, self.Y_test, self.m_test)\n",
    "            \n",
    "        return self.min_error_test, self.min_error_train, hist\n",
    "    \n",
    "\n",
    "    def _aux_matrix_update(self, X):\n",
    "\n",
    "        I = [np.eye(self.M)]\n",
    "        if self.dictionary_type==\"separated\":\n",
    "            LLu = [lu for lu in self.Luj]\n",
    "            LLd = [ld for ld in self.Ldj]\n",
    "            LL = np.array(I+LLu+LLd)\n",
    "        else:\n",
    "            LL = np.array(I + [l for l in self.Lj])\n",
    "\n",
    "        P_aux = np.array([LL@X[(i*self.M): ((i+1)*self.M), :] for i in range(self.P)])\n",
    "        self.P_aux = rearrange(P_aux, 'b h w c -> (b h) w c')\n",
    "    \n",
    "    def topological_dictionary_learn_qp(self,\n",
    "                                        lambda_: float = 1e-3, \n",
    "                                        max_iter: int = 10, \n",
    "                                        patience: int = 10,\n",
    "                                        tol: float = 1e-7,\n",
    "                                        solver: str = 'MOSEK',\n",
    "                                        step_h: float = 1.,\n",
    "                                        step_x: float = 1.,\n",
    "                                        verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        iter_, pat_iter = 1, 0\n",
    "        hist = []\n",
    "\n",
    "        if self.dictionary_type != \"fourier\":\n",
    "        \n",
    "            # Init the the sparse representation\n",
    "            h_opt = np.hstack([h.flatten() for h in self.h_opt]).reshape(-1,1)\n",
    "            X_tr = self.X_opt_train\n",
    "            X_te = self.X_opt_test\n",
    "            reg = lambda_ * np.eye(self.P*(2*self.J+1))\n",
    "            I_s = cp.Constant(np.eye(self.P))\n",
    "            i_s = cp.Constant(np.ones((self.P,1)))\n",
    "            B = cp.Constant(self.B)\n",
    "\n",
    "            while pat_iter < patience and iter_ <= max_iter:\n",
    "\n",
    "                # Init variables and parameters\n",
    "                h = cp.Variable((self.P*(2*self.J+1), 1))\n",
    "                self._aux_matrix_update(X_tr)\n",
    "                h.value = h_opt\n",
    "\n",
    "                Q = cp.Constant(np.einsum('imn, lmn -> il', self.P_aux, self.P_aux) + reg)\n",
    "                l = cp.Constant(np.einsum('mn, imn -> i', self.Y_train, self.P_aux))\n",
    "\n",
    "                # Quadratic term\n",
    "                term2 = cp.quad_form(h, Q, assume_PSD = True)\n",
    "                # Linear term\n",
    "                term1 = l@h\n",
    "                term1 = cp.multiply(-2, term1)[0]\n",
    "                \n",
    "                obj = cp.Minimize(term2+term1)\n",
    "\n",
    "                # Define the constraints\n",
    "                cons1 = cp.kron(I_s, B)@h\n",
    "                cons2 = cp.kron(i_s.T, B)@h\n",
    "                constraints = [cons1 >= 0] + \\\n",
    "                                [cons1 <= self.c] + \\\n",
    "                                [cons2 >= (self.c - self.epsilon)] + \\\n",
    "                                [cons2 <= (self.c + self.epsilon)]\n",
    "\n",
    "                prob = cp.Problem(obj, constraints)\n",
    "                prob.solve(solver=eval(f'cp.{solver}'), verbose=False)\n",
    "\n",
    "                # Update the dictionary\n",
    "\n",
    "                if self.dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                    h_list = _split_coeffs(h, self.P, self.J)\n",
    "                    D = generate_dictionary(h.value, self.P, self.Lj)                      \n",
    "                    h_opt = h_opt + step_h*(h.value - h_opt)\n",
    "                else:\n",
    "\n",
    "                    h_list = _split_coeffs(h, self.P, self.J, sep=True)\n",
    "                    D = generate_dictionary(h_list, self.P, self.Luj, self.Ldj)                                \n",
    "                    h_opt = h_opt + step_h*(h.value - h_opt)\n",
    "\n",
    "\n",
    "                # OMP Step\n",
    "                X_te_tmp, X_tr_tmp = sparse_transform(D, self.K0, self.Y_test, self.Y_train)\n",
    "                # Sparse Representation Update\n",
    "                X_tr = X_tr + step_x*(X_tr_tmp - X_tr)\n",
    "                X_te = X_te + step_x*(X_te_tmp - X_te)\n",
    "\n",
    "                # Error Update\n",
    "                error_train = nmse(D, X_tr, self.Y_train, self.m_train)\n",
    "                error_test = nmse(D, X_te, self.Y_test, self.m_test)\n",
    "\n",
    "                hist.append(error_test)\n",
    "                \n",
    "                # Error Storing\n",
    "                if (error_train < self.min_error_train) and (abs(error_train) > np.finfo(float).eps) and (abs(error_train - self.min_error_train) > tol):\n",
    "                    self.X_opt_train = X_tr\n",
    "                    self.min_error_train = error_train\n",
    "\n",
    "                if (error_test < self.min_error_test) and (abs(error_test) > np.finfo(float).eps) and (abs(error_test - self.min_error_test) > tol):\n",
    "                    self.h_opt = h_list if self.dictionary_type == 'separated' else h_opt\n",
    "                    self.D_opt = D\n",
    "                    self.X_opt_test = X_te\n",
    "                    self.min_error_test = error_test\n",
    "                    pat_iter = 0\n",
    "\n",
    "                    if verbose == 1:\n",
    "                        print(\"New Best Test Error:\", self.min_error_test)\n",
    "                else:\n",
    "                    pat_iter += 1\n",
    "\n",
    "                iter_ += 1\n",
    "        \n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        return self.min_error_test, self.min_error_train, hist\n",
    "    \n",
    "    @save_results\n",
    "    def learn_upper_laplacian(self,\n",
    "                              Lu_new: np.ndarray = None,\n",
    "                              filter: np.ndarray = 1,\n",
    "                              h_prior: np.ndarray = None,\n",
    "                              lambda_: float = 1e-3, \n",
    "                              max_iter: int = 10, \n",
    "                              patience: int = 10,\n",
    "                              tol: float = 1e-7,\n",
    "                              step_h: float = 1.,\n",
    "                              step_x: float = 1.,\n",
    "                              mode: str = \"optimistic\",\n",
    "                              verbose: bool = False,\n",
    "                              warmup: int = 0,\n",
    "                              QP=False,\n",
    "                              cont=False):\n",
    "    \n",
    "        assert step_h<1 or step_h>0, \"You must provide a step-size between 0 and 1.\"\n",
    "        assert step_x<1 or step_x>0, \"You must provide a step-size between 0 and 1.\"\n",
    "        assert (mode==\"optimistic\") or (mode==\"pessimistic\"), f'{mode} is not a legal mode: \\\"optimistic\\\" or \\\"pessimistic\\\" are the only ones allowed.'\n",
    "        \n",
    "        # Check if we are executing the first recursive iteration\n",
    "        if np.all(Lu_new == None):\n",
    "            T = self.B2.shape[1]\n",
    "            if mode==\"optimistic\":\n",
    "                filter = np.ones(T)\n",
    "                self.warmup=0  \n",
    "            else:\n",
    "                filter = np.zeros(T)\n",
    "                self.update_Lu(np.zeros(self.Lu.shape)) # start with an \"empty\" upper Laplacian\n",
    "                self.warmup = warmup\n",
    "        else:\n",
    "            self.update_Lu(Lu_new)\n",
    "\n",
    "        self.init_dict(h_prior=h_prior,\n",
    "                       mode=\"only_X\")\n",
    "\n",
    "        if QP:\n",
    "            _, _, hist = self.topological_dictionary_learn_qp(lambda_=lambda_,\n",
    "                                                            max_iter=max_iter,\n",
    "                                                            patience=patience,\n",
    "                                                            tol=tol,\n",
    "                                                            step_h=step_h,\n",
    "                                                            step_x=step_x,\n",
    "                                                            solver='GUROBI')\n",
    "        else:\n",
    "            _, _, hist = self.topological_dictionary_learn(lambda_=lambda_,\n",
    "                                                            max_iter=max_iter,\n",
    "                                                            patience=patience,\n",
    "                                                            tol=tol,\n",
    "                                                            step_h=step_h,\n",
    "                                                            step_x=step_x)\n",
    "                        \n",
    "        self.history.append(hist)\n",
    "        search_space = np.where(filter == 1) if mode==\"optimistic\" else np.where(filter == 0)   \n",
    "        sigmas = pd.DataFrame({\"idx\": search_space[0]})\n",
    "\n",
    "        sigmas[\"sigma\"] = sigmas.idx.apply(lambda _: filter)\n",
    "        if mode==\"optimistic\":\n",
    "            sigmas[\"sigma\"] = sigmas.apply(lambda x: _indicator_matrix(x), axis=1)\n",
    "        else:\n",
    "            sigmas[\"sigma\"] = sigmas.apply(lambda x: _indicator_matrix_rev(x), axis=1)\n",
    "        sigmas[\"Luj\"] = sigmas.apply(lambda x: _compute_Luj(x, self.B2, self.J), axis=1)\n",
    "        sigmas[\"D\"] = sigmas.apply(lambda x: generate_dictionary(self.h_opt, self.P, x.Luj, self.Ldj), axis=1)\n",
    "        sigmas[\"X\"] = sigmas.D.apply(lambda x: sparse_transform(x, self.K0, self.Y_test))\n",
    "        sigmas[\"NMSE\"] = sigmas.apply(lambda x: nmse(x.D, x.X, self.Y_test, self.m_test), axis=1)\n",
    "        \n",
    "        if self.warmup>0:\n",
    "            candidate_error = sigmas.NMSE.min() - np.finfo(float).eps\n",
    "            self.warmup-=1\n",
    "        else:\n",
    "            candidate_error = sigmas.NMSE.min()\n",
    "        idx_min = sigmas.NMSE.idxmin()\n",
    "\n",
    "        \n",
    "        if candidate_error < self.min_error_test:\n",
    "            S = sigmas.sigma[idx_min]\n",
    "            Lu_new = self.B2 @ S @ self.B2.T\n",
    "            filter = np.diagonal(S)\n",
    "\n",
    "            if verbose:\n",
    "                if mode==\"optimistic\":\n",
    "                    print(f'Removing 1 triangle from topology... \\n ... New min test error: {candidate_error} !')\n",
    "                else:\n",
    "                    print(f'Adding 1 triangle to topology... \\n ... New min test error: {candidate_error} !')\n",
    "\n",
    "            return self.learn_upper_laplacian(h_prior=self.h_opt,\n",
    "                                              Lu_new=Lu_new,\n",
    "                                              filter=filter,\n",
    "                                              lambda_=lambda_,\n",
    "                                              max_iter=max_iter,\n",
    "                                              patience=patience,\n",
    "                                              tol=tol,\n",
    "                                              step_h=step_h,\n",
    "                                              step_x=step_x,\n",
    "                                              mode=mode,\n",
    "                                              verbose=verbose,\n",
    "                                              QP=QP,\n",
    "                                              cont=cont)\n",
    "        \n",
    "        # For the last recursions of \"pessimistic\" mode try some recursion of the \"optimistic\"\n",
    "        # to remove the warm-up randomly-added triangles\n",
    "        if mode == \"pessimistic\" and not cont:\n",
    "            return  self.learn_upper_laplacian(h_prior=self.h_opt,\n",
    "                                              Lu_new=Lu_new,\n",
    "                                              filter=filter,\n",
    "                                              lambda_=lambda_,\n",
    "                                              max_iter=max_iter,\n",
    "                                              patience=patience,\n",
    "                                              tol=tol,\n",
    "                                              step_h=step_h,\n",
    "                                              step_x=step_x,\n",
    "                                              mode=\"optimistic\",\n",
    "                                              verbose=verbose,\n",
    "                                              QP=QP,\n",
    "                                              cont=True)\n",
    "        \n",
    "        # Then after we added triangles and removed the randomly added ones, continue adding!\n",
    "        elif mode != \"pessimistic\" and cont:\n",
    "\n",
    "            print(\"Ce provo!\")\n",
    "            return  self.learn_upper_laplacian(h_prior=self.h_opt,\n",
    "                                              Lu_new=Lu_new,\n",
    "                                              filter=filter,\n",
    "                                              lambda_=lambda_,\n",
    "                                              max_iter=max_iter,\n",
    "                                              patience=patience,\n",
    "                                              tol=tol,\n",
    "                                              step_h=step_h,\n",
    "                                              step_x=step_x,\n",
    "                                              mode=\"pessimistic\",\n",
    "                                              verbose=verbose,\n",
    "                                              QP=QP,\n",
    "                                              cont=cont,\n",
    "                                              warmup=warmup)  \n",
    "                  \n",
    "        self.B2 = self.B2@np.diag(filter)\n",
    "        return self.min_error_test, self.history, self.Lu, self.B2\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
