{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b2__generation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "from numpy.linalg import matrix_rank\n",
    "from scipy.linalg import qr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy.linalg import qr, matrix_rank\n",
    "\n",
    "def get_adj_en(g):\n",
    "    return nx.adjacency_matrix(g).todense()\n",
    "\n",
    "def get_b1(a):\n",
    "    g = nx.Graph(a)\n",
    "    return (-1)*nx.incidence_matrix(g, oriented=True).todense()\n",
    "\n",
    "def getCycles(A, max_len=math.inf):\n",
    "    G = nx.DiGraph(A)\n",
    "    cycles = nx.simple_cycles(G)\n",
    "    \n",
    "    seen = set()\n",
    "    final = []\n",
    "    \n",
    "    for cycle in cycles:\n",
    "        # Create a unique identifier for the cycle by sorting and converting to a tuple\n",
    "        cycle_tuple = tuple(sorted(cycle))\n",
    "        if cycle_tuple not in seen and 3 <= len(cycle) <= max_len:\n",
    "            seen.add(cycle_tuple)\n",
    "            final.append(cycle)\n",
    "    \n",
    "    final.sort(key=len)\n",
    "    return final\n",
    "\n",
    "def get_b2(a, p_max_len=math.inf):\n",
    "    G = nx.Graph(a)\n",
    "    E_list = list(G.edges)\n",
    "    All_P = getCycles(a, p_max_len)\n",
    "    cycles = [cycle + [cycle[0]] for cycle in All_P]  # Closing the cycles\n",
    "\n",
    "    # Precompute edge indices for faster lookup\n",
    "    edge_index_map = {edge: i for i, edge in enumerate(E_list)}\n",
    "    \n",
    "    # Initialize the matrix B2\n",
    "    B2 = np.zeros((len(E_list), len(cycles)))\n",
    "    \n",
    "    # Populate B2\n",
    "    for cycle_index, cycle in enumerate(cycles):\n",
    "        for i in range(len(cycle) - 1):\n",
    "            edge = (cycle[i], cycle[i + 1])\n",
    "            edge_reversed = (cycle[i + 1], cycle[i])\n",
    "\n",
    "            # Use edge indices from the map to avoid repeated searches\n",
    "            if edge in edge_index_map:\n",
    "                B2[edge_index_map[edge], cycle_index] = 1\n",
    "            elif edge_reversed in edge_index_map:\n",
    "                B2[edge_index_map[edge_reversed], cycle_index] = -1\n",
    "                \n",
    "    # Perform QR Decomposition without pivoting\n",
    "    QR = qr(B2, pivoting=True)  # 'reduced' mode for economic QR, use 'complete' if full QR is needed\n",
    "    rank = matrix_rank(B2)\n",
    "    B2 = B2[:, sorted(QR[2]):rank]\n",
    "    \n",
    "    return B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Claudio's implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np \n",
    "import random\n",
    "import math \n",
    "from numpy.linalg import matrix_rank\n",
    "from scipy import linalg\n",
    "\n",
    "def get_adj(g):\n",
    "  adj = np.zeros((len(g.nodes()),len(g.nodes())))\n",
    "  #print(g.edges())\n",
    "  for e in np.array(g.edges()):\n",
    "    adj[e[0],e[1]] = 1\n",
    "    adj[e[1],e[0]] = 1\n",
    "  return adj\n",
    "\n",
    "\n",
    "def getCycles(A,max_len=math.inf):\n",
    "  ''''A = adj matrix'''\n",
    "  ''''p_max_len = lenght if max cycles wanted, inf default for all cycles'''\n",
    "  G = nx.DiGraph(A)\n",
    "  cycles = nx.simple_cycles(G)\n",
    "  final = []\n",
    "  for elem in cycles:\n",
    "    if sorted(elem) not in [sorted(x) for x in final]:\n",
    "      final.append(elem)\n",
    "  final = [c for c in final if len(c)>=3 and len(c)<= max_len]\n",
    "  final.sort(key=len)\n",
    "  return final\n",
    "\n",
    "\n",
    "def create_B1(A):\n",
    "  G = nx.Graph(A) #nx.from_numpy_matrix(A) #\n",
    "  E_list = list(G.edges) \n",
    "  B1 = np.zeros([len(G.nodes),len(E_list)])\n",
    "  for n in G.nodes: \n",
    "    for e in E_list:\n",
    "      if n==e[0]:\n",
    "        B1[n,E_list.index(e)] = 1\n",
    "      elif n==e[1]:\n",
    "        B1[n,E_list.index(e)] = -1 \n",
    "      else:\n",
    "        B1[n,E_list.index(e)] = 0\n",
    "  return B1\n",
    "\n",
    "def create_B2(A,p_max_len=math.inf):\n",
    "  ''''A = adj matrix'''\n",
    "  ''''p_max_len = lenght if max cycles wanted, inf default for all cycles'''\n",
    "  G = nx.Graph(A) #Â nx.from_numpy_matrix(A) #\n",
    "  E_list = list(G.edges)\n",
    "  All_P = getCycles(A,p_max_len)\n",
    "  cycles = [x + [x[0]] for x in All_P]\n",
    "  P_list = []\n",
    "  for c in cycles:\n",
    "    p = []\n",
    "    for i in range(len(c)-1):\n",
    "      p.append([c[i],c[i+1]])\n",
    "    P_list.append(p)\n",
    "  B2 = np.zeros([len(E_list),len(P_list)])\n",
    "  for e in E_list:\n",
    "    for p in P_list:\n",
    "      if list(e) in p:\n",
    "        B2[E_list.index(e),P_list.index(p)] = 1\n",
    "      elif [e[1],e[0]] in p:\n",
    "        B2[E_list.index(e),P_list.index(p)] = -1\n",
    "      else:\n",
    "        B2[E_list.index(e),P_list.index(p)] = 0\n",
    "  qr = linalg.qr(B2,pivoting=True)\n",
    "  B2 = B2[: ,sorted(qr[2])[:matrix_rank(B2)]]\n",
    "  return B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 40\n",
    "p = .2\n",
    "seed = 0\n",
    "G = nx.erdos_renyi_graph(n, p, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = get_adj(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = get_b1(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[183], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m B2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_b2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[182], line 31\u001b[0m, in \u001b[0;36mget_b2\u001b[1;34m(a, p_max_len)\u001b[0m\n\u001b[0;32m     29\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph(a)\n\u001b[0;32m     30\u001b[0m E_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39medges)\n\u001b[1;32m---> 31\u001b[0m All_P \u001b[38;5;241m=\u001b[39m \u001b[43mgetCycles\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_max_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m cycles \u001b[38;5;241m=\u001b[39m [cycle \u001b[38;5;241m+\u001b[39m [cycle[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m cycle \u001b[38;5;129;01min\u001b[39;00m All_P]  \u001b[38;5;66;03m# Closing the cycles\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Precompute edge indices for faster lookup\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[182], line 18\u001b[0m, in \u001b[0;36mgetCycles\u001b[1;34m(A, max_len)\u001b[0m\n\u001b[0;32m     15\u001b[0m seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m     16\u001b[0m final \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cycle \u001b[38;5;129;01min\u001b[39;00m cycles:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Create a unique identifier for the cycle by sorting and converting to a tuple\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     cycle_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28msorted\u001b[39m(cycle))\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cycle_tuple \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;241m3\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(cycle) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_len:\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\cycles.py:230\u001b[0m, in \u001b[0;36msimple_cycles\u001b[1;34m(G, length_bound)\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m directed:\n\u001b[1;32m--> 230\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _directed_cycle_search(G, length_bound)\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _undirected_cycle_search(G, length_bound)\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\cycles.py:277\u001b[0m, in \u001b[0;36m_directed_cycle_search\u001b[1;34m(G, length_bound)\u001b[0m\n\u001b[0;32m    275\u001b[0m v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(c))\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length_bound \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _johnson_cycle_search(Gc, [v])\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _bounded_cycle_search(Gc, [v], length_bound)\n",
      "File \u001b[1;32mc:\\Users\\engri\\Anaconda3\\lib\\site-packages\\networkx\\algorithms\\cycles.py:408\u001b[0m, in \u001b[0;36m_johnson_cycle_search\u001b[1;34m(G, path)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m G[v]:\n\u001b[1;32m--> 408\u001b[0m         \u001b[43mB\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m(v)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "B2 = get_b2(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-size array to reduction operation maximum which has no identity",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[171], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m A \u001b[38;5;241m=\u001b[39m get_adj(G)\n\u001b[0;32m      8\u001b[0m B1 \u001b[38;5;241m=\u001b[39m create_B1(A)\n\u001b[1;32m----> 9\u001b[0m B2 \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_B2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[161], line 68\u001b[0m, in \u001b[0;36mcreate_B2\u001b[1;34m(A, p_max_len)\u001b[0m\n\u001b[0;32m     66\u001b[0m       B2[E_list\u001b[38;5;241m.\u001b[39mindex(e),P_list\u001b[38;5;241m.\u001b[39mindex(p)] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     67\u001b[0m qr \u001b[38;5;241m=\u001b[39m linalg\u001b[38;5;241m.\u001b[39mqr(B2,pivoting\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 68\u001b[0m B2 \u001b[38;5;241m=\u001b[39m B2[: ,\u001b[38;5;28msorted\u001b[39m(qr[\u001b[38;5;241m2\u001b[39m])[:\u001b[43mmatrix_rank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mB2\u001b[49m\u001b[43m)\u001b[49m]]\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m B2\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\linalg\\linalg.py:1900\u001b[0m, in \u001b[0;36mmatrix_rank\u001b[1;34m(A, tol, hermitian)\u001b[0m\n\u001b[0;32m   1898\u001b[0m S \u001b[38;5;241m=\u001b[39m svd(A, compute_uv\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, hermitian\u001b[38;5;241m=\u001b[39mhermitian)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1900\u001b[0m     tol \u001b[38;5;241m=\u001b[39m \u001b[43mS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]) \u001b[38;5;241m*\u001b[39m finfo(S\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39meps\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1902\u001b[0m     tol \u001b[38;5;241m=\u001b[39m asarray(tol)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, newaxis]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\_methods.py:40\u001b[0m, in \u001b[0;36m_amax\u001b[1;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_amax\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m           initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_maximum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: zero-size array to reduction operation maximum which has no identity"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Example\n",
    "n = 10\n",
    "p = .2\n",
    "seed = 0\n",
    "G = nx.erdos_renyi_graph(n, p, seed)\n",
    "nx.draw(G)\n",
    "A = get_adj(G)\n",
    "B1 = create_B1(A)\n",
    "B2 = get_b2(A,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create_ground_truth.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import pandas as pd\n",
    "from scipy.sparse.linalg import eigs\n",
    "\n",
    "# __all__ = ['create_ground_truth']\n",
    "\n",
    "def compute_Lk_and_lambdak(L, K, separated=False):\n",
    "    lambdas, _ = eigs(L)\n",
    "    lambdas[np.abs(lambdas) < np.finfo(float).eps] = 0\n",
    "    lambda_max = np.max(lambdas)\n",
    "    lambda_min = np.min(lambdas)\n",
    "    Lk = np.array([la.matrix_power(L, i) for i in range(1, K + 1)])\n",
    "    # for the \"separated\" implementation we need a different dimensionality\n",
    "    if separated:\n",
    "        lambda_max_k = lambda_max ** np.arange(1, K + 1)\n",
    "        lambda_min_k = lambda_min ** np.arange(1, K + 1)\n",
    "    else:\n",
    "        lambda_max_k = lambda_max ** np.array(list(np.arange(1, K + 1))+[0])\n",
    "        lambda_min_k = lambda_min ** np.array(list(np.arange(1, K + 1))+[0])\n",
    "    return Lk, lambda_max_k, lambda_min_k\n",
    "\n",
    "def generate_coeffs(*arrays, s, mult=10):\n",
    "    \"\"\" \n",
    "    Select ad hoc parameters for synthetic data generation, randomly over\n",
    "    an interval dependent on the max eigenvalues of the Laplacian(s)\n",
    "    \"\"\"\n",
    "\n",
    "    # if passing four arguments (two for upper and two for lower laplacian eigevals)\n",
    "    # it means that you are using dictionary_type=\"separated\"\n",
    "    if len(arrays)==2:\n",
    "        lambda_max_k, lambda_min_k = arrays\n",
    "        K = lambda_max_k.shape[0]\n",
    "        h = mult / np.max(lambda_max_k) * np.random.rand(s, K)\n",
    "        # For later sanity check in optimization phase \n",
    "        tmp_max_vec = h @ lambda_max_k # parallelize the code with simple matrix multiplications\n",
    "        tmp_min_vec = h @ lambda_min_k\n",
    "        c = np.max(tmp_max_vec)\n",
    "        tmp_sum_max = np.sum(tmp_max_vec)\n",
    "        tmp_sum_min = np.sum(tmp_min_vec)\n",
    "\n",
    "        Delta_min = c - tmp_sum_min\n",
    "        Delta_max = tmp_sum_max - c\n",
    "        epsilon = (Delta_max - Delta_min) * np.random.rand() + Delta_min\n",
    "\n",
    "    elif len(arrays)==4:\n",
    "        lambda_max_u_k, lambda_min_u_k, lambda_max_d_k, lambda_min_d_k = arrays\n",
    "        K = lambda_max_u_k.shape[0]\n",
    "        hI = mult / np.max(lambda_max_d_k) * np.random.rand(s, K)\n",
    "        hS = mult / np.max(lambda_max_u_k) * np.random.rand(s, K)\n",
    "        hH = mult / np.min([np.max(lambda_max_u_k), np.max(lambda_max_d_k)]) * np.random.rand(s, 1)\n",
    "        h = [hS, hI, hH]\n",
    "        # For later sanity check in optimization phase\n",
    "        tmp_max_vec_S = (hS @ lambda_max_u_k).reshape(s,1)\n",
    "        tmp_min_vec_S = (hS @ lambda_min_u_k).reshape(s,1)\n",
    "        tmp_max_vec_I = (hI @ lambda_max_d_k).reshape(s,1)\n",
    "        tmp_min_vec_I = (hI @ lambda_min_d_k).reshape(s,1)\n",
    "        c = np.max(tmp_max_vec_I + tmp_max_vec_S + hH)\n",
    "        tmp_sum_min = np.sum(tmp_min_vec_I + tmp_min_vec_S + hH)\n",
    "        tmp_sum_max = np.sum(tmp_max_vec_I + tmp_max_vec_S + hH)\n",
    "        Delta_min = c - tmp_sum_min\n",
    "        Delta_max = tmp_sum_max - c\n",
    "        epsilon = np.max([Delta_min, Delta_max])\n",
    "    else:\n",
    "        raise ValueError(\"Function accepts either 2 or 4 arrays! In case of 4 arrays are provided,\\\n",
    "                        the first 2 refer to upper laplacian and the other two to lower laplacian.\")\n",
    "    return h, c, epsilon, tmp_sum_min, tmp_sum_max\n",
    "\n",
    "def generate_dictionary(h, s, *matrices):\n",
    "    D = []\n",
    "    # Always check if upper and lower Laplacians are separately provided\n",
    "    if len(matrices)==1:\n",
    "        Lk = matrices[0]\n",
    "        n = Lk.shape[-1]\n",
    "        k = Lk.shape[0]\n",
    "        # iterate over each kernel dimension\n",
    "        for i in range(0,s):\n",
    "            # Replicate formula (8) of the paper for the i-th sub-dictionary\n",
    "            tmp = np.sum(h[i]*Lk, axis=0) + h[i,-1]*np.eye(n,n)\n",
    "            D.append(tmp)\n",
    "    elif len(matrices)==2:\n",
    "        Luk , Ldk = matrices\n",
    "        n = Luk.shape[-1]\n",
    "        k = Luk.shape[0]\n",
    "        # iterate over each kernel dimension\n",
    "        for i in range(0,s):\n",
    "            # Replicate formula (8) of the paper for the i-th sub-dictionary\n",
    "            hu = h[0][i].reshape(k,1,1)\n",
    "            hd = h[1][i].reshape(k,1,1)\n",
    "            hid = h[2][i]\n",
    "            tmp = np.sum(hu*Luk + hd*Ldk, axis=0) + hid*np.eye(n,n)\n",
    "            D.append(tmp)\n",
    "    else:\n",
    "        raise ValueError(\"Function accepts one vector and either 1 or 2 matrices.\")\n",
    "    D = np.array(D).reshape(n, n*s)\n",
    "    return D\n",
    "\n",
    "def create_ground_truth(Lu, Ld, m_train, m_test, s, K, K0, dictionary_type, sparsity_mode):\n",
    "\n",
    "    # Joint Dictionary Model\n",
    "    if dictionary_type == \"joint\":\n",
    "        Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        h, c, epsilon, _, _ = generate_coeffs(lambda_max_k, lambda_min_k, s=s)\n",
    "        D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "    # Edge Laplacian Dictionary Model\n",
    "    elif dictionary_type == \"edge_laplacian\":\n",
    "        Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Ld, K)\n",
    "        h, c, epsilon, _, _ = generate_coeffs(lambda_max_k, lambda_min_k, s=s)\n",
    "        D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "    # Separated Dictionary Model\n",
    "    elif dictionary_type == \"separated\":\n",
    "        Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "        Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "        h, c, epsilon, _, _ = generate_coeffs(lambda_max_d_k, lambda_min_d_k, lambda_max_u_k, lambda_min_u_k, s=s)\n",
    "        D = generate_dictionary(h, s, Luk, Ldk)\n",
    "\n",
    "    # Signal Generation\n",
    "    def create_column_vec(row):\n",
    "        tmp = np.zeros(n*s)\n",
    "        tmp[row['idxs']]=row['non_zero_coeff']\n",
    "        return tmp\n",
    "    \n",
    "    m_total = m_train + m_test\n",
    "    tmp = pd.DataFrame()\n",
    "    # Determine the sparsity for each column based on sparsity_mode\n",
    "    if sparsity_mode == \"max\":\n",
    "        tmp_K0 = np.random.choice(np.arange(1,K0+1), size=(m_total), replace=True)\n",
    "    else:\n",
    "        tmp_K0 = np.full((m_total,), K0)\n",
    "    # sparsity coefficient for each column\n",
    "    tmp['K0'] = tmp_K0\n",
    "    # for each column get K0 indexes\n",
    "    tmp['idxs'] = tmp.K0.apply(lambda x: np.random.choice(n*s, x, replace=False))\n",
    "    # for each of the K0 row indexes in each column, sample K0 values\n",
    "    tmp['non_zero_coeff'] = tmp.K0.apply(lambda x: np.random.randn(x))\n",
    "    # create the column vectors with the desired characteristics\n",
    "    tmp['column_vec'] = tmp.apply(lambda x: create_column_vec(x), axis=1)\n",
    "    # finally derive the sparse signal representation matrix\n",
    "    X = np.column_stack(tmp['column_vec'].values)\n",
    "\n",
    "    all_data = D @ X\n",
    "    X_train = X[:, :m_train]\n",
    "    X_test = X[:, m_train:]\n",
    "    train_Y = all_data[:, :m_train]\n",
    "    test_Y = all_data[:, m_train:]\n",
    "\n",
    "    return D, h, train_Y, test_Y, epsilon, c, X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 976,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_omp_coeff(K0, Domp, col):\n",
    "    omp = OrthogonalMatchingPursuit(n_nonzero_coefs=K0)\n",
    "    omp.fit(Domp, col)\n",
    "    return omp.coef_\n",
    "\n",
    "def initialize_dic(Lu, Ld, s, K, Y_train, K0, dictionary_type, c, epsilon, only):\n",
    "\n",
    "    n = Lu.shape[0]\n",
    "    D = np.zeros((n, n*s))\n",
    "    X = np.zeros(Y_train.shape)\n",
    "    X = np.tile(X, (s,1))\n",
    "    discard = 0\n",
    "\n",
    "    # maybe is better to create a wrapper\n",
    "    def multiplier_search(*arrays, s=s):\n",
    "        is_okay = 0\n",
    "        mult = 100\n",
    "        tries = 0\n",
    "        while is_okay==0:\n",
    "            is_okay = 1\n",
    "            h, c_try, _, tmp_sum_min, tmp_sum_max = generate_coeffs(arrays, s=s, mult=mult)\n",
    "            if c_try <= c:\n",
    "                is_okay *= 1\n",
    "            if tmp_sum_min > c-epsilon:\n",
    "                is_okay *= 1\n",
    "                incr_mult = 0\n",
    "            else:\n",
    "                is_okay = is_okay*0\n",
    "                incr_mult = 1\n",
    "            if tmp_sum_max < c+epsilon:\n",
    "                is_okay *= 1\n",
    "                decr_mult = 0\n",
    "            else:\n",
    "                is_okay *= 0\n",
    "                decr_mult = 1\n",
    "            if is_okay == 0:\n",
    "                tries += 1\n",
    "            if tries >3:\n",
    "                discard = 1\n",
    "                break\n",
    "            if incr_mult == 1:\n",
    "                mult *= 2\n",
    "            if decr_mult == 1:\n",
    "                mult /= 2\n",
    "        return h, discard\n",
    "\n",
    "    if (only == \"only_D\") or (only == \"all\"):\n",
    "        X = 0\n",
    "        # Joint Dictionary Model\n",
    "        if dictionary_type == \"joint\":\n",
    "            Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "            h, discard = multiplier_search(lambda_max_k, lambda_min_k)\n",
    "            D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "        # Edge Laplacian Dictionary Model\n",
    "        elif dictionary_type == \"edge_laplacian\":\n",
    "            Lk, lambda_max_k, lambda_min_k = compute_Lk_and_lambdak(Ld, K)\n",
    "            h, discard = multiplier_search(lambda_max_k, lambda_min_k)\n",
    "            D = generate_dictionary(h, s, Lk)\n",
    "\n",
    "        # Separated Dictionary Model\n",
    "        elif dictionary_type == \"separated\":\n",
    "            Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "            h, discard = multiplier_search(lambda_max_d_k, lambda_min_d_k, lambda_max_u_k, lambda_min_u_k)\n",
    "            D = generate_dictionary(h, s, Luk, Ldk)\n",
    "    \n",
    "    if (only == \"only_X\" or only == \"all\"):\n",
    "        \n",
    "        if dictionary_type == \"edge_laplacian\":\n",
    "            L = Ld\n",
    "        else:\n",
    "            L = Lu+Ld\n",
    "\n",
    "        _, Dx = eigs(L)\n",
    "        dd = la.norm(Dx, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        Domp = Dx@W\n",
    "        X = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp.real, x), axis=0, arr=Y_train.real)\n",
    "        X = np.tile(X, (s,1))\n",
    "        \n",
    "    return D, X, discard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify_dic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "\n",
    "def verify_dic(D, Y_train, X_train_true, K0_max, acc_thresh):\n",
    "    # OMP\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)  # Normalization Step\n",
    "    Domp = D @ W\n",
    "    fin_acc = 0\n",
    "    for K0 in range(1, K0_max+1):\n",
    "        idx = np.sum(np.abs(X_train_true) > 0, axis=0) == K0  # select all column vectors with certain sparsity (K0 non-null elements)\n",
    "        tmp_train = Y_train[:, idx].real\n",
    "        ##########################\n",
    "        if tmp_train.shape[1]==0:\n",
    "            continue\n",
    "        X_true_tmp = X_train_true[:, idx].real\n",
    "        idx_group = np.abs(X_true_tmp) > 0\n",
    "        X_tr = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp.real, x), axis=0, arr=tmp_train)\n",
    "        idx_train = np.abs(X_tr) > 0\n",
    "        acc = np.sum(np.sum(idx_group == idx_train, axis=0) == idx_group.shape[0])/idx_group.shape[1]\n",
    "        if acc < 0.7:\n",
    "            break\n",
    "        else:\n",
    "            fin_acc = acc\n",
    "    # ????????\n",
    "    max_possible_sparsity = K0 - 1\n",
    "    return max_possible_sparsity, fin_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1058,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prova\n",
    "\n",
    "# Subsampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1, dtype=float)\n",
    "Lu = np.matmul(B2, np.transpose(B2), dtype=float)\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "_ ,U = la.eig(L)\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type_true = \"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s_true = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k_true = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = dictionary_type_true\n",
    "s = s_true\n",
    "k = k_true\n",
    "K0_coll = np.arange(5, 26, 4) # K0_coll = 5:4:25 %4:4:40 %5:3:20\n",
    "lambda_ = 1e-6 # l2 multiplier\n",
    "max_iter = 100 # Maximum number of iterations\n",
    "patience = 5 # Patience\n",
    "tol = 1e-7 # Tolerance for Patience\n",
    "n_sim = 10\n",
    "verbose = 0\n",
    "\n",
    "# Luk, lambda_max_u_k, lambda_min_u_k = compute_Lk_and_lambdak(Lu, k, separated=True)\n",
    "# Ldk, lambda_max_d_k, lambda_min_d_k = compute_Lk_and_lambdak(Ld, k, separated=True)\n",
    "\n",
    "# h, c, epsilon, _, _ = generate_coeffs(lambda_max_d_k, lambda_min_d_k, lambda_max_u_k, lambda_min_u_k, s=s)\n",
    "\n",
    "# D = generate_dictionary(h, s, Luk, Ldk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1059,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D, h, Y_train, Y_test, epsilon, c, X_train, X_test = create_ground_truth(Lu,\n",
    "#                                                                          Ld,\n",
    "#                                                                          m_train,\n",
    "#                                                                          m_test, \n",
    "#                                                                          s, \n",
    "#                                                                          k, \n",
    "#                                                                          K0_coll[0], \n",
    "#                                                                          dictionary_type, \n",
    "#                                                                          sparsity_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topological_dictionary_learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "\n",
    "def topological_dictionary_learn(Y_train, Y_test, K, n, s, D0, X0, Lu, Ld, dictionary_type, c, epsilon, K0, lambda_=1e-5, max_iter=10, patience=10, tol=1e-7, verbose=0):\n",
    "    h_opt =0  ##############################################\n",
    "    X_opt_train=0\n",
    "    X_opt_test=0  ###########################################\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        # Joint Dictionary\n",
    "        if dictionary_type == \"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        # Separated Dictionary\n",
    "        elif dictionary_type == \"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "        # Edge Laplacian\n",
    "        elif dictionary_type == \"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, K)\n",
    "\n",
    "        # Init the dictionary and the sparse representation\n",
    "        D0 = D0.reshape(n,n,s)\n",
    "        D_coll = [cp.Constant(D0[:,:,j]) for j in range(s)]\n",
    "        X_train = X0\n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            I = cp.Constant(np.eye(n))\n",
    "\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                h = cp.Variable((s, K))\n",
    "                hI = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += cp.multiply(h[i,j], Lk[j, :, :])\n",
    "                    tmp += cp.multiply(hI[i,:], I)\n",
    "                    D_coll[i] = tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                term1 = cp.norm(Y_train - D @ X_train, 'fro')**2\n",
    "                term2 = lambda_ * cp.norm(h, 'fro')**2\n",
    "                term3 = lambda_ * cp.norm(hI, 'fro')**2\n",
    "                obj = cp.Minimize(term1+term2+term3)\n",
    "            else:\n",
    "                hI = cp.Variable((s, K))\n",
    "                hS = cp.Variable((s, K))\n",
    "                hH = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    hu = hS[i,:]\n",
    "                    hd = hI[i,:]\n",
    "                    hid = hH[i]\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += cp.multiply(hu[j], Luk[j, :, :]) + cp.multiply(hd[j], Ldk[j, :, :])\n",
    "                    tmp += cp.multiply(hid, I)\n",
    "                    D_coll[i] = tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                \n",
    "                term1 = cp.norm(Y_train - D @ X_train, 'fro')**2\n",
    "                term2 = lambda_ * cp.norm(hI, 'fro')**2\n",
    "                term3 = lambda_ * cp.norm(hS, 'fro')**2\n",
    "                term4 = lambda_ * cp.norm(hH, 'fro')**2\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            Dsum = cp.sum(D_coll)\n",
    "            constraints = [D_coll[i] >= 0 * I for i in range(s)] + \\\n",
    "                            [D_coll[i] <= c * I for i in range(s)] + \\\n",
    "                            [Dsum >= cp.multiply((c - epsilon), I), Dsum <= cp.multiply((c + epsilon), I)]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.CLARABEL)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train.real)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test.real)\n",
    "            # Normalize?\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "                print(\"ping!\")\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI.value, hS.value, hH.value])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                print(\"ping!\")\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "\n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = eigs(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train.real)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test.real)\n",
    "        # Normalize?\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "    return h_opt, X_opt_test, X_opt_train, min_error_test_norm, min_error_train_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init param_dic.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('datiSC.mat')\n",
    "B1 = mat[\"B1\"]\n",
    "B2 = mat[\"B2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.linalg as la\n",
    "\n",
    "# Subsampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1)\n",
    "Lu = np.matmul(B2, np.transpose(B2))\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "_ ,U = la.eig(L)\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type_true = \"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s_true = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k_true = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = dictionary_type_true\n",
    "s = s_true\n",
    "k = k_true\n",
    "K0_coll = np.arange(5, 26, 4) # K0_coll = 5:4:25 %4:4:40 %5:3:20\n",
    "lambda_ = 1e-6 # l2 multiplier\n",
    "max_iter = 100 # Maximum number of iterations\n",
    "patience = 5 # Patience\n",
    "tol = 1e-7 # Tolerance for Patience\n",
    "n_sim = 10\n",
    "verbose = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D, h, Y_train, Y_test, epsilon, c, X_train, X_test = create_ground_truth(Lu,\n",
    "#                                                                          Ld,\n",
    "#                                                                          m_train,\n",
    "#                                                                          m_test, \n",
    "#                                                                          s, \n",
    "#                                                                          k, \n",
    "#                                                                          K0_coll[0], \n",
    "#                                                                          dictionary_type, \n",
    "#                                                                          sparsity_mode)\n",
    "\n",
    "# D0, X0, discard = initialize_dic(Lu,\n",
    "#                                  Ld, \n",
    "#                                  s, \n",
    "#                                  k, \n",
    "#                                  Y_train, \n",
    "#                                  K0_max, \n",
    "#                                  dictionary_type, \n",
    "#                                  c, \n",
    "#                                  epsilon, \n",
    "#                                  only=\"only_X\")\n",
    "\n",
    "\n",
    "# D_sim = topological_dictionary_learn(Y_train, \n",
    "#                                      Y_test, \n",
    "#                                      k, \n",
    "#                                      n, \n",
    "#                                      s, \n",
    "#                                      X_train, \n",
    "#                                      D0, \n",
    "#                                      X0, \n",
    "#                                      Lu, \n",
    "#                                      Ld, \n",
    "#                                      dictionary_type, \n",
    "#                                      c, \n",
    "#                                      epsilon, \n",
    "#                                      K0_max, \n",
    "#                                      lambda_, \n",
    "#                                      max_iter, \n",
    "#                                      patience, \n",
    "#                                      tol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1062,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3000/3000 [03:23<00:00, 14.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3000/3000 [03:12<00:00, 15.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3000/3000 [03:14<00:00, 15.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 3000/3000 [03:14<00:00, 15.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "mat = scipy.io.loadmat('datiSC.mat')\n",
    "B1 = mat[\"B1\"]\n",
    "B2 = mat[\"B2\"]\n",
    "\n",
    "# Subsampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1, dtype=float)\n",
    "Lu = np.matmul(B2, np.transpose(B2), dtype=float)\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "_ ,U = la.eig(L)\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type_true = \"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s_true = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k_true = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = dictionary_type_true\n",
    "s = s_true\n",
    "k = k_true\n",
    "K0_coll = np.arange(5, 26, 4) # K0_coll = 5:4:25 %4:4:40 %5:3:20\n",
    "lambda_ = 1e-6 # l2 multiplier\n",
    "max_iter = 100 # Maximum number of iterations\n",
    "patience = 5 # Patience\n",
    "tol = 1e-7 # Tolerance for Patience\n",
    "n_sim = 10\n",
    "verbose = 0\n",
    "\n",
    "\n",
    "D_true = np.zeros((n, n * s_true, n_sim))\n",
    "D_true_coll = np.zeros((n, n, s_true, n_sim))\n",
    "Y_train = np.zeros((n, m_train, n_sim))\n",
    "Y_test = np.zeros((n, m_test, n_sim))\n",
    "epsilon_true = np.zeros(n_sim)\n",
    "c_true = np.zeros(n_sim)\n",
    "X_train = np.zeros((n * s, m_train, n_sim))\n",
    "X_test = np.zeros((n * s, m_test, n_sim))\n",
    "n_search = 3000\n",
    "\n",
    "for sim in range(6, n_sim):\n",
    "    # from there I can create a function to parallelize by the number of simulations\n",
    "    best_sparsity = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for i in tqdm(range(n_search)):\n",
    "        # try:\n",
    "        D_try, h, Y_train_try, Y_test_try, epsilon_try, c_try, X_train_try, X_test_try = create_ground_truth(Lu,\n",
    "                                                                                Ld,\n",
    "                                                                                m_train,\n",
    "                                                                                m_test, \n",
    "                                                                                s, \n",
    "                                                                                k, \n",
    "                                                                                K0_max, \n",
    "                                                                                dictionary_type, \n",
    "                                                                                sparsity_mode)\n",
    "        # Replace with your actual function to verify dictionaries\n",
    "        max_possible_sparsity, acc = verify_dic(D_try, Y_train_try, X_train_try, K0_max, .7)\n",
    "        if max_possible_sparsity > best_sparsity:\n",
    "            best_sparsity = max_possible_sparsity\n",
    "            best_acc = acc\n",
    "            D_true[:, :, sim] = D_try\n",
    "            # D_true_coll[:, :, :, sim] = D_true_coll_tries\n",
    "            Y_train[:, :, sim] = Y_train_try\n",
    "            Y_test[:, :, sim] = Y_test_try\n",
    "            epsilon_true[sim] = epsilon_try\n",
    "            c_true[sim] = c_try\n",
    "            X_train[:, :, sim] = X_train_try\n",
    "            X_test[:, :, sim] = X_test_try\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Error during dictionary creation: {e}\")\n",
    "\n",
    "    print(f\"...Done! # Best Sparsity: {best_sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1064,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 150, 10)"
      ]
     },
     "execution_count": 1064,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1074,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = Y_train[:,:,1]\n",
    "y_te = Y_test[:,:,0]\n",
    "X_tr = X_train[:,:,0]\n",
    "X_te = X_test[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1079,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 1079,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K0_coll[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The number of atoms cannot be more than the number of features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1076], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43minitialize_dic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK0_coll\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_true\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon_true\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monly_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1052], line 82\u001b[0m, in \u001b[0;36minitialize_dic\u001b[1;34m(Lu, Ld, s, K, Y_train, K0, dictionary_type, c, epsilon, only)\u001b[0m\n\u001b[0;32m     80\u001b[0m     W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mdd)  \n\u001b[0;32m     81\u001b[0m     Domp \u001b[38;5;241m=\u001b[39m Dx\u001b[38;5;129m@W\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(X, (s,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m D, X, discard\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind0], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[1;32mIn[1052], line 82\u001b[0m, in \u001b[0;36minitialize_dic.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     80\u001b[0m     W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mdd)  \n\u001b[0;32m     81\u001b[0m     Domp \u001b[38;5;241m=\u001b[39m Dx\u001b[38;5;129m@W\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_train\u001b[38;5;241m.\u001b[39mreal)\n\u001b[0;32m     83\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(X, (s,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m D, X, discard\n",
      "Cell \u001b[1;32mIn[1052], line 6\u001b[0m, in \u001b[0;36mget_omp_coeff\u001b[1;34m(K0, Domp, col)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_omp_coeff\u001b[39m(K0, Domp, col):\n\u001b[0;32m      5\u001b[0m     omp \u001b[38;5;241m=\u001b[39m OrthogonalMatchingPursuit(n_nonzero_coefs\u001b[38;5;241m=\u001b[39mK0)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43momp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m omp\u001b[38;5;241m.\u001b[39mcoef_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:772\u001b[0m, in \u001b[0;36mOrthogonalMatchingPursuit.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    770\u001b[0m     norms_sq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     coef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43morthogonal_mp_gram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mGram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_nonzero_coefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_nonzero_coefs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorms_squared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorms_sq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_Gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_Xy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m coef_\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_intercept(X_offset, y_offset, X_scale)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:554\u001b[0m, in \u001b[0;36morthogonal_mp_gram\u001b[1;34m(Gram, Xy, n_nonzero_coefs, tol, norms_squared, copy_Gram, copy_Xy, return_path, return_n_iter)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of atoms must be positive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m n_nonzero_coefs \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(Gram):\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of atoms cannot be more than the number of features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[0;32m    559\u001b[0m     coef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(Gram), Xy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(Gram)), dtype\u001b[38;5;241m=\u001b[39mGram\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: The number of atoms cannot be more than the number of features"
     ]
    }
   ],
   "source": [
    "initialize_dic(Lu, Ld, s, k, y_tr, K0_coll[1], dictionary_type, c_true[1], epsilon_true[1], \"only_X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1073,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00, 15.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "0\n",
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of atoms cannot be more than the number of features",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1073], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(k0)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m discard \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 26\u001b[0m     D0, X0, discard \u001b[38;5;241m=\u001b[39m \u001b[43minitialize_dic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdictionary_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monly_X\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1052], line 82\u001b[0m, in \u001b[0;36minitialize_dic\u001b[1;34m(Lu, Ld, s, K, Y_train, K0, dictionary_type, c, epsilon, only)\u001b[0m\n\u001b[0;32m     80\u001b[0m     W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mdd)  \n\u001b[0;32m     81\u001b[0m     Domp \u001b[38;5;241m=\u001b[39m Dx\u001b[38;5;129m@W\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(X, (s,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m D, X, discard\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind0], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[1;32mIn[1052], line 82\u001b[0m, in \u001b[0;36minitialize_dic.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     80\u001b[0m     W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39mdd)  \n\u001b[0;32m     81\u001b[0m     Domp \u001b[38;5;241m=\u001b[39m Dx\u001b[38;5;129m@W\u001b[39m\n\u001b[1;32m---> 82\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_train\u001b[38;5;241m.\u001b[39mreal)\n\u001b[0;32m     83\u001b[0m     X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(X, (s,\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m D, X, discard\n",
      "Cell \u001b[1;32mIn[1052], line 6\u001b[0m, in \u001b[0;36mget_omp_coeff\u001b[1;34m(K0, Domp, col)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_omp_coeff\u001b[39m(K0, Domp, col):\n\u001b[0;32m      5\u001b[0m     omp \u001b[38;5;241m=\u001b[39m OrthogonalMatchingPursuit(n_nonzero_coefs\u001b[38;5;241m=\u001b[39mK0)\n\u001b[1;32m----> 6\u001b[0m     \u001b[43momp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m omp\u001b[38;5;241m.\u001b[39mcoef_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:772\u001b[0m, in \u001b[0;36mOrthogonalMatchingPursuit.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    770\u001b[0m     norms_sq \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(y\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 772\u001b[0m     coef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43morthogonal_mp_gram\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[43mGram\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    774\u001b[0m \u001b[43m        \u001b[49m\u001b[43mXy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mXy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    775\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_nonzero_coefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_nonzero_coefs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    776\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    777\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorms_squared\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorms_sq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_Gram\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy_Xy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_n_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_ \u001b[38;5;241m=\u001b[39m coef_\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    783\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_intercept(X_offset, y_offset, X_scale)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:554\u001b[0m, in \u001b[0;36morthogonal_mp_gram\u001b[1;34m(Gram, Xy, n_nonzero_coefs, tol, norms_squared, copy_Gram, copy_Xy, return_path, return_n_iter)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of atoms must be positive\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tol \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m n_nonzero_coefs \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(Gram):\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    555\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe number of atoms cannot be more than the number of features\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    556\u001b[0m     )\n\u001b[0;32m    558\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[0;32m    559\u001b[0m     coef \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(Gram), Xy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mlen\u001b[39m(Gram)), dtype\u001b[38;5;241m=\u001b[39mGram\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[1;31mValueError\u001b[0m: The number of atoms cannot be more than the number of features"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initializations of Error Storing Variables\n",
    "min_error_edge_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "np.random.seed(0)\n",
    "verbose = 0\n",
    "for sim in range(n_sim):\n",
    "    c = c_true[sim]  # d\n",
    "    epsilon = epsilon_true[sim]  # epsilon\n",
    "    for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "        # Initializations\n",
    "        discard = 1\n",
    "        print(sim)\n",
    "        print(k0)\n",
    "        while discard == 1:\n",
    "            D0, X0, discard = initialize_dic(Lu, Ld, s, k, Y_train[:, :, sim], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "#         try:\n",
    "#             h_opt_edge, _, _, min_error_edge_train[sim, k0_index], min_error_edge_test[sim, k0_index] = topological_dictionary_learn(Y_train[:,:,sim], Y_test[:,:,sim], k, n, s, D0, X0, Lu, Ld, \"edge_laplacian\", c, epsilon, k0, lambda_, max_iter, patience, tol)\n",
    "#             print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing Edge Laplacian... Done! Test Error: {min_error_edge_test[sim, k0_index]}\")\n",
    "#         except Exception as e:\n",
    "#             min_error_edge_train[sim, k0_index] = 100\n",
    "#             min_error_edge_test[sim, k0_index] = 100\n",
    "#             print(f\"Divergent Run in Edge Laplacian... Discarded! Error: {e}\")\n",
    "#         try:\n",
    "#             h_opt_joint, _, _, min_error_joint_train[sim, k0_index], min_error_joint_test[sim, k0_index] = topological_dictionary_learn(Y_train[:,:,sim], Y_test[:,:,sim], k, n, s, D0, X0, Lu, Ld, \"joint\", c, epsilon, k0, lambda_, max_iter, patience, tol)\n",
    "#             print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing Joint... Done! Test Error: {min_error_joint_test[sim, k0_index]}\")\n",
    "#         except Exception as e:\n",
    "#             min_error_joint_train[sim, k0_index] = 100\n",
    "#             min_error_joint_test[sim, k0_index] = 100\n",
    "#             print(f\"Divergent Run in joint... Discarded! Error: {e}\")\n",
    "#         try:\n",
    "#             h_opt_sep, _, _, min_error_sep_train[sim, k0_index], min_error_sep_test[sim, k0_index] = topological_dictionary_learn(Y_train[:,:,sim], Y_test[:,:,sim], k, n, s, D0, X0, Lu, Ld, \"separated\", c, epsilon, k0, lambda_, max_iter, patience, tol)\n",
    "#             print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing Separated... Done! Test Error: {min_error_sep_test[sim, k0_index]}\")\n",
    "#         except Exception as e:\n",
    "#             min_error_sep_train[sim, k0_index] = 100\n",
    "#             min_error_sep_test[sim, k0_index] = 100\n",
    "#             print(f\"Divergent Run in Separated... Discarded! Error: {e}\")\n",
    "\n",
    "\n",
    "# # Plotting -----> I want to use a ppd.DataFrame for the simulations' values and then sns for plotting\n",
    "# if n_sim > 1:\n",
    "#     plt.semilogy(K0_coll, np.mean(min_error_fou_test, axis=0), label=\"Fourier\")\n",
    "#     plt.semilogy(K0_coll, np.mean(min_error_edge_test, axis=0), label=\"Edge Laplacian\")\n",
    "#     plt.semilogy(K0_coll, np.mean(min_error_joint_test, axis=0), label=\"Joint\")\n",
    "#     plt.semilogy(K0_coll, np.mean(min_error_sep_test, axis=0), label=\"Separated\")\n",
    "# else:\n",
    "#     plt.semilogy(K0_coll, min_error_fou_test[0], label=\"Fourier\")\n",
    "#     plt.semilogy(K0_coll, min_error_edge_test[0], label=\"Edge Laplacian\")\n",
    "#     plt.semilogy(K0_coll, min_error_joint_test[0], label=\"Joint\")\n",
    "#     plt.semilogy(K0_coll, min_error_sep_test[0], label=\"Separated\")\n",
    "\n",
    "# plt.legend()\n",
    "# plt.title(f\"True dictionary: {dictionary_type_true}\")\n",
    "# plt.xlabel('Sparsity')\n",
    "# plt.ylabel('Error')\n",
    "# plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
