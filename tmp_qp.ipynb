{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p=0.162, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1, dtype=float)\n",
    "Lu = np.matmul(B2, np.transpose(B2), dtype=float)\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = \"\"\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 30 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "n_sim = 10\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_type = \"separated\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:24<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:58<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:55<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:56<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:54<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:59<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:01<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:05<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:45<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:20<00:00, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dictionary_type = \"separated\"\n",
    "\n",
    "D_true = np.zeros((n, n * s, n_sim))\n",
    "D_true_coll = np.zeros((n, n, s, n_sim))\n",
    "Y_train = np.zeros((n, m_train, n_sim))\n",
    "Y_test = np.zeros((n, m_test, n_sim))\n",
    "epsilon_true = np.zeros(n_sim)\n",
    "c_true = np.zeros(n_sim)\n",
    "X_train = np.zeros((n * s, m_train, n_sim))\n",
    "X_test = np.zeros((n * s, m_test, n_sim))\n",
    "n_search = 3000\n",
    "\n",
    "for sim in range(n_sim):\n",
    "    best_sparsity = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for i in tqdm(range(n_search)):\n",
    "        try:\n",
    "            D_try, h, Y_train_try, Y_test_try, epsilon_try, c_try, X_train_try, X_test_try = create_ground_truth(Lu,\n",
    "                                                                                    Ld,\n",
    "                                                                                    m_train,\n",
    "                                                                                    m_test, \n",
    "                                                                                    s=s, \n",
    "                                                                                    K=k, \n",
    "                                                                                    K0=K0_max, \n",
    "                                                                                    dictionary_type=dictionary_type, \n",
    "                                                                                    sparsity_mode=sparsity_mode)\n",
    "            \n",
    "            max_possible_sparsity, acc = verify_dic(D_try, Y_train_try, X_train_try, K0_max, .7)\n",
    "            if max_possible_sparsity > best_sparsity:\n",
    "                best_sparsity = max_possible_sparsity\n",
    "                best_acc = acc\n",
    "                D_true[:, :, sim] = D_try\n",
    "                Y_train[:, :, sim] = Y_train_try\n",
    "                Y_test[:, :, sim] = Y_test_try\n",
    "                epsilon_true[sim] = epsilon_try\n",
    "                c_true[sim] = c_try\n",
    "                X_train[:, :, sim] = X_train_try\n",
    "                X_test[:, :, sim] = X_test_try\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dictionary creation: {e}\")\n",
    "    if verbose:\n",
    "        print(f\"...Done! # Best Sparsity: {best_sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, K)\n",
    "        elif dictionary_type==\"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "\n",
    "        # Init the dictionary and the sparse representation \n",
    "        D_coll = [cp.Constant(D0[:,(n*i):(n*(i+1))]) for i in range(s)]\n",
    "        Y = cp.Constant(Y_train)\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            # Init constants and parameters\n",
    "            D_coll = [cp.Constant(np.zeros((n, n))) for i in range(s)] \n",
    "            Dsum = cp.Constant(np.zeros((n, n)))\n",
    "            X = cp.Constant(X_train)\n",
    "            I = cp.Constant(np.eye(n))\n",
    "            \n",
    "            # Define the objective function\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                # Init the variables\n",
    "                h = cp.Variable((s, K))\n",
    "                hI = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += (cp.Constant(Lk[j, :, :]) * h[i,j])\n",
    "                    tmp += (I*hI[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "            else:\n",
    "                # Init the variables\n",
    "                hI = cp.Variable((s, K))\n",
    "                hS = cp.Variable((s, K))\n",
    "                hH = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += ((cp.Constant(Luk[j, :, :])*hS[i,j]) + (cp.Constant(Ldk[j, :, :])*hI[i,j]))\n",
    "                    tmp += (I*hH[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                \n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            # Define the constraints\n",
    "            constraints = [D_coll[i] >> 0 for i in range(s)] + \\\n",
    "                            [(cp.multiply(c, I) - D_coll[i]) >> 0 for i in range(s)] + \\\n",
    "                            [(Dsum - cp.multiply((c - epsilon), I)) >> 0, (cp.multiply((c + epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalize?\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI.value, hS.value, hH.value])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- find the dimensions of X_train and divide X_train in blocks like D_coll (to find X_p for each dictionary D_p)\n",
    "- check the dimensionality of L_k to understand the multiplication of L_k by X_p (or X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "dill.load_session(path+'\\\\results\\\\separated\\\\ipynb_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLARABEL', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLPK', 'GLPK_MI', 'MOSEK', 'OSQP', 'SCIPY', 'SCS', 'SDPA']\n"
     ]
    }
   ],
   "source": [
    "print(cp.installed_solvers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def compute_vandermonde(L):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            L = Lu + Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermonde(L)\n",
    "            B = cp.Constant(B.real)\n",
    "\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            L = Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermonde(L)\n",
    "            B = cp.Constant(B.real)\n",
    "        elif dictionary_type==\"separated\":\n",
    "\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "            Bu = compute_vandermonde(Lu)\n",
    "            Bd = compute_vandermonde(Ld)\n",
    "            B = cp.hstack([Bu.real, Bd[:, 1:].real])\n",
    "\n",
    "        # Init the the sparse representation \n",
    "        Y = cp.Constant(Y_train)\n",
    "        I = np.eye(n)\n",
    "        I_2 = cp.Constant(np.eye(s*(2*K+1)))\n",
    "        I_s = cp.Constant(np.eye(s))\n",
    "        i_s = cp.Constant(np.ones((s,1)))\n",
    "        X_train = X0\n",
    "\n",
    "        def aux_matrix(Lu,Ld,X_train,K,N):\n",
    "\n",
    "            LL = [np.eye(N)]\n",
    "            LL_tmp = []\n",
    "            for i in range(1, K + 1):\n",
    "                LL.append(la.matrix_power(Lu, i))\n",
    "                LL_tmp.append(la.matrix_power(Ld, i))\n",
    "\n",
    "            LL = LL + LL_tmp\n",
    "            LL = np.array(LL)\n",
    "            P = np.array([LL@X_train[(i*n): ((i+1)*n), :] for i in range(s)])\n",
    "\n",
    "            return P\n",
    "        \n",
    "        P = aux_matrix(Lu,Ld,X_train,K,n)\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # QP Step\n",
    "            \n",
    "            # # Define the objective function\n",
    "            # if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "            #     # Init the variables\n",
    "            #     break\n",
    "\n",
    "            # else:\n",
    "            \n",
    "\n",
    "            # Init variables and parameters\n",
    "            h = cp.Variable((s*(2*k+1), 1))\n",
    "            l = cp.Constant(np.zeros((s*(2*k+1), 1)))\n",
    "            Q = cp.Constant(np.zeros((s*(2*k+1), s*(2*k+1))))\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(m_train):\n",
    "                    Pij = cp.Constant(P[:,:,i,j].flatten().reshape(-1,1))\n",
    "                    l = l + (Y[i,j]*Pij)\n",
    "                    Q = Q + Pij@Pij.T\n",
    "\n",
    "            Q = Q + cp.multiply(lambda_, I_2)\n",
    "            # Quadratic term\n",
    "            term2 = cp.quad_form(h, Q, assume_PSD = True)\n",
    "            # Linear term\n",
    "            term1 = (-2)*l.T@h\n",
    "            \n",
    "            obj = cp.Minimize(term2+term1)\n",
    "\n",
    "            # Define the constraints\n",
    "            cons1 = cp.kron(I_s,B)@h\n",
    "            cons2 = cp.kron(i_s.T,B)@h\n",
    "            bound1 = cp.Constant(np.ones((n*s,1)))\n",
    "            bound2 = cp.Constant(np.ones((n,1)))\n",
    "            constraints = [cons1 >= cp.multiply(0, bound1)] + \\\n",
    "                            [cons1 <= cp.multiply(c, bound1)] + \\\n",
    "                            [cons2 >= cp.multiply((c-epsilon), bound2)] + \\\n",
    "                            [cons2 <= cp.multiply((c+epsilon), bound2)]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK)\n",
    "            \n",
    "            # Update the dictionary\n",
    "            h = h.value.flatten()\n",
    "            D_coll = []\n",
    "            hH = h[:s].reshape((s,1))\n",
    "            hS = h[s:((s*K)+s)].reshape((s,k))\n",
    "            hI = h[((s*K)+s):].reshape((s,k))\n",
    "\n",
    "            for i in range(0,s):\n",
    "                # tmp =  np.zeros((n, n))\n",
    "                # for j in range(0,K):\n",
    "                #     tmp += ((Luk[j, :, :])*hS[(i*(s-1)+j)]) + ((Ldk[j, :, :])*hI[(i*(s-1)+j)])\n",
    "                # tmp += (I*hH[i])\n",
    "                hu = hS[i].reshape(k,1,1)\n",
    "                hd = hI[i].reshape(k,1,1)\n",
    "                hid = hH[i]\n",
    "                tmp = np.sum(hu*Luk + hd*Ldk, axis=0) + hid*np.eye(n,n)\n",
    "                D_coll.append(tmp)\n",
    "\n",
    "            D = np.hstack(tuple(D_coll))\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalize\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI, hS, hH])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_matrix(Lu,Ld,X_train,K,N):\n",
    "\n",
    "    LL = [np.eye(N)]\n",
    "    LL_tmp = []\n",
    "    for i in range(1, K + 1):\n",
    "        LL.append(la.matrix_power(Lu, i))\n",
    "        LL_tmp.append(la.matrix_power(Ld, i))\n",
    "\n",
    "\n",
    "    LL= LL + LL_tmp\n",
    "    LL = np.array(LL)\n",
    "    P = np.array([LL@X_train[(i*n): ((i+1)*n), :] for i in range(s)])\n",
    "\n",
    "    return P, LL\n",
    "\n",
    "P , LL= aux_matrix(Lu, Ld, X_train[:,:,7], k, n)\n",
    "I_2 = cp.Constant(np.eye(s*(2*k+1)))\n",
    "\n",
    "# Init variable \n",
    "h = cp.Variable((s*(2*k+1), 1))\n",
    "c = cp.Constant(np.zeros((s*(2*k+1), 1)))\n",
    "Q = cp.Constant(np.zeros((s*(2*k+1), s*(2*k+1))))\n",
    "\n",
    "for i in range(n):\n",
    "    for j in range(m_train):\n",
    "        Pij = cp.Constant(P[:,:,i,j].flatten().reshape(-1,1))\n",
    "        c = c + (Y_train[:,:,7][i,j]*Pij)\n",
    "        Q = Q + Pij@Pij.T\n",
    "\n",
    "Q = Q + cp.multiply(lambda_, I_2)\n",
    "term2 = cp.quad_form(h, Q)\n",
    "term1 = cp.multiply(-2, c.T@h)\n",
    "\n",
    "# Check convexity\n",
    "assert np.all(np.linalg.eigvals(Q.value) >= 0), f'element {(i,j)} not positive semi-definite'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = cp.Variable((s*(2*k+1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "h.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = cp.Constant(np.ones((s*(2*k+1), 1))).value.flatten()\n",
    "\n",
    "hH = h[:s]\n",
    "hS = h[s:((s*k)+s)]\n",
    "hI = h[((s*k)+s):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hI.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.linalg.ishermitian(Q.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.all(Q.value == Q.T.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8.44892279e+05, 7.92792867e+05, 7.21606356e+05, 2.40006786e+05,\n",
       "       2.26664942e+05, 1.65928087e+05, 5.82632652e+01, 7.39616138e+01,\n",
       "       7.13863804e+01, 9.08104578e+02, 9.63831258e+02, 7.66645640e+02,\n",
       "       5.05784958e+02, 6.02422921e+02, 6.63128410e+02])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.eigvals(Q.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:33, 153.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 7/10 Sparsity: 5 Testing Separated Hodge Laplacian... Done! Test Error: 0.08087501978562617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [05:21, 162.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 7/10 Sparsity: 9 Testing Separated Hodge Laplacian... Done! Test Error: 0.03028946014671574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [08:07, 163.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 7/10 Sparsity: 13 Testing Separated Hodge Laplacian... Done! Test Error: 0.015102613661098399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [10:48, 162.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 7/10 Sparsity: 17 Testing Separated Hodge Laplacian... Done! Test Error: 0.009149020365912636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [13:37, 165.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 7/10 Sparsity: 21 Testing Separated Hodge Laplacian... Done! Test Error: 0.006280094506717785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [16:20, 163.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 7/10 Sparsity: 25 Testing Separated Hodge Laplacian... Done! Test Error: 0.004636052395979597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:40, 160.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 5 Testing Separated Hodge Laplacian... Done! Test Error: 0.13649756489307577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [05:19, 159.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 9 Testing Separated Hodge Laplacian... Done! Test Error: 0.13626372224202069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [08:02, 161.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 13 Testing Separated Hodge Laplacian... Done! Test Error: 0.05714689398060823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [10:43, 161.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 17 Testing Separated Hodge Laplacian... Done! Test Error: 0.038388216921671325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [13:34, 164.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 21 Testing Separated Hodge Laplacian... Done! Test Error: 0.014545727536205845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [16:14, 162.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 8/10 Sparsity: 25 Testing Separated Hodge Laplacian... Done! Test Error: 0.010036156947665648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [02:40, 160.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 9/10 Sparsity: 5 Testing Separated Hodge Laplacian... Done! Test Error: 0.08153360112225705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [05:20, 160.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 9/10 Sparsity: 9 Testing Separated Hodge Laplacian... Done! Test Error: 0.02842892294320222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [07:36, 228.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization Failed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dict_types\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     dict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m0\u001b[39m][sim,k0_index], dict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m1\u001b[39m][sim,k0_index], _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopological_dictionary_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_sim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sparsity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Testing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... Done! Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m1\u001b[39m][sim,k0_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[87], line 188\u001b[0m, in \u001b[0;36mtopological_dictionary_learn\u001b[1;34m(Y_train, Y_test, K, n, s, D0, X0, Lu, Ld, dictionary_type, c, epsilon, K0, lambda_, max_iter, patience, tol, verbose)\u001b[0m\n\u001b[0;32m    186\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m dd)\n\u001b[0;32m    187\u001b[0m Domp \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m--> 188\u001b[0m X_train \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_omp_coeff(K0, Domp\u001b[38;5;241m=\u001b[39mDomp, col\u001b[38;5;241m=\u001b[39mx), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_test)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Normalize\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\shape_base.py:402\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m buff[ind0] \u001b[38;5;241m=\u001b[39m res\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m inds:\n\u001b[1;32m--> 402\u001b[0m     buff[ind] \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(res, matrix):\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;66;03m# wrap the array, to preserve subclasses\u001b[39;00m\n\u001b[0;32m    406\u001b[0m     buff \u001b[38;5;241m=\u001b[39m res\u001b[38;5;241m.\u001b[39m__array_wrap__(buff)\n",
      "Cell \u001b[1;32mIn[87], line 188\u001b[0m, in \u001b[0;36mtopological_dictionary_learn.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    186\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m dd)\n\u001b[0;32m    187\u001b[0m Domp \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m--> 188\u001b[0m X_train \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_train)\n\u001b[0;32m    189\u001b[0m X_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: get_omp_coeff(K0, Domp\u001b[38;5;241m=\u001b[39mDomp, col\u001b[38;5;241m=\u001b[39mx), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_test)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Normalize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\engri\\Desktop\\tesi\\TSP-DictionaryLearning\\tsplearn\\data_gen.py:236\u001b[0m, in \u001b[0;36mget_omp_coeff\u001b[1;34m(K0, Domp, col)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03mCompute the coefficients using Orthogonal Matching Pursuit.\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m    np.ndarray: Coefficients.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    235\u001b[0m omp \u001b[38;5;241m=\u001b[39m OrthogonalMatchingPursuit(n_nonzero_coefs\u001b[38;5;241m=\u001b[39mK0)\n\u001b[1;32m--> 236\u001b[0m \u001b[43momp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omp\u001b[38;5;241m.\u001b[39mcoef_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:745\u001b[0m, in \u001b[0;36mOrthogonalMatchingPursuit.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    742\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, y_numeric\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    743\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 745\u001b[0m X, y, X_offset, y_offset, X_scale, Gram, Xy \u001b[38;5;241m=\u001b[39m \u001b[43m_pre_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecompute\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_normalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    747\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    750\u001b[0m     y \u001b[38;5;241m=\u001b[39m y[:, np\u001b[38;5;241m.\u001b[39mnewaxis]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_base.py:805\u001b[0m, in \u001b[0;36m_pre_fit\u001b[1;34m(X, y, Xy, precompute, normalize, fit_intercept, copy, check_input, sample_weight)\u001b[0m\n\u001b[0;32m    794\u001b[0m     X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    795\u001b[0m         X,\n\u001b[0;32m    796\u001b[0m         y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    802\u001b[0m     )\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;66;03m# copy was done in fit if necessary\u001b[39;00m\n\u001b[1;32m--> 805\u001b[0m     X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m \u001b[43m_preprocess_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;66;03m# Rescale only in dense case. Sparse cd solver directly deals with\u001b[39;00m\n\u001b[0;32m    815\u001b[0m     \u001b[38;5;66;03m# sample_weight.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    817\u001b[0m         \u001b[38;5;66;03m# This triggers copies anyway.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_base.py:231\u001b[0m, in \u001b[0;36m_preprocess_data\u001b[1;34m(X, y, fit_intercept, normalize, copy, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    228\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(sample_weight)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_input:\n\u001b[1;32m--> 231\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:919\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    915\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    916\u001b[0m         )\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 919\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    927\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 122\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36msum\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\fromnumeric.py:2298\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2295\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2299\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ufunc\u001b[38;5;241m.\u001b[39mreduce(obj, axis, dtype, out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "min_error_fou_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "dict_errors = {\n",
    "    \"sep\": (min_error_sep_train,min_error_sep_test)\n",
    "    }\n",
    "\n",
    "\n",
    "dict_types = {\n",
    "    \"sep\": (\"Separated Hodge Laplacian\",\"separated\")\n",
    "    }\n",
    "\n",
    "for sim in range(6,n_sim):\n",
    "    c = c_true[sim]  \n",
    "    epsilon = epsilon_true[sim] \n",
    "    for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "        discard = 1\n",
    "        while discard == 1:\n",
    "            \n",
    "            try:\n",
    "                D0, X0, discard = initialize_dic(Lu, Ld, s, k, Y_train[:, :, sim], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "            except:\n",
    "                print(\"Initialization Failed!\")\n",
    "\n",
    "        for d in dict_types.items():\n",
    "            # try:\n",
    "            dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index], _, _, _ = topological_dictionary_learn(Y_train[:,:,sim], Y_test[:,:,sim],\n",
    "                                                                                                                        k, n, s, D0, X0, Lu, Ld, d[1][1],\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)\n",
    "            if verbose:\n",
    "                print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Done! Test Error: {dict_errors[d[0]][1][sim,k0_index]}\")\n",
    "            # except:\n",
    "            #     print(f'Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Diverged!')\n",
    "            #     try:\n",
    "            #         dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index] = (dict_errors[d[0]][0][sim-1,k0_index]\n",
    "            #                                                                               , dict_errors[d[0]][1][sim-1,k0_index])\n",
    "            #     except:\n",
    "            #         dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index] = (dict_errors[d[0]][0][sim+1,k0_index]\n",
    "            #                                                                               , dict_errors[d[0]][1][sim+1,k0_index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
