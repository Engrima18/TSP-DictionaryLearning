{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p=0.162, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "\n",
    "# Laplacians\n",
    "Ld = np.matmul(np.transpose(B1), B1, dtype=float)\n",
    "Lu = np.matmul(B2, np.transpose(B2), dtype=float)\n",
    "L = Lu+Ld\n",
    "n =  L.shape[0]\n",
    "nu = B2.shape[1]\n",
    "nd = B1.shape[1]\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "s = 3 # Number of Kernels (Sub-dictionaries)\n",
    "k = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(n*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "dictionary_type = \"\"\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 100 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "n_sim = 10\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:24<00:00, 11.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:58<00:00, 10.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:55<00:00, 10.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:56<00:00, 10.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:54<00:00, 10.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:59<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:01<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [05:05<00:00,  9.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:45<00:00, 10.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3000/3000 [04:20<00:00, 11.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Done! # Best Sparsity: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "dictionary_type = \"separated\"\n",
    "\n",
    "D_true = np.zeros((n, n * s, n_sim))\n",
    "D_true_coll = np.zeros((n, n, s, n_sim))\n",
    "Y_train = np.zeros((n, m_train, n_sim))\n",
    "Y_test = np.zeros((n, m_test, n_sim))\n",
    "epsilon_true = np.zeros(n_sim)\n",
    "c_true = np.zeros(n_sim)\n",
    "X_train = np.zeros((n * s, m_train, n_sim))\n",
    "X_test = np.zeros((n * s, m_test, n_sim))\n",
    "n_search = 3000\n",
    "\n",
    "for sim in range(n_sim):\n",
    "    best_sparsity = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for i in tqdm(range(n_search)):\n",
    "        try:\n",
    "            D_try, h, Y_train_try, Y_test_try, epsilon_try, c_try, X_train_try, X_test_try = create_ground_truth(Lu,\n",
    "                                                                                    Ld,\n",
    "                                                                                    m_train,\n",
    "                                                                                    m_test, \n",
    "                                                                                    s=s, \n",
    "                                                                                    K=k, \n",
    "                                                                                    K0=K0_max, \n",
    "                                                                                    dictionary_type=dictionary_type, \n",
    "                                                                                    sparsity_mode=sparsity_mode)\n",
    "            \n",
    "            max_possible_sparsity, acc = verify_dic(D_try, Y_train_try, X_train_try, K0_max, .7)\n",
    "            if max_possible_sparsity > best_sparsity:\n",
    "                best_sparsity = max_possible_sparsity\n",
    "                best_acc = acc\n",
    "                D_true[:, :, sim] = D_try\n",
    "                Y_train[:, :, sim] = Y_train_try\n",
    "                Y_test[:, :, sim] = Y_test_try\n",
    "                epsilon_true[sim] = epsilon_try\n",
    "                c_true[sim] = c_try\n",
    "                X_train[:, :, sim] = X_train_try\n",
    "                X_test[:, :, sim] = X_test_try\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error during dictionary creation: {e}\")\n",
    "    if verbose:\n",
    "        print(f\"...Done! # Best Sparsity: {best_sparsity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Lu + Ld, K)\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(Ld, K)\n",
    "        elif dictionary_type==\"separated\":\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "\n",
    "        # Init the dictionary and the sparse representation \n",
    "        D_coll = [cp.Constant(D0[:,(n*i):(n*(i+1))]) for i in range(s)]\n",
    "        Y = cp.Constant(Y_train)\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # SDP Step\n",
    "            # Init constants and parameters\n",
    "            D_coll = [cp.Constant(np.zeros((n, n))) for i in range(s)] \n",
    "            Dsum = cp.Constant(np.zeros((n, n)))\n",
    "            X = cp.Constant(X_train)\n",
    "            I = cp.Constant(np.eye(n))\n",
    "            \n",
    "            # Define the objective function\n",
    "            if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                # Init the variables\n",
    "                h = cp.Variable((s, K))\n",
    "                hI = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += (cp.Constant(Lk[j, :, :]) * h[i,j])\n",
    "                    tmp += (I*hI[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "            else:\n",
    "                # Init the variables\n",
    "                hI = cp.Variable((s, K))\n",
    "                hS = cp.Variable((s, K))\n",
    "                hH = cp.Variable((s, 1))\n",
    "                for i in range(0,s):\n",
    "                    tmp =  cp.Constant(np.zeros((n, n)))\n",
    "                    for j in range(0,K):\n",
    "                        tmp += ((cp.Constant(Luk[j, :, :])*hS[i,j]) + (cp.Constant(Ldk[j, :, :])*hI[i,j]))\n",
    "                    tmp += (I*hH[i])\n",
    "                    D_coll[i] = tmp\n",
    "                    Dsum += tmp\n",
    "                D = cp.hstack([D_coll[i]for i in range(s)])\n",
    "                \n",
    "                term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "            # Define the constraints\n",
    "            constraints = [D_coll[i] >> 0 for i in range(s)] + \\\n",
    "                            [(cp.multiply(c, I) - D_coll[i]) >> 0 for i in range(s)] + \\\n",
    "                            [(Dsum - cp.multiply((c - epsilon), I)) >> 0, (cp.multiply((c + epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=False)\n",
    "            # Update the dictionary\n",
    "            D = D.value\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalize?\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI.value, hS.value, hH.value])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO DO\n",
    "\n",
    "- find the dimensions of X_train and divide X_train in blocks like D_coll (to find X_p for each dictionary D_p)\n",
    "- check the dimensionality of L_k to understand the multiplication of L_k by X_p (or X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynomial_exp(x, k):\n",
    "    x = x** np.arange(0, k + 1)\n",
    "    return x\n",
    "\n",
    "eigenvalues, _ = sla.eig(L)\n",
    "idx = eigenvalues.argsort()\n",
    "tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "B = np.vstack(tmp_df['Poly'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(tmp_df['Poly'].to_numpy()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenvalues.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vandermode(L):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bu = compute_vandermode(Lu)\n",
    "Bd = compute_vandermode(Ld)\n",
    "B = np.hstack([Bu, Bd[:, 1:]])\n",
    "I_s = cp.Constant(np.ones((s,1)))\n",
    "# I_s = cp.Constant(np.eye(s))\n",
    "B = cp.hstack([Bu.real, Bd[:, 1:].real])\n",
    "pp = cp.kron(I_s.T,B)\n",
    "# pp = cp.kron(I_s,B) \n",
    "h = cp.Constant(np.ones((s*(2*k+1), 1)))\n",
    "pp2 = pp@h\n",
    "pp2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 5)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bound = cp.multiply((c_true[0]-epsilon_true[0]), I_s)\n",
    "bound.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "dill.load_session(path+'\\\\results\\\\separated\\\\ipynb_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Luk, _, _ = compute_Lk_and_lambdak(Lu, k, separated=True)\n",
    "Ldk, _, _ = compute_Lk_and_lambdak(Ld, k, separated=True)\n",
    "Bu = compute_vandermode(Lu)\n",
    "Bd = compute_vandermode(Ld)\n",
    "B = cp.hstack([Bu.real, Bd[:, 1:].real])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Luk@X_train[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 150, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prova = np.array([Luk@X_train[(i*n): ((i+1)*n), :,0] for i in range(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2, 100, 150)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([la.matrix_power(np.full((3, 3), 2), i) for i in range(1, k + 1)])\n",
    "b = np.full((3, 3), 2)\n",
    "c= np.array([a@b for i in range(s)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 72, 12, 72, 12, 72])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[:,:,0,0].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Luk[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.,  2.,  2.],\n",
       "        [ 2.,  2.,  2.],\n",
       "        [ 2.,  2.,  2.]],\n",
       "\n",
       "       [[12., 12., 12.],\n",
       "        [12., 12., 12.],\n",
       "        [12., 12., 12.]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 150)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CLARABEL', 'CVXOPT', 'ECOS', 'ECOS_BB', 'GLPK', 'GLPK_MI', 'MOSEK', 'OSQP', 'SCIPY', 'SCS', 'SDPA']\n"
     ]
    }
   ],
   "source": [
    "print(cp.installed_solvers())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def compute_vandermode(L):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            L = Lu + Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermode(L)\n",
    "            B = cp.Constant(B.real)\n",
    "\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            L = Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermode(L)\n",
    "            B = cp.Constant(B.real)\n",
    "        elif dictionary_type==\"separated\":\n",
    "\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "            Bu = compute_vandermode(Lu)\n",
    "            Bd = compute_vandermode(Ld)\n",
    "            B = cp.hstack([Bu.real, Bd[:, 1:].real])\n",
    "\n",
    "        # Init the the sparse representation \n",
    "        Y = cp.Constant(Y_train)\n",
    "        I = np.eye(n)\n",
    "        I_1 = cp.Constant(I)\n",
    "        I_2 = cp.Constant(np.eye(s*(2*K+1)))\n",
    "        I_s = cp.Constant(np.eye(s))\n",
    "        i_s = cp.Constant(np.ones((s,1)))\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # QP Step\n",
    "            X = cp.Constant(X_train)\n",
    "            \n",
    "            # # Define the objective function\n",
    "            # if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "            #     # Init the variables\n",
    "            #     break\n",
    "\n",
    "            # else:\n",
    "\n",
    "            def compute_Pi(i, m, n, N, K, Luk, Ldk, I, X):\n",
    "                '''\n",
    "                i: i-th kernel\n",
    "                m: m-th node (cell)\n",
    "                n: n-th example\n",
    "                N: number of nodes\n",
    "                K: polynomial order\n",
    "                L_u_k: upper laplacian of k-th power\n",
    "                L_d_k: lower laplacian of k-th power\n",
    "                '''\n",
    "\n",
    "                X_i = X[(i*N): ((i+1)*N), :]  # Extract X_i from X\n",
    "                column_n = X_i[:, n]  # n-th column from X_i\n",
    "\n",
    "                Pui = [I[m, :] @ column_n]\n",
    "                Pdi = []\n",
    "                for j in range(0, K):\n",
    "                    Pui.append(Luk[j, m, :] @ column_n)\n",
    "                    Pdi.append(Ldk[j, m, :] @ column_n)\n",
    "                \n",
    "                Pi = Pui + Pdi\n",
    "                Pi = cp.vstack(Pi)  \n",
    "                return Pi\n",
    "\n",
    "            def compute_local_P(m, n, N, s, K, Luk, Ldk, I, X):\n",
    "                '''\n",
    "                s: tot. number of kernels\n",
    "                '''\n",
    "                P = [cp.Constant(np.zeros((2*K+1))) for _ in range(s)]\n",
    "                for i in range(0,s):\n",
    "                    Pi = compute_Pi(i, m, n, N, K, Luk, Ldk, I, X)\n",
    "                    P[i] = Pi\n",
    "\n",
    "                P = cp.vstack(P[i] for i in range(s))\n",
    "\n",
    "                return P\n",
    "            \n",
    "\n",
    "            # Init variable \n",
    "            h = cp.Variable((s*(2*K+1), 1))\n",
    "            term1 = 0\n",
    "            term2 = 0\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(m_train):\n",
    "                    Pij = compute_local_P(i, j, n, s, K, Luk, Ldk, I_1, X)\n",
    "                    term1 += Y[i,j]*(Pij.T@h)\n",
    "                    term2 += Pij@Pij.T\n",
    "\n",
    "            term1 = cp.multiply(2, term1)\n",
    "            term2 = h.T@(term2 + cp.multiply(lambda_, I_2))@h\n",
    "            obj = cp.Minimize(-term1 + term2)\n",
    "\n",
    "            # Define the constraints\n",
    "            cons1 = cp.kron(I_s,B)@h\n",
    "            cons2 = cp.kron(i_s.T,B)@h\n",
    "            constraints = [cons1 >= 0] + [cons1 <= c] + \\\n",
    "                            [cons2 >= (c-epsilon)] + \\\n",
    "                            [cons2 <= (c+epsilon)]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.CLARABEL, verbose=True)\n",
    "            \n",
    "            # Update the dictionary\n",
    "            h = h.value\n",
    "            D_coll = []\n",
    "            hI = h[:(s*K)]\n",
    "            hS = h[(s*K):(2*s*K)]\n",
    "            hH = h[(2*s*K):]\n",
    "            for i in range(0,s):\n",
    "                tmp =  np.zeros((n, n))\n",
    "                for j in range(0,K):\n",
    "                    tmp += ((cp.Constant(Luk[j, :, :])*hS[(i*s+j)]) + (cp.Constant(Ldk[j, :, :])*hI[(i*s+j)]))\n",
    "                tmp += (I*hH[i])\n",
    "                D_coll.append(tmp)\n",
    "\n",
    "            D = np.hstack(tuple(D_coll))\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalize\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI.value, hS.value, hH.value])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100, 150)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Luk@X_train[(1*n): ((1+1)*n), :,0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aux_matrix(Lu,Ld,K,N):\n",
    "\n",
    "    LL = [np.eye(N)]\n",
    "    LL_tmp = []\n",
    "    for i in range(1, K + 1):\n",
    "        LL.append(la.matrix_power(Lu, i))\n",
    "        LL_tmp.append(la.matrix_power(Ld, i))\n",
    "\n",
    "\n",
    "    LL= LL + LL_tmp\n",
    "    LL = np.array(LL)\n",
    "    I = np.eye(n)\n",
    "    P = np.array([LL@X_train[(i*n): ((i+1)*n), :,0] for i in range(s)])\n",
    "\n",
    "    return P\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100, 100)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15,)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[:,:,0,0].flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.data_gen import *\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def compute_vandermode(L):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B\n",
    "\n",
    "def topological_dictionary_learn(Y_train: np.ndarray,\n",
    "                                 Y_test: np.ndarray, \n",
    "                                 K: int, \n",
    "                                 n: int, \n",
    "                                 s: int,\n",
    "                                 D0: np.ndarray, \n",
    "                                 X0: np.ndarray, \n",
    "                                 Lu: np.ndarray, \n",
    "                                 Ld: np.ndarray,\n",
    "                                 dictionary_type: str, \n",
    "                                 c: float, \n",
    "                                 epsilon: float, \n",
    "                                 K0: int,\n",
    "                                 lambda_: float = 1e-3, \n",
    "                                 max_iter: int = 10, \n",
    "                                 patience: int = 10,\n",
    "                                 tol: float = 1e-7, \n",
    "                                 verbose: int = 0) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "    The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "    for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "    the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "    Args:\n",
    "        Y_train (np.ndarray): Training data.\n",
    "        Y_test (np.ndarray): Testing data.\n",
    "        K (int): Max order of the polynomial for the single sub-dictionary.\n",
    "        n (int): Number of data points (number of nodes in the data graph).\n",
    "        s (int): Number of kernels (sub-dictionaries).\n",
    "        D0 (np.ndarray): Initial dictionary.\n",
    "        X0 (np.ndarray): Initial sparse representation.\n",
    "        Lu (np.ndarray): Upper Laplacian matrix\n",
    "        Ld (np.ndarray): Lower Laplacian matrix\n",
    "        dictionary_type (str): Type of dictionary.\n",
    "        c (float): Boundary constant from the synthetic data generation process.\n",
    "        epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "        K0 (int): Sparsity of the signal representation.\n",
    "        lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "        max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "        patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "        tol (float, optional): Tolerance value. Defaults to 1e-7.\n",
    "        verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "         minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define hyperparameters\n",
    "    min_error_train_norm, min_error_test_norm = 1e20, 1e20\n",
    "    m_test, m_train = Y_test.shape[1], Y_train.shape[1]\n",
    "    iter_, pat_iter = 1, 0\n",
    "\n",
    "    if dictionary_type != \"fourier\":\n",
    "        if dictionary_type==\"joint\":\n",
    "            L = Lu + Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermode(L)\n",
    "            B = cp.Constant(B.real)\n",
    "\n",
    "        elif dictionary_type==\"edge_laplacian\":\n",
    "            L = Ld\n",
    "            Lk, _, _ = compute_Lk_and_lambdak(L, K)\n",
    "            B = compute_vandermode(L)\n",
    "            B = cp.Constant(B.real)\n",
    "        elif dictionary_type==\"separated\":\n",
    "\n",
    "            Luk, _, _ = compute_Lk_and_lambdak(Lu, K, separated=True)\n",
    "            Ldk, _, _ = compute_Lk_and_lambdak(Ld, K, separated=True)\n",
    "            Bu = compute_vandermode(Lu)\n",
    "            Bd = compute_vandermode(Ld)\n",
    "            B = cp.hstack([Bu.real, Bd[:, 1:].real])\n",
    "\n",
    "        # Init the the sparse representation \n",
    "        Y = cp.Constant(Y_train)\n",
    "        I = np.eye(n)\n",
    "        I_2 = cp.Constant(np.eye(s*(2*K+1)))\n",
    "        I_s = cp.Constant(np.eye(s))\n",
    "        i_s = cp.Constant(np.ones((s,1)))\n",
    "        X_train = X0\n",
    "        \n",
    "        while pat_iter < patience and iter_ <= max_iter:\n",
    "            \n",
    "            # QP Step\n",
    "            \n",
    "            # # Define the objective function\n",
    "            # if dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "            #     # Init the variables\n",
    "            #     break\n",
    "\n",
    "            # else:\n",
    "\n",
    "\n",
    "            def aux_matrix(Lu,Ld,X_train,K,N):\n",
    "\n",
    "                LL = [np.eye(N)]\n",
    "                LL_tmp = []\n",
    "                for i in range(1, K + 1):\n",
    "                    LL.append(la.matrix_power(Lu, i))\n",
    "                    LL_tmp.append(la.matrix_power(Ld, i))\n",
    "\n",
    "\n",
    "                LL= LL + LL_tmp\n",
    "                LL = np.array(LL)\n",
    "                P = np.array([LL@X_train[(i*n): ((i+1)*n), :] for i in range(s)])\n",
    "\n",
    "                return P\n",
    "            \n",
    "            P = aux_matrix(Lu,Ld,X_train,K,n)\n",
    "            \n",
    "\n",
    "            # Init variable \n",
    "            h = cp.Variable((s*(2*K+1), 1))\n",
    "            term1 = cp.Constant(0)\n",
    "            term2 = cp.Constant(np.zeros((s*(2*K+1), s*(2*K+1))))\n",
    "\n",
    "            for i in range(n):\n",
    "                for j in range(m_train):\n",
    "                    Pij = cp.Constant(P[:,:,i,j].flatten())\n",
    "                    term1 += Y[i,j]*(Pij.T@h)\n",
    "                    term2 = term2 + Pij@Pij.T\n",
    "\n",
    "            term2 = term2 + cp.multiply(lambda_, I_2)\n",
    "            term2 = h.T@(term2)@h\n",
    "            term1 = cp.multiply(2, term1)\n",
    "            \n",
    "            obj = cp.Minimize(term1)\n",
    "\n",
    "            # Define the constraints\n",
    "            cons1 = cp.kron(I_s,B)@h\n",
    "            cons2 = cp.kron(i_s.T,B)@h\n",
    "            bound1 = cp.Constant(np.ones((n*s,1)))\n",
    "            bound2 = cp.Constant(np.ones((n,1)))\n",
    "            constraints = [cons1 >= cp.multiply(0, bound1)] + \\\n",
    "                            [cons1 <= cp.multiply(c, bound1)] + \\\n",
    "                            [cons2 >= cp.multiply((c-epsilon), bound2)] + \\\n",
    "                            [cons2 <= cp.multiply((c+epsilon), bound2)]\n",
    "\n",
    "            prob = cp.Problem(obj, constraints)\n",
    "            prob.solve(solver=cp.MOSEK, verbose=True)\n",
    "            \n",
    "            # Update the dictionary\n",
    "            h = h.value\n",
    "            D_coll = []\n",
    "            hI = h[:(s*K)]\n",
    "            hS = h[(s*K):(2*s*K)]\n",
    "            hH = h[(2*s*K):]\n",
    "            for i in range(0,s):\n",
    "                tmp =  np.zeros((n, n))\n",
    "                for j in range(0,K):\n",
    "                    tmp += ((Luk[j, :, :])*hS[(i*s+j)]) + ((Ldk[j, :, :])*hI[(i*s+j)])\n",
    "                tmp += (I*hH[i])\n",
    "                D_coll.append(tmp)\n",
    "\n",
    "            D = np.hstack(tuple(D_coll))\n",
    "\n",
    "            # OMP Step\n",
    "            dd = la.norm(D, axis=0)\n",
    "            W = np.diag(1. / dd)\n",
    "            Domp = D @ W\n",
    "            X_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_train)\n",
    "            X_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_test)\n",
    "            # Normalize\n",
    "            X_train = W @ X_train\n",
    "            X_test = W @ X_test\n",
    "\n",
    "            # Error Updating\n",
    "            error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D @ X_train), axis=0)**2 /\n",
    "                                    la.norm(Y_train, axis=0)**2)\n",
    "            error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D @ X_test), axis=0)**2 /\n",
    "                                    la.norm(Y_test, axis=0)**2)\n",
    "\n",
    "            # Error Storing\n",
    "            if (error_train_norm < min_error_train_norm) and (abs(error_train_norm) > np.finfo(float).eps) and (abs(error_train_norm - min_error_train_norm) > tol):\n",
    "                X_opt_train = X_train\n",
    "                min_error_train_norm = error_train_norm\n",
    "\n",
    "            if (error_test_norm < min_error_test_norm) and (abs(error_test_norm) > np.finfo(float).eps) and (abs(error_test_norm - min_error_test_norm) > tol):\n",
    "                h_opt = h.value if dictionary_type in [\"joint\", \"edge_laplacian\"] else np.hstack([hI.value, hS.value, hH.value])\n",
    "                D_opt = D\n",
    "                X_opt_test = X_test\n",
    "                min_error_test_norm = error_test_norm\n",
    "                pat_iter = 0\n",
    "                if verbose == 1:\n",
    "                    print(\"New Best Test Error:\", min_error_test_norm)\n",
    "            else:\n",
    "                pat_iter += 1\n",
    "\n",
    "            iter_ += 1\n",
    "    \n",
    "    else:\n",
    "        # Fourier Dictionary Benchmark\n",
    "        L = Lu + Ld\n",
    "        _, D_opt = sla.eigh(L)\n",
    "        dd = la.norm(D_opt, axis=0)\n",
    "        W = np.diag(1./dd)  \n",
    "        D_opt = D_opt / la.norm(D_opt)\n",
    "        Domp = D_opt@W\n",
    "        X_opt_train = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_train)\n",
    "        X_opt_test = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp.real, col=x), axis=0, arr=Y_test)\n",
    "        X_opt_train = W @ X_opt_train\n",
    "        X_opt_test = W @ X_opt_test\n",
    "        # Error Updating\n",
    "        min_error_train_norm = (1/m_train)* np.sum(la.norm(Y_train - (D_opt @ X_opt_train), axis=0)**2 /\n",
    "                                la.norm(Y_train, axis=0)**2)\n",
    "        min_error_test_norm = (1/m_test)* np.sum(la.norm(Y_test - (D_opt @ X_opt_test), axis=0)**2 /\n",
    "                                la.norm(Y_test, axis=0)**2)\n",
    "        h_opt = 0\n",
    "        \n",
    "    return min_error_train_norm, min_error_test_norm, h_opt, X_opt_test, X_opt_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================\n",
      "                                     CVXPY                                     \n",
      "                                     v1.4.1                                    \n",
      "===============================================================================\n",
      "(CVXPY) Mar 16 06:51:06 PM: Your problem has 15 variables, 4 constraints, and 0 parameters.\n",
      "(CVXPY) Mar 16 06:51:06 PM: It is compliant with the following grammars: DCP, DQCP\n",
      "(CVXPY) Mar 16 06:51:06 PM: (If you need to solve this problem multiple times, but with different data, consider using parameters.)\n",
      "(CVXPY) Mar 16 06:51:06 PM: CVXPY will first compile your problem; then, it will invoke a numerical solver to obtain a solution.\n",
      "(CVXPY) Mar 16 06:51:06 PM: Your problem is compiled with the CPP canonicalization backend.\n",
      "-------------------------------------------------------------------------------\n",
      "                                  Compilation                                  \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Mar 16 06:51:07 PM: Compiling problem (target solver=MOSEK).\n",
      "(CVXPY) Mar 16 06:51:07 PM: Reduction chain: Dcp2Cone -> CvxAttr2Constr -> ConeMatrixStuffing -> MOSEK\n",
      "(CVXPY) Mar 16 06:51:07 PM: Applying reduction Dcp2Cone\n",
      "(CVXPY) Mar 16 06:51:08 PM: Applying reduction CvxAttr2Constr\n",
      "(CVXPY) Mar 16 06:51:08 PM: Applying reduction ConeMatrixStuffing\n",
      "(CVXPY) Mar 16 06:51:18 PM: Applying reduction MOSEK\n",
      "(CVXPY) Mar 16 06:51:18 PM: Finished problem compilation (took 1.164e+01 seconds).\n",
      "-------------------------------------------------------------------------------\n",
      "                                Numerical solver                               \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Mar 16 06:51:18 PM: Invoking solver MOSEK  to obtain a solution.\n",
      "\n",
      "\n",
      "(CVXPY) Mar 16 06:51:18 PM: Problem\n",
      "(CVXPY) Mar 16 06:51:18 PM:   Name                   :                 \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Objective sense        : maximize        \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Type                   : LO (linear optimization problem)\n",
      "(CVXPY) Mar 16 06:51:18 PM:   Constraints            : 15              \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Affine conic cons.     : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Disjunctive cons.      : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Cones                  : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Scalar variables       : 800             \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Matrix variables       : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Integer variables      : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM: \n",
      "(CVXPY) Mar 16 06:51:18 PM: Optimizer started.\n",
      "(CVXPY) Mar 16 06:51:18 PM: Presolve started.\n",
      "(CVXPY) Mar 16 06:51:18 PM: Eliminator - tries                  : 0                 time                   : 0.00            \n",
      "(CVXPY) Mar 16 06:51:18 PM: Lin. dep.  - tries                  : 0                 time                   : 0.00            \n",
      "(CVXPY) Mar 16 06:51:18 PM: Lin. dep.  - primal attempts        : 0                 successes              : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM: Lin. dep.  - dual attempts          : 0                 successes              : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM: Lin. dep.  - primal deps.           : 0                 dual deps.             : 0               \n",
      "(CVXPY) Mar 16 06:51:18 PM: Presolve terminated. Time: 0.00    \n",
      "(CVXPY) Mar 16 06:51:18 PM: Optimizer terminated. Time: 0.03    \n",
      "(CVXPY) Mar 16 06:51:18 PM: \n",
      "(CVXPY) Mar 16 06:51:18 PM: \n",
      "(CVXPY) Mar 16 06:51:18 PM: Interior-point solution summary\n",
      "(CVXPY) Mar 16 06:51:18 PM:   Problem status  : PRIMAL_AND_DUAL_FEASIBLE\n",
      "(CVXPY) Mar 16 06:51:18 PM:   Solution status : OPTIMAL\n",
      "(CVXPY) Mar 16 06:51:18 PM:   Primal.  obj: 0.0000000000e+00    nrm: 0e+00    Viol.  con: 0e+00    var: 0e+00  \n",
      "(CVXPY) Mar 16 06:51:18 PM:   Dual.    obj: 0.0000000000e+00    nrm: 0e+00    Viol.  con: 0e+00    var: 0e+00  \n",
      "-------------------------------------------------------------------------------\n",
      "                                    Summary                                    \n",
      "-------------------------------------------------------------------------------\n",
      "(CVXPY) Mar 16 06:51:18 PM: Problem status: optimal\n",
      "(CVXPY) Mar 16 06:51:18 PM: Optimal value: 0.000e+00\n",
      "(CVXPY) Mar 16 06:51:18 PM: Compilation took 1.164e+01 seconds\n",
      "(CVXPY) Mar 16 06:51:18 PM: Solver (including time spent in interface) took 3.679e-02 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:29, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 37\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization Failed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m dict_types\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     dict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m0\u001b[39m][sim,k0_index], dict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m1\u001b[39m][sim,k0_index], _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtopological_dictionary_learn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43msim\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                                                                                                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_sim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sparsity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Testing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00md[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... Done! Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdict_errors[d[\u001b[38;5;241m0\u001b[39m]][\u001b[38;5;241m1\u001b[39m][sim,k0_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[122], line 173\u001b[0m, in \u001b[0;36mtopological_dictionary_learn\u001b[1;34m(Y_train, Y_test, K, n, s, D0, X0, Lu, Ld, dictionary_type, c, epsilon, K0, lambda_, max_iter, patience, tol, verbose)\u001b[0m\n\u001b[0;32m    171\u001b[0m tmp \u001b[38;5;241m=\u001b[39m  np\u001b[38;5;241m.\u001b[39mzeros((n, n))\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,K):\n\u001b[1;32m--> 173\u001b[0m     tmp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m ((Luk[j, :, :])\u001b[38;5;241m*\u001b[39m\u001b[43mhS\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m ((Ldk[j, :, :])\u001b[38;5;241m*\u001b[39mhI[(i\u001b[38;5;241m*\u001b[39ms\u001b[38;5;241m+\u001b[39mj)])\n\u001b[0;32m    174\u001b[0m tmp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (I\u001b[38;5;241m*\u001b[39mhH[i])\n\u001b[0;32m    175\u001b[0m D_coll\u001b[38;5;241m.\u001b[39mappend(tmp)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "min_error_fou_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_sep_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_train = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_joint_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "dict_errors = {\n",
    "    \"sep\": (min_error_sep_train,min_error_sep_test)\n",
    "    }\n",
    "\n",
    "\n",
    "dict_types = {\n",
    "    \"sep\": (\"Separated Hodge Laplacian\",\"separated\")\n",
    "    }\n",
    "\n",
    "for sim in range(n_sim):\n",
    "    c = c_true[sim]  \n",
    "    epsilon = epsilon_true[sim] \n",
    "    for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "        discard = 1\n",
    "        while discard == 1:\n",
    "            \n",
    "            try:\n",
    "                D0, X0, discard = initialize_dic(Lu, Ld, s, k, Y_train[:, :, sim], k0, dictionary_type, c, epsilon, \"only_X\")\n",
    "            except:\n",
    "                print(\"Initialization Failed!\")\n",
    "\n",
    "        for d in dict_types.items():\n",
    "            # try:\n",
    "            dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index], _, _, _ = topological_dictionary_learn(Y_train[:,:,sim], Y_test[:,:,sim],\n",
    "                                                                                                                        k, n, s, D0, X0, Lu, Ld, d[1][1],\n",
    "                                                                                                                        c, epsilon, k0, lambda_, max_iter,\n",
    "                                                                                                                        patience, tol)\n",
    "            if verbose:\n",
    "                print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Done! Test Error: {dict_errors[d[0]][1][sim,k0_index]}\")\n",
    "            # except:\n",
    "            #     print(f'Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {d[1][0]}... Diverged!')\n",
    "            #     try:\n",
    "            #         dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index] = (dict_errors[d[0]][0][sim-1,k0_index]\n",
    "            #                                                                               , dict_errors[d[0]][1][sim-1,k0_index])\n",
    "            #     except:\n",
    "            #         dict_errors[d[0]][0][sim,k0_index], dict_errors[d[0]][1][sim,k0_index] = (dict_errors[d[0]][0][sim+1,k0_index]\n",
    "            #                                                                               , dict_errors[d[0]][1][sim+1,k0_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X0' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX0\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X0' is not defined"
     ]
    }
   ],
   "source": [
    "X0.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
