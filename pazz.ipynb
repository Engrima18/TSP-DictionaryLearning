{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsplearn import *\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "prob_T=0.8\n",
    "\n",
    "# Load the graph\n",
    "G = EnhancedGraph(n=40, p_edges=0.162, p_triangles=prob_T, seed=0)\n",
    "B1 = G.get_b1()\n",
    "B2 = G.get_b2()\n",
    "\n",
    "# Sub-sampling if needed to decrease complexity\n",
    "sub_size = 100\n",
    "B1 = B1[:, :sub_size]\n",
    "B2 = B2[:sub_size, :]\n",
    "B2 = B2[:,np.sum(np.abs(B2), 0) == 3]\n",
    "nu = B2.shape[1] # N2\n",
    "nd = B1.shape[1] # N1\n",
    "T = int(np.ceil(nu*(1-prob_T)))\n",
    "\n",
    "# Laplacians\n",
    "Lu, Ld, L = G.get_laplacians(sub_size=100)\n",
    "Lu_full = G.get_laplacians(sub_size=100, full=True)\n",
    "B2_true = B2@G.mask\n",
    "M =  L.shape[0]\n",
    "\n",
    "\n",
    "# Problem and Dictionary Dimensionalities\n",
    "dictionary_type=\"separated\"\n",
    "m_train = 150 # Number of Train Signals\n",
    "m_test = 80 # Number of Test Signal\n",
    "P = 3 # Number of Kernels (Sub-dictionaries)\n",
    "J = 2 # Polynomial order\n",
    "sparsity = .1 # Sparsity percentage\n",
    "K0_max = 20 #floor(M*sparsity) # Sparsity\n",
    "sparsity_mode = \"max\"\n",
    "n_search = 3000\n",
    "n_sim = 10\n",
    "\n",
    "# Data-Independent Problem Hyperparameters\n",
    "K0_coll = np.arange(5, 26, 4) \n",
    "max_iter = 100 \n",
    "patience = 5 \n",
    "tol = 1e-7 # tolerance for Patience\n",
    "lambda_ = 1e-7 # l2 multiplier\n",
    "verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_params={'dictionary_type':dictionary_type,\n",
    "        'm_train':m_train,\n",
    "        'm_test':m_test,\n",
    "        'P':P,\n",
    "        'M':M,\n",
    "        'J':J,\n",
    "        'sparsity':sparsity,\n",
    "        'K0_max':K0_max,\n",
    "        'sparsity_mode':sparsity_mode,\n",
    "        'n_search':n_search,\n",
    "        'n_sim':n_sim,\n",
    "        'prob_T':prob_T}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = generate_data(Lu, Ld, **gen_params)\n",
    "\n",
    "D_true = load_data['D_true']\n",
    "Y_train = load_data['Y_train']\n",
    "Y_test = load_data['Y_test']\n",
    "X_train = load_data['X_train']\n",
    "X_test = load_data['X_test']\n",
    "epsilon_true =  load_data['epsilon_true']\n",
    "c_true = load_data['c_true']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 8\n",
    "c = c_true[s]  \n",
    "epsilon = epsilon_true[s] \n",
    "k0 = K0_coll[4]\n",
    "\n",
    "topo_params = {\"K0\":K0_coll[4],\n",
    "               \"J\":J,\n",
    "               \"P\":P,\n",
    "               \"true_prob_T\":prob_T,\n",
    "               \"sub_size\":100,\n",
    "               \"dictionary_type\":\"wavelet\",\n",
    "               \"c\":c_true[s],\n",
    "               \"epsilon\":epsilon_true[s],\n",
    "               \"seed\":0,\n",
    "               \"n\":40,\n",
    "               \"p_edges\":0.162\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K0_coll[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonic Component is Present!\n",
      "Saving Directory not Valid!\n"
     ]
    }
   ],
   "source": [
    "from lib import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "\n",
    "option = \"One-shot-diffusion\"#\"One-shot-diffusion\"\n",
    "F_sol,F_irr = get_frequency_mask(B1,B2) # Get frequency bands\n",
    "diff_order_sol= 1\n",
    "diff_order_irr = 1\n",
    "step_prog = 1\n",
    "source_sol = np.ones((nd,))\n",
    "source_irr = np.ones((nd,))\n",
    "S_neigh, complete_coverage = cluster_on_neigh(B1,B2,diff_order_sol,diff_order_irr,source_sol,source_irr,option,step_prog)\n",
    "R = [F_sol, F_irr]\n",
    "S = S_neigh\n",
    "top_K_coll = [2,4, None]\n",
    "spars_level = list(range(10,80,10))\n",
    "\n",
    "DD = SimplicianSlepians(B1, B2, S, R, top_K = top_K_coll[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from typing import Tuple, List, Union, Dict\n",
    "import pickle\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "import scipy.linalg as sla\n",
    "import numpy as np\n",
    "import numpy.linalg as la\n",
    "import cvxpy as cp\n",
    "from tsplearn.tsp_generation import *\n",
    "from typing import Tuple, List, Union, Dict\n",
    "import pickle\n",
    "from functools import wraps\n",
    "from einops import rearrange\n",
    "\n",
    "\n",
    "def _indicator_matrix(row):\n",
    "    tmp = row.sigma.copy()\n",
    "    tmp[row.idx] = 0\n",
    "    return np.diag(tmp)\n",
    "\n",
    "def _indicator_matrix_rev(row):\n",
    "    tmp = row.sigma.copy()\n",
    "    tmp[row.idx] = 1\n",
    "    return np.diag(tmp)\n",
    "\n",
    "def _compute_Luj(row, b2, J):\n",
    "    Lu = b2 @ row.sigma @ b2.T\n",
    "    Luj = np.array([la.matrix_power(Lu, i) for i in range(1, J + 1)])\n",
    "    return Luj\n",
    "\n",
    "def _split_coeffs(h ,s ,k, sep=False):\n",
    "    h_tmp = h.value.flatten()\n",
    "    # hH = h_tmp[:s,].reshape((s,1))\n",
    "    # hS = h_tmp[s:s*(k+1),].reshape((s,k))\n",
    "    # hI = h_tmp[s*(k+1):,].reshape((s,k))\n",
    "    if sep:\n",
    "        hH = h_tmp[np.arange(0, (s*(2*k+1)), (2*k+1))].reshape((s,1))\n",
    "        hS = h_tmp[np.hstack([[i,i+1] for i in range(1, (s*(2*k+1)), (2*k+1))])].reshape((s,k))\n",
    "        hI = h_tmp[np.hstack([[i,i+1] for i in range((k+1), (s*(2*k+1)), (2*k+1))])].reshape((s,k))\n",
    "        return [hH, hS, hI]\n",
    "    h = h_tmp[:s*k]\n",
    "    hi = h_tmp[s*k:]\n",
    "    return [h, hi]\n",
    "    \n",
    "def sparse_transform(D, K0, Y_te, Y_tr=None):\n",
    "\n",
    "    dd = la.norm(D, axis=0)\n",
    "    W = np.diag(1. / dd)\n",
    "    Domp = D @ W\n",
    "    X_te = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_te)\n",
    "    # Normalization\n",
    "    X_te = W @ X_te\n",
    "\n",
    "    if np.all(Y_tr == None):\n",
    "\n",
    "        return X_te\n",
    "    \n",
    "    # Same for the training set\n",
    "    X_tr = np.apply_along_axis(lambda x: get_omp_coeff(K0, Domp=Domp, col=x), axis=0, arr=Y_tr)\n",
    "    X_tr = W @ X_tr\n",
    "    \n",
    "    return X_te, X_tr\n",
    "\n",
    "\n",
    "def compute_vandermonde(L, k):\n",
    "    \n",
    "    def polynomial_exp(x, k):\n",
    "        x = x** np.arange(0, k + 1)\n",
    "        return x\n",
    "\n",
    "    eigenvalues, _ = sla.eig(L)\n",
    "    idx = eigenvalues.argsort()\n",
    "    tmp_df = pd.DataFrame({'Eigs': eigenvalues[idx]})\n",
    "    tmp_df['Poly'] = tmp_df['Eigs'].apply(lambda x:  polynomial_exp(x,k))\n",
    "    B = np.vstack(tmp_df['Poly'].to_numpy())\n",
    "\n",
    "    return B\n",
    "\n",
    "\n",
    "def nmse(D, X, Y, m):\n",
    "    return (1/m)* np.sum(la.norm(Y - (D @ X), axis=0)**2 /la.norm(Y, axis=0)**2)\n",
    "\n",
    "\n",
    "class TspSolver:\n",
    "\n",
    "    def __init__(self, X_train, X_test, Y_train, Y_test, *args, **kwargs):\n",
    "\n",
    "        params = {\n",
    "                'P': None,      # Number of Kernels (Sub-dictionaries)\n",
    "                'J': None,      # Polynomial order\n",
    "                'K0': None,     # Sparsity level\n",
    "                'dictionary_type': None,\n",
    "                'c': None,      # spectral control parameter \n",
    "                'epsilon': None,# spectral control parameter\n",
    "                'n': 10,        # number of nodes\n",
    "                'sub_size': None,   # Number of sub-sampled nodes\n",
    "                'prob_T': 1.,   # Ratio of colored triangles\n",
    "                'true_prob_T': 1.,   # True ratio of colored triangles\n",
    "                'p_edges': 1.,  # Probability of edge existence\n",
    "                'seed': None,       ####\n",
    "                'option' : \"One-shot-diffusion\",        ####\n",
    "                'diff_order_sol' : 1,       ####\n",
    "                'diff_order_irr' : 1,       ####\n",
    "                'step_prog' : 1,        ####\n",
    "                'top_k_slepians' : 2        ####\n",
    "                }\n",
    "        \n",
    "        if args:\n",
    "            if len(args) != 1 or not isinstance(args[0], dict):\n",
    "                raise ValueError(\"When using positional arguments, must provide a single dictionary\")\n",
    "            params.update(args[0])\n",
    "\n",
    "        params.update(kwargs)\n",
    "\n",
    "        # Data\n",
    "        self.X_train: np.ndarray = X_train\n",
    "        self.X_test: np.ndarray = X_test\n",
    "        self.Y_train: np.ndarray = Y_train\n",
    "        self.Y_test: np.ndarray = Y_test\n",
    "        self.m_train: int = Y_train.shape[1]\n",
    "        self.m_test: int = Y_test.shape[1]\n",
    "\n",
    "        # Topology and geometry behind data\n",
    "        self.G = EnhancedGraph(n=params['n'],\n",
    "                               p_edges=params['p_edges'], \n",
    "                               p_triangles=params['prob_T'], \n",
    "                               seed=params['seed']) \n",
    "        # Incidence matrices\n",
    "        self.B1: np.ndarray = self.G.get_b1()\n",
    "        self.B2: np.ndarray = self.G.get_b2()\n",
    "\n",
    "        # Sub-sampling if needed to decrease complexity\n",
    "        if params['sub_size'] != None:\n",
    "            self.B1 = self.B1[:, :params['sub_size']]\n",
    "            self.B2 = self.B2[:params['sub_size'], :]\n",
    "            self.B2 = self.B2[:,np.sum(np.abs(self.B2), 0) == 3]\n",
    "        \n",
    "        # Topology dimensions and hyperparameters\n",
    "        self.nu: int = self.B2.shape[1]\n",
    "        self.nd: int = self.B1.shape[1]\n",
    "        self.true_prob_T = params['true_prob_T']\n",
    "        self.T: int = int(np.ceil(self.nu*(1-params['prob_T'])))\n",
    "\n",
    "        # Laplacians according to the Hodge Theory for cell complexes\n",
    "        Lu, Ld, L = self.G.get_laplacians(sub_size=params['sub_size'])\n",
    "        self.Lu: np.ndarray = Lu                             # Ground-truth upper Laplacian\n",
    "        self.Ld: np.ndarray = Ld                             # Ground-truth lower Laplacian\n",
    "        self.L: np.ndarray = L                               # Ground-truth sum Laplacian\n",
    "        self.Lu_full: np.ndarray = G.get_laplacians(sub_size=params['sub_size'], \n",
    "                                                    full=True)\n",
    "        self.M =  L.shape[0]\n",
    "        \n",
    "\n",
    "        # Dictionary hyperparameters\n",
    "        self.P = params['P']                                 # Number of sub-dicts\n",
    "        self.J = params['J']                                 # Polynomial order for the Hodge Laplacian\n",
    "        self.c = params['c']                                 # Hyperparameter for stability in frequency domain\n",
    "        self.epsilon = params['epsilon']                     # Hyperparameter for stability in frequency domain\n",
    "        self.K0 = params['K0']                               # Assumed sparsity level\n",
    "        self.dictionary_type = params['dictionary_type']\n",
    "        # Init optimal values for sparse representations and overcomplete dictionary\n",
    "        self.D_opt: np.ndarray = np.zeros((self.M, self.M*self.P))\n",
    "        self.X_opt_train: np.ndarray = np.zeros(self.X_train.shape)\n",
    "        self.X_opt_test: np.ndarray = np.zeros(self.X_test.shape)\n",
    "        # Init the learning errors and error curve (history)\n",
    "        self.min_error_train = 1e20\n",
    "        self.min_error_test = 1e20\n",
    "        self.history: List[np.ndarray] = []\n",
    "\n",
    "        ############################################################################################################\n",
    "        ##                                                                                                        ##\n",
    "        ##               This section is only for learnable (data-driven) dictionaries                            ##\n",
    "        ##                                                                                                        ##\n",
    "        ############################################################################################################\n",
    "\n",
    "        # Init the dictionary parameters according to the specific parameterization setup\n",
    "        if self.dictionary_type==\"separated\":\n",
    "            hs = np.zeros((self.P,self.J))\n",
    "            hi = np.zeros((self.P,self.J))\n",
    "            hh = np.zeros((self.P,1))\n",
    "            self.h_opt: List[np.ndarray] = [hh,hs,hi]\n",
    "        else:\n",
    "            h = np.zeros((self.P, self.J))\n",
    "            hI = np.zeros((self.P, 1))\n",
    "            self.h_opt: np.ndarray = [h, hI]\n",
    "\n",
    "        # Compute the polynomial extension for the Laplacians and the auxiliary \n",
    "        # \"pseudo-vandermonde\" matrix for the constraints in the quadratic form\n",
    "        if self.dictionary_type == \"joint\":\n",
    "            self.Lj, self.lambda_max_j, self.lambda_min_j = compute_Lj_and_lambdaj(self.L, self.J)\n",
    "            self.B = compute_vandermonde(self.L, self.J).real\n",
    "        elif self.dictionary_type == \"edge_laplacian\":\n",
    "            self.Lj, self.lambda_max_j, self.lambda_min_j = compute_Lj_and_lambdaj(self.Ld, self.J)\n",
    "            self.B = compute_vandermonde(self.Ld, self.J).real\n",
    "        elif  self.dictionary_type == 'separated':\n",
    "            self.Luj, self.lambda_max_u_j, self.lambda_min_u_j = compute_Lj_and_lambdaj(self.Lu, self.J, separated=True)\n",
    "            self.Ldj, self.lambda_max_d_j, self.lambda_min_d_j = compute_Lj_and_lambdaj(self.Ld, self.J, separated=True)\n",
    "            self.Bu = compute_vandermonde(self.Lu, self.J).real\n",
    "            self.Bd = compute_vandermonde(self.Ld, self.J)[:, 1:].real\n",
    "            self.B = np.hstack([self.Bu, self.Bd])\n",
    "\n",
    "        # Auxiliary matrix to define quadratic form dor the dictionary learning step\n",
    "        self.P_aux: np.ndarray = None\n",
    "        # Flag variable: the dictionary is learnable or analytic\n",
    "        self.dict_is_learnable = self.dictionary_type in [\"separated\", \"joint\", \"edge_laplacian\"]\n",
    "\n",
    "        # Auxiliary tools for the Slepians-based and Wavelet-based dictionary setup\n",
    "        if self.dictionary_type == 'slepians':\n",
    "            self.option = params['option']\n",
    "            self.diff_order_sol = params['diff_order_sol']\n",
    "            self.step_prog = params['step_prog']\n",
    "            self.diff_order_irr = params['diff_order_irr']\n",
    "            self.F_sol,self.F_irr = get_frequency_mask(self.B1,self.B2) # Get frequency bands\n",
    "            self.source_sol = np.ones((self.nd,))\n",
    "            self.source_irr = np.ones((self.nd,))\n",
    "            self.S_neigh, self.complete_coverage = cluster_on_neigh(B1,B2,diff_order_sol,diff_order_irr,source_sol,source_irr,option,step_prog)\n",
    "            self.R = [self.F_sol, self.F_irr]\n",
    "            self.S = self.S_neigh\n",
    "            self.top_K_slepians = params['top_k_slepians']\n",
    "            self.spars_level = list(range(10,80,10))\n",
    "        elif self.dictionary_type == 'wavelet':\n",
    "            pass\n",
    "\n",
    "    # def fit(self) -> Tuple[float, List[np.ndarray], np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    #     min_error_test, _, _, h_opt, X_opt_test, D_opt = self.learn_upper_laplacian()\n",
    "    #     return min_error_test, self.history, params['Lu'], h_opt, X_opt_test, D_opt\n",
    "\n",
    "    def update_Lu(self, Lu_new):\n",
    "        self.Lu = Lu_new\n",
    "        self.Luj, self.lambda_max_u_j, self.lambda_min_u_j = compute_Lj_and_lambdaj(self.Lu, \n",
    "                                                                                    self.J, \n",
    "                                                                                    separated=True)\n",
    "        self.Bu = compute_vandermonde(self.Lu, self.J).real\n",
    "        self.B = np.hstack([self.Bu, self.Bd])\n",
    "\n",
    "    @staticmethod\n",
    "    def _multiplier_search(*arrays, P, c, epsilon):\n",
    "        is_okay = 0\n",
    "        mult = 100\n",
    "        tries = 0\n",
    "        while is_okay==0:\n",
    "            is_okay = 1\n",
    "            h, c_try, _, tmp_sum_min, tmp_sum_max = generate_coeffs(arrays, P=P, mult=mult)\n",
    "            if c_try <= c:\n",
    "                is_okay *= 1\n",
    "            if tmp_sum_min > c-epsilon:\n",
    "                is_okay *= 1\n",
    "                incr_mult = 0\n",
    "            else:\n",
    "                is_okay = is_okay*0\n",
    "                incr_mult = 1\n",
    "            if tmp_sum_max < c+epsilon:\n",
    "                is_okay *= 1\n",
    "                decr_mult = 0\n",
    "            else:\n",
    "                is_okay *= 0\n",
    "                decr_mult = 1\n",
    "            if is_okay == 0:\n",
    "                tries += 1\n",
    "            if tries >3:\n",
    "                discard = 1\n",
    "                break\n",
    "            if incr_mult == 1:\n",
    "                mult *= 2\n",
    "            if decr_mult == 1:\n",
    "                mult /= 2\n",
    "        return h, discard\n",
    "\n",
    "    def init_dict(self,\n",
    "                  h_prior: np.ndarray = None, \n",
    "                  mode: str = \"only_X\") -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Initialize the dictionary and the signal sparse representation for the alternating\n",
    "        optimization algorithm.\n",
    "\n",
    "        Args:\n",
    "            Lu (np.ndarray): Upper Laplacian matrix\n",
    "            Ld (np.ndarray): Lower Laplacian matrix\n",
    "            P (int): Number of kernels (sub-dictionaries).\n",
    "            J (int): Max order of the polynomial for the single sub-dictionary.\n",
    "            Y_train (np.ndarray): Training data.\n",
    "            K0 (int): Sparsity of the signal representation.\n",
    "            dictionary_type (str): Type of dictionary.\n",
    "            c (float): Boundary constant from the synthetic data generation process.\n",
    "            epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "            only (str): Type of initialization. Can be one of: \"only_X\", \"all\", \"only_D\".\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, bool]: Initialized dictionary, initialized sparse representation, and discard flag value.\n",
    "        \"\"\"\n",
    "        \n",
    "        # If no prior info on the dictionary\n",
    "        if np.all(h_prior == None):\n",
    "\n",
    "            # Init Dictionary\n",
    "            if (mode in [\"all\",\"only_D\"]):\n",
    "\n",
    "                discard = 1\n",
    "                while discard==1:\n",
    "\n",
    "                    if self.dictionary_type != \"separated\":\n",
    "                        h_prior, discard = self._multiplier_search(self.lambda_max_j, \n",
    "                                                              self.lambda_min_j, \n",
    "                                                              P=self.P, \n",
    "                                                              c=self.c, \n",
    "                                                              epsilon=self.epsilon)\n",
    "                        self.D_opt = generate_dictionary(h_prior, \n",
    "                                                         self.P, \n",
    "                                                         self.Lj)\n",
    "\n",
    "                    else:\n",
    "                        h_prior, discard = self._multiplier_search(self.lambda_max_d_j, \n",
    "                                                              self.lambda_min_d_j, \n",
    "                                                              self.lambda_max_u_j, \n",
    "                                                              self.lambda_min_u_j,\n",
    "                                                              P=self.P, \n",
    "                                                              c=self.c, \n",
    "                                                              epsilon=self.epsilon)\n",
    "                        self.D_opt = generate_dictionary(h_prior, \n",
    "                                                         self.P, \n",
    "                                                         self.Luj, \n",
    "                                                         self.Ldj)\n",
    "\n",
    "            # Init Sparse Representations\n",
    "            if (mode in [\"all\",\"only_X\"]):\n",
    "                \n",
    "                L = self.Ld if self.dictionary_type == \"edge_laplacian\" else self.L\n",
    "                _, Dx = sla.eig(L)\n",
    "                dd = la.norm(Dx, axis=0)\n",
    "                W = np.diag(1./dd)\n",
    "                Dx = Dx / la.norm(Dx)  \n",
    "                Domp = Dx@W\n",
    "                X = np.apply_along_axis(lambda x: get_omp_coeff(self.K0, Domp.real, x), axis=0, arr=self.Y_train)\n",
    "                X = np.tile(X, (self.P,1))\n",
    "                self.X_opt_train = X\n",
    "\n",
    "        # Otherwise use prior info about the dictionary to initialize both the dictionary and the sparse representation\n",
    "        else:\n",
    "            \n",
    "            self.h_opt = h_prior\n",
    "\n",
    "            if self.dictionary_type == \"separated\":\n",
    "                self.D_opt = generate_dictionary(h_prior, \n",
    "                                                 self.P, \n",
    "                                                 self.Luj, \n",
    "                                                 self.Ldj)\n",
    "                self.X_opt_train = sparse_transform(self.D_opt, \n",
    "                                                    self.K0, \n",
    "                                                    self.Y_train)\n",
    "            else: \n",
    "                self.D_opt = generate_dictionary(h_prior, \n",
    "                                                 self.P, \n",
    "                                                 self.Lj)\n",
    "                self.X_opt_train = sparse_transform(self.D_opt, \n",
    "                                                    self.K0, \n",
    "                                                    self.Y_train)             \n",
    "\n",
    "   \n",
    "    def save_results(func):\n",
    "        @wraps(func)\n",
    "        def wrapper(self, *args, **kwargs):\n",
    "\n",
    "            outputs = func(self, *args, **kwargs)\n",
    "            func_name = func.__name__\n",
    "\n",
    "            if func_name == \"topological_dictionary_learn\":\n",
    "\n",
    "                path = os.getcwd()\n",
    "                dir_path = os.path.join(path, \n",
    "                                        'results', \n",
    "                                        'dictionary_learning',\n",
    "                                        f'{self.dictionary_type}')\n",
    "                name = f'learn_D_{self.dictionary_type}'\n",
    "                filename = os.path.join(dir_path, f'{name}.pkl')\n",
    "                save_var = {\"min_error_test\": self.min_error_test,\n",
    "                            \"min_error_train\": self.min_error_train,\n",
    "                            \"history\": outputs[2],\n",
    "                            \"h_opt\": self.h_opt,\n",
    "                            \"X_opt_test\": self.X_opt_test,\n",
    "                            \"X_opt_train\": self.X_opt_train,\n",
    "                            \"D_opt\": self.D_opt}\n",
    "                \n",
    "            elif func_name == \"learn_upper_laplacian\":\n",
    "\n",
    "                path = os.getcwd()\n",
    "                dir_path = os.path.join(path, 'results', 'topology_learning')\n",
    "                name = f'learn_T{int(self.true_prob_T*100)}'\n",
    "                filename = os.path.join(dir_path, f'{name}.pkl')\n",
    "                save_var = {\"min_error_test\": self.min_error_test,\n",
    "                            \"min_error_train\": self.min_error_train,\n",
    "                            \"history\": self.history,\n",
    "                            \"Lu_opt\": self.Lu,\n",
    "                            \"h_opt\": self.h_opt,\n",
    "                            \"X_opt_test\": self.X_opt_test,\n",
    "                            \"X_opt_train\": self.X_opt_train,\n",
    "                            \"D_opt\": self.D_opt}\n",
    "\n",
    "            if not os.path.exists(dir_path):\n",
    "                os.makedirs(dir_path)\n",
    "\n",
    "            try:\n",
    "                with open(filename, 'wb') as file:\n",
    "                    pickle.dump(save_var, file)\n",
    "            except IOError as e:\n",
    "                print(f\"An error occurred while writing the file: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "            return outputs  \n",
    "        return wrapper\n",
    "\n",
    "    def topological_dictionary_learn(self,\n",
    "                                     lambda_: float = 1e-3, \n",
    "                                     max_iter: int = 10, \n",
    "                                     patience: int = 10,\n",
    "                                     tol: float = 1e-7,\n",
    "                                     step_h: float = 1.,\n",
    "                                     step_x: float = 1.,\n",
    "                                     solver: str =\"MOSEK\", \n",
    "                                     verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
    "        \"\"\"\n",
    "        Dictionary learning algorithm implementation for sparse representations of a signal on complex regular cellular.\n",
    "        The algorithm consists of an iterative alternating optimization procedure defined in two steps: the positive semi-definite programming step\n",
    "        for obtaining the coefficients and dictionary based on Hodge theory, and the Orthogonal Matching Pursuit step for constructing \n",
    "        the K0-sparse solution from the dictionary found in the previous step, which best approximates the original signal.\n",
    "        Args:\n",
    "            Y_train (np.ndarray): Training data.\n",
    "            Y_test (np.ndarray): Testing data.\n",
    "            J (int): Max order of the polynomial for the single sub-dictionary.\n",
    "            M (int): Number of data points (number of nodes in the data graph).\n",
    "            P (int): Number of kernels (sub-dictionaries).\n",
    "            D0 (np.ndarray): Initial dictionary.\n",
    "            X0 (np.ndarray): Initial sparse representation.\n",
    "            Lu (np.ndarray): Upper Laplacian matrix\n",
    "            Ld (np.ndarray): Lower Laplacian matrix\n",
    "            dictionary_type (str): Type of dictionary.\n",
    "            c (float): Boundary constant from the synthetic data generation process.\n",
    "            epsilon (float): Boundary constant from the synthetic data generation process.\n",
    "            K0 (int): Sparsity of the signal representation.\n",
    "            lambda_ (float, optional): Regularization parameter. Defaults to 1e-3.\n",
    "            max_iter (int, optional): Maximum number of iterations. Defaults to 10.\n",
    "            patience (int, optional): Patience for early stopping. Defaults to 10.\n",
    "            tol (float, optional): Tolerance value. Defaults to 1e-s.\n",
    "            verbose (int, optional): Verbosity level. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "            minimum training error, minimum testing error, optimal coefficients, optimal testing sparse representation, and optimal training sparse representation.\n",
    "        \"\"\"\n",
    "\n",
    "        # Define hyperparameters\n",
    "        iter_, pat_iter = 1, 0\n",
    "        hist = []\n",
    "\n",
    "        if self.dict_is_learnalbe:\n",
    "\n",
    "            # Init the dictionary and the sparse representation \n",
    "            D_coll = [cp.Constant(self.D_opt[:,(self.M*i):(self.M*(i+1))]) for i in range(self.P)]\n",
    "            Dsum = cp.Constant(np.zeros((self.M, self.M)))\n",
    "            h_opt = self.h_opt\n",
    "            Y = cp.Constant(self.Y_train)\n",
    "            X_tr = self.X_opt_train\n",
    "            X_te = self.X_opt_test\n",
    "            I = cp.Constant(np.eye(self.M))\n",
    "            \n",
    "            while pat_iter < patience and iter_ <= max_iter:\n",
    "                \n",
    "                # SDP Step\n",
    "                X = cp.Constant(X_tr)\n",
    "                if iter_ != 1:\n",
    "                    D_coll = [cp.Constant(D[:,(self.M*i):(self.M*(i+1))]) for i in range(self.P)]\n",
    "                    Dsum = cp.Constant(np.zeros((self.M, self.M)))\n",
    "                \n",
    "                # Define the objective function\n",
    "                if self.dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                    # Init the variables\n",
    "                    h = cp.Variable((self.P, self.J))\n",
    "                    hI = cp.Variable((self.P, 1))\n",
    "                    h.value, hI.value = h_opt\n",
    "                    for i in range(0,self.P):\n",
    "                        tmp =  cp.Constant(np.zeros((self.M, self.M)))\n",
    "                        for j in range(0,self.J):\n",
    "                            tmp += (cp.Constant(self.Lj[j, :, :]) * h[i,j])\n",
    "                        tmp += (I*hI[i])\n",
    "                        D_coll[i] = tmp\n",
    "                        Dsum += tmp\n",
    "                    D = cp.hstack([D_coll[i]for i in range(self.P)])\n",
    "                    term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                    term2 = cp.square(cp.norm(h, 'fro')*lambda_)\n",
    "                    term3 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                    obj = cp.Minimize(term1 + term2 + term3)\n",
    "\n",
    "                else:\n",
    "                    # Init the variables\n",
    "                    hI = cp.Variable((self.P, self.J))\n",
    "                    hS = cp.Variable((self.P, self.J))\n",
    "                    hH = cp.Variable((self.P, 1))\n",
    "                    hH.value, hS.value, hI.value = h_opt ##################### OCCHIO\n",
    "                    for i in range(0,self.P):\n",
    "                        tmp =  cp.Constant(np.zeros((self.M, self.M)))\n",
    "                        for j in range(0,self.J):\n",
    "                            tmp += ((cp.Constant(self.Luj[j, :, :])*hS[i,j]) + (cp.Constant(self.Ldj[j, :, :])*hI[i,j]))\n",
    "                        tmp += (I*hH[i])\n",
    "                        D_coll[i] = tmp\n",
    "                        Dsum += tmp\n",
    "                    D = cp.hstack([D_coll[i] for i in range(self.P)])\n",
    "        \n",
    "                    term1 = cp.square(cp.norm((Y - D @ X), 'fro'))\n",
    "                    term2 = cp.square(cp.norm(hI, 'fro')*lambda_)\n",
    "                    term3 = cp.square(cp.norm(hS, 'fro')*lambda_)\n",
    "                    term4 = cp.square(cp.norm(hH, 'fro')*lambda_)\n",
    "                    obj = cp.Minimize(term1 + term2 + term3 + term4)\n",
    "\n",
    "                # Define the constraints\n",
    "                constraints = [D_coll[i] >> 0 for i in range(self.P)] + \\\n",
    "                                [(cp.multiply(self.c, I) - D_coll[i]) >> 0 for i in range(self.P)] + \\\n",
    "                                [(Dsum - cp.multiply((self.c - self.epsilon), I)) >> 0, (cp.multiply((self.c + self.epsilon), I) - Dsum) >> 0]\n",
    "\n",
    "                prob = cp.Problem(obj, constraints)\n",
    "                prob.solve(solver=eval(f'cp.{solver}'), verbose=False)\n",
    "\n",
    "                # Dictionary Update\n",
    "                D = D.value\n",
    "                if self.dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                    h_opt = [h_opt[0] + step_h*(h.value - h_opt[0]),\n",
    "                             h_opt[1] + step_h*(hI.value - h_opt[1])]\n",
    "                else:\n",
    "                    h_opt = [h_opt[0] + step_h*(hH.value-h_opt[0]),\n",
    "                             h_opt[1] + step_h*(hS.value-h_opt[1]), \n",
    "                             h_opt[2] + step_h*(hI.value-h_opt[2])]\n",
    "\n",
    "                # OMP Step\n",
    "                X_te_tmp, X_tr_tmp = sparse_transform(D, self.K0, self.Y_test, self.Y_train)\n",
    "                # Sparse Representation Update\n",
    "                X_tr = X_tr + step_x*(X_tr_tmp - X_tr)\n",
    "                X_te = X_te + step_x*(X_te_tmp - X_te)\n",
    "\n",
    "                # Error Update\n",
    "                error_train = nmse(D, X_tr, self.Y_train, self.m_train)\n",
    "                error_test = nmse(D, X_te, self.Y_test, self.m_test)\n",
    "\n",
    "                hist.append(error_test)\n",
    "                \n",
    "                # Error Storing\n",
    "                if (error_train < self.min_error_train) and (abs(error_train) > np.finfo(float).eps) and (abs(error_train - self.min_error_train) > tol):\n",
    "                    self.X_opt_train = X_tr\n",
    "                    self.min_error_train = error_train\n",
    "\n",
    "                if (error_test < self.min_error_test) and (abs(error_test) > np.finfo(float).eps) and (abs(error_test - self.min_error_test) > tol):\n",
    "                    self.h_opt = h_opt\n",
    "                    self.D_opt = D\n",
    "                    self.X_opt_test = X_te\n",
    "                    self.min_error_test = error_test\n",
    "                    pat_iter = 0\n",
    "\n",
    "                    if verbose == 1:\n",
    "                        print(\"New Best Test Error:\", self.min_error_test)\n",
    "                else:\n",
    "                    pat_iter += 1\n",
    "\n",
    "                iter_ += 1\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # Fourier Dictionary Benchmark\n",
    "            _, self.D_opt = sla.eigh(self.L)\n",
    "            self.X_opt_test, self.X_opt_train = sparse_transform(self.D_opt, self.K0, self.Y_test, self.Y_train)\n",
    "\n",
    "            # Error Updating\n",
    "            self.min_error_train = nmse(self.D_opt, self.X_opt_train, self.Y_train, self.m_train)\n",
    "            self.min_error_test= nmse(self.D_opt, self.X_opt_test, self.Y_test, self.m_test)\n",
    "            \n",
    "        return self.min_error_test, self.min_error_train, hist\n",
    "    \n",
    "\n",
    "    def _aux_matrix_update(self, X):\n",
    "\n",
    "        I = [np.eye(self.M)]\n",
    "        if self.dictionary_type==\"separated\":\n",
    "            LLu = [lu for lu in self.Luj]\n",
    "            LLd = [ld for ld in self.Ldj]\n",
    "            LL = np.array(I+LLu+LLd)\n",
    "        else:\n",
    "            LL = np.array(I + [l for l in self.Lj])\n",
    "\n",
    "        P_aux = np.array([LL@X[(i*self.M): ((i+1)*self.M), :] for i in range(self.P)])\n",
    "        self.P_aux = rearrange(P_aux, 'b h w c -> (b h) w c')\n",
    "    \n",
    "    def topological_dictionary_learn_qp(self,\n",
    "                                        lambda_: float = 1e-3, \n",
    "                                        max_iter: int = 10, \n",
    "                                        patience: int = 10,\n",
    "                                        tol: float = 1e-7,\n",
    "                                        solver: str = 'GUROBI',\n",
    "                                        step_h: float = 1.,\n",
    "                                        step_x: float = 1.,\n",
    "                                        verbose: bool = False) -> Tuple[np.ndarray, np.ndarray, List[float]]:\n",
    "        \n",
    "        # Define hyperparameters\n",
    "        iter_, pat_iter = 1, 0\n",
    "        hist = []\n",
    "\n",
    "        # Learnable Dictionary -> ADMM optimization algorithm\n",
    "        if self.dict_is_learnable:\n",
    "        \n",
    "            # Init the the sparse representation\n",
    "            h_opt = np.hstack([h.flatten() for h in self.h_opt]).reshape(-1,1)\n",
    "            X_tr = self.X_opt_train\n",
    "            X_te = self.X_opt_test\n",
    "            reg = lambda_ * np.eye(self.P*(2*self.J+1))\n",
    "            I_s = cp.Constant(np.eye(self.P))\n",
    "            i_s = cp.Constant(np.ones((self.P,1)))\n",
    "            B = cp.Constant(self.B)\n",
    "\n",
    "            while pat_iter < patience and iter_ <= max_iter:\n",
    "\n",
    "                # Init variables and parameters\n",
    "                h = cp.Variable((self.P*(2*self.J+1), 1))\n",
    "                self._aux_matrix_update(X_tr)\n",
    "                h.value = h_opt\n",
    "\n",
    "                Q = cp.Constant(np.einsum('imn, lmn -> il', self.P_aux, self.P_aux) + reg)\n",
    "                l = cp.Constant(np.einsum('mn, imn -> i', self.Y_train, self.P_aux))\n",
    "\n",
    "                # Quadratic term\n",
    "                term2 = cp.quad_form(h, Q, assume_PSD = True)\n",
    "                # Linear term\n",
    "                term1 = l@h\n",
    "                term1 = cp.multiply(-2, term1)[0]\n",
    "                \n",
    "                obj = cp.Minimize(term2+term1)\n",
    "\n",
    "                # Define the constraints\n",
    "                cons1 = cp.kron(I_s, B)@h\n",
    "                cons2 = cp.kron(i_s.T, B)@h\n",
    "                constraints = [cons1 >= 0] + \\\n",
    "                                [cons1 <= self.c] + \\\n",
    "                                [cons2 >= (self.c - self.epsilon)] + \\\n",
    "                                [cons2 <= (self.c + self.epsilon)]\n",
    "\n",
    "                prob = cp.Problem(obj, constraints)\n",
    "                prob.solve(solver=eval(f'cp.{solver}'), verbose=False)\n",
    "\n",
    "                # Update the dictionary\n",
    "\n",
    "                if self.dictionary_type in [\"joint\", \"edge_laplacian\"]:\n",
    "                    h_list = _split_coeffs(h, self.P, self.J)\n",
    "                    D = generate_dictionary(h.value, self.P, self.Lj)                      \n",
    "                    h_opt = h_opt + step_h*(h.value - h_opt)\n",
    "                else:\n",
    "\n",
    "                    h_list = _split_coeffs(h, self.P, self.J, sep=True)\n",
    "                    D = generate_dictionary(h_list, self.P, self.Luj, self.Ldj)                                \n",
    "                    h_opt = h_opt + step_h*(h.value - h_opt)\n",
    "\n",
    "\n",
    "                # OMP Step\n",
    "                X_te_tmp, X_tr_tmp = sparse_transform(D, self.K0, self.Y_test, self.Y_train)\n",
    "                # Sparse Representation Update\n",
    "                X_tr = X_tr + step_x*(X_tr_tmp - X_tr)\n",
    "                X_te = X_te + step_x*(X_te_tmp - X_te)\n",
    "\n",
    "                # Error Update\n",
    "                error_train = nmse(D, X_tr, self.Y_train, self.m_train)\n",
    "                error_test = nmse(D, X_te, self.Y_test, self.m_test)\n",
    "\n",
    "                hist.append(error_test)\n",
    "                \n",
    "                # Error Storing\n",
    "                if (error_train < self.min_error_train) and (abs(error_train) > np.finfo(float).eps) and (abs(error_train - self.min_error_train) > tol):\n",
    "                    self.X_opt_train = X_tr\n",
    "                    self.min_error_train = error_train\n",
    "\n",
    "                if (error_test < self.min_error_test) and (abs(error_test) > np.finfo(float).eps) and (abs(error_test - self.min_error_test) > tol):\n",
    "                    self.h_opt = h_list if self.dictionary_type == 'separated' else h_opt\n",
    "                    self.D_opt = D\n",
    "                    self.X_opt_test = X_te\n",
    "                    self.min_error_test = error_test\n",
    "                    pat_iter = 0\n",
    "\n",
    "                    if verbose == 1:\n",
    "                        print(\"New Best Test Error:\", self.min_error_test)\n",
    "                else:\n",
    "                    pat_iter += 1\n",
    "\n",
    "                iter_ += 1\n",
    "        \n",
    "        # Analytic Dictionary -> directly go to OMP step\n",
    "        else:\n",
    "            \n",
    "            if self.dictionary_type == \"fourier\":\n",
    "                # Fourier Dictionary Benchmark\n",
    "                _, self.D_opt = sla.eigh(self.L)\n",
    "\n",
    "            elif self.dictionary_type == \"slepians\":\n",
    "                SS = SimplicianSlepians(self.B1, \n",
    "                                        self.B2, \n",
    "                                        self.S, \n",
    "                                        self.R, \n",
    "                                        top_K = self.top_K_slepians)\n",
    "                self.D_opt = SS.atoms_flat\n",
    "                \n",
    "\n",
    "            elif self.dictionary_type == \"wavelet\":\n",
    "                print(\"here\")\n",
    "                # w = np.linalg.eigvalsh(self.L)\n",
    "                w1 = np.linalg.eigvalsh(self.Lu)\n",
    "                w2 = np.linalg.eigvalsh(self.Ld)\n",
    "                SH = SeparateHodgelet(self.B1, \n",
    "                                      self.B2,\n",
    "                                      *log_wavelet_kernels_gen(3, 4, np.log(np.max(w1))),\n",
    "                                      *log_wavelet_kernels_gen(3, 4, np.log(np.max(w2))))\n",
    "                self.D_opt = SH.atoms_flat\n",
    "                print(self.D_opt.shape)\n",
    "            \n",
    "            # OMP\n",
    "            self.X_opt_test, self.X_opt_train = sparse_transform(self.D_opt, self.K0, self.Y_test, self.Y_train)\n",
    "            # Error Updating\n",
    "            self.min_error_train = nmse(self.D_opt, self.X_opt_train, self.Y_train, self.m_train)\n",
    "            self.min_error_test= nmse(self.D_opt, self.X_opt_test, self.Y_test, self.m_test)\n",
    "\n",
    "        return self.min_error_test, self.min_error_train, hist\n",
    "    \n",
    "    @save_results\n",
    "    def learn_upper_laplacian(self,\n",
    "                              Lu_new: np.ndarray = None,\n",
    "                              filter: np.ndarray = 1,\n",
    "                              h_prior: np.ndarray = None,\n",
    "                              lambda_: float = 1e-3, \n",
    "                              max_iter: int = 10, \n",
    "                              patience: int = 10,\n",
    "                              tol: float = 1e-7,\n",
    "                              step_h: float = 1.,\n",
    "                              step_x: float = 1.,\n",
    "                              mode: str = \"optimistic\",\n",
    "                              verbose: bool = False,\n",
    "                              warmup: int = 0,\n",
    "                              QP=False,\n",
    "                              cont=False):\n",
    "    \n",
    "        assert step_h<1 or step_h>0, \"You must provide a step-size between 0 and 1.\"\n",
    "        assert step_x<1 or step_x>0, \"You must provide a step-size between 0 and 1.\"\n",
    "        assert (mode==\"optimistic\") or (mode==\"pessimistic\"), f'{mode} is not a legal mode: \\\"optimistic\\\" or \\\"pessimistic\\\" are the only ones allowed.'\n",
    "        \n",
    "        # Check if we are executing the first recursive iteration\n",
    "        if np.all(Lu_new == None):\n",
    "            T = self.B2.shape[1]\n",
    "            if mode==\"optimistic\":\n",
    "                filter = np.ones(T)\n",
    "                self.warmup=0  \n",
    "            else:\n",
    "                filter = np.zeros(T)\n",
    "                self.update_Lu(np.zeros(self.Lu.shape)) # start with an \"empty\" upper Laplacian\n",
    "                self.warmup = warmup\n",
    "        else:\n",
    "            self.update_Lu(Lu_new)\n",
    "\n",
    "        self.init_dict(h_prior=h_prior,\n",
    "                       mode=\"only_X\")\n",
    "\n",
    "        if QP:\n",
    "            _, _, hist = self.topological_dictionary_learn_qp(lambda_=lambda_,\n",
    "                                                            max_iter=max_iter,\n",
    "                                                            patience=patience,\n",
    "                                                            tol=tol,\n",
    "                                                            step_h=step_h,\n",
    "                                                            step_x=step_x,\n",
    "                                                            solver='GUROBI')\n",
    "        else:\n",
    "            _, _, hist = self.topological_dictionary_learn(lambda_=lambda_,\n",
    "                                                            max_iter=max_iter,\n",
    "                                                            patience=patience,\n",
    "                                                            tol=tol,\n",
    "                                                            step_h=step_h,\n",
    "                                                            step_x=step_x)\n",
    "                        \n",
    "        self.history.append(hist)\n",
    "        search_space = np.where(filter == 1) if mode==\"optimistic\" else np.where(filter == 0)   \n",
    "        sigmas = pd.DataFrame({\"idx\": search_space[0]})\n",
    "\n",
    "        sigmas[\"sigma\"] = sigmas.idx.apply(lambda _: filter)\n",
    "        if mode==\"optimistic\":\n",
    "            sigmas[\"sigma\"] = sigmas.apply(lambda x: _indicator_matrix(x), axis=1)\n",
    "        else:\n",
    "            sigmas[\"sigma\"] = sigmas.apply(lambda x: _indicator_matrix_rev(x), axis=1)\n",
    "        sigmas[\"Luj\"] = sigmas.apply(lambda x: _compute_Luj(x, self.B2, self.J), axis=1)\n",
    "        sigmas[\"D\"] = sigmas.apply(lambda x: generate_dictionary(self.h_opt, self.P, x.Luj, self.Ldj), axis=1)\n",
    "        sigmas[\"X\"] = sigmas.D.apply(lambda x: sparse_transform(x, self.K0, self.Y_test))\n",
    "        sigmas[\"NMSE\"] = sigmas.apply(lambda x: nmse(x.D, x.X, self.Y_test, self.m_test), axis=1)\n",
    "        \n",
    "        if self.warmup>0:\n",
    "            candidate_error = sigmas.NMSE.min() - np.finfo(float).eps\n",
    "            self.warmup-=1\n",
    "        else:\n",
    "            candidate_error = sigmas.NMSE.min()\n",
    "        idx_min = sigmas.NMSE.idxmin()\n",
    "\n",
    "        \n",
    "        if candidate_error < self.min_error_test:\n",
    "            S = sigmas.sigma[idx_min]\n",
    "            Lu_new = self.B2 @ S @ self.B2.T\n",
    "            filter = np.diagonal(S)\n",
    "\n",
    "            if verbose:\n",
    "                if mode==\"optimistic\":\n",
    "                    print(f'Removing 1 triangle from topology... \\n ... New min test error: {candidate_error} !')\n",
    "                else:\n",
    "                    print(f'Adding 1 triangle to topology... \\n ... New min test error: {candidate_error} !')\n",
    "\n",
    "            return self.learn_upper_laplacian(h_prior=self.h_opt,\n",
    "                                              Lu_new=Lu_new,\n",
    "                                              filter=filter,\n",
    "                                              lambda_=lambda_,\n",
    "                                              max_iter=max_iter,\n",
    "                                              patience=patience,\n",
    "                                              tol=tol,\n",
    "                                              step_h=step_h,\n",
    "                                              step_x=step_x,\n",
    "                                              mode=mode,\n",
    "                                              verbose=verbose,\n",
    "                                              QP=QP,\n",
    "                                              cont=cont)\n",
    "        \n",
    "        # For the last recursions of \"pessimistic\" mode try some recursion of the \"optimistic\"\n",
    "        # to remove the warm-up randomly-added triangles\n",
    "        if mode == \"pessimistic\" and not cont:\n",
    "            return  self.learn_upper_laplacian(h_prior=self.h_opt,\n",
    "                                              Lu_new=Lu_new,\n",
    "                                              filter=filter,\n",
    "                                              lambda_=lambda_,\n",
    "                                              max_iter=max_iter,\n",
    "                                              patience=patience,\n",
    "                                              tol=tol,\n",
    "                                              step_h=step_h,\n",
    "                                              step_x=step_x,\n",
    "                                              mode=\"optimistic\",\n",
    "                                              verbose=verbose,\n",
    "                                              QP=QP,\n",
    "                                              cont=True)\n",
    "        \n",
    "        # Then after we added triangles and removed the randomly added ones, continue adding!\n",
    "        elif mode != \"pessimistic\" and cont:\n",
    "\n",
    "            print(\"Ce provo!\")\n",
    "            return  self.learn_upper_laplacian(h_prior=self.h_opt,\n",
    "                                              Lu_new=Lu_new,\n",
    "                                              filter=filter,\n",
    "                                              lambda_=lambda_,\n",
    "                                              max_iter=max_iter,\n",
    "                                              patience=patience,\n",
    "                                              tol=tol,\n",
    "                                              step_h=step_h,\n",
    "                                              step_x=step_x,\n",
    "                                              mode=\"pessimistic\",\n",
    "                                              verbose=verbose,\n",
    "                                              QP=QP,\n",
    "                                              cont=cont,\n",
    "                                              warmup=warmup)  \n",
    "                  \n",
    "        self.B2 = self.B2@np.diag(filter)\n",
    "        return self.min_error_test, self.history, self.Lu, self.B2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "topoX2 = TspSolver(X_train=X_train[:, :, s], \n",
    "                X_test=X_test[:, :, s], \n",
    "                Y_train=Y_train[:, :, s], \n",
    "                Y_test=Y_test[:, :, s],\n",
    "                **topo_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_x = 1.\n",
    "step_h = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n",
      "(100, 800)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nOrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m e1, e2, hist \u001b[38;5;241m=\u001b[39m \u001b[43mtopoX2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_dictionary_learn_qp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mstep_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mstep_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGUROBI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 722\u001b[0m, in \u001b[0;36mTspSolver.topological_dictionary_learn_qp\u001b[1;34m(self, lambda_, max_iter, patience, tol, solver, step_h, step_x, verbose)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_opt\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m    721\u001b[0m \u001b[38;5;66;03m# OMP\u001b[39;00m\n\u001b[1;32m--> 722\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_opt_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_opt_train \u001b[38;5;241m=\u001b[39m \u001b[43msparse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# Error Updating\u001b[39;00m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_error_train \u001b[38;5;241m=\u001b[39m nmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_opt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_opt_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_train)\n",
      "Cell \u001b[1;32mIn[80], line 55\u001b[0m, in \u001b[0;36msparse_transform\u001b[1;34m(D, K0, Y_te, Y_tr)\u001b[0m\n\u001b[0;32m     53\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m dd)\n\u001b[0;32m     54\u001b[0m Domp \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m---> 55\u001b[0m X_te \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_te\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Normalization\u001b[39;00m\n\u001b[0;32m     57\u001b[0m X_te \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m@\u001b[39m X_te\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind0], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[1;32mIn[80], line 55\u001b[0m, in \u001b[0;36msparse_transform.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     53\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m dd)\n\u001b[0;32m     54\u001b[0m Domp \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m---> 55\u001b[0m X_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_te)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Normalization\u001b[39;00m\n\u001b[0;32m     57\u001b[0m X_te \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m@\u001b[39m X_te\n",
      "File \u001b[1;32mc:\\Users\\engri\\Desktop\\tesi\\TSP-DictionaryLearning\\tsplearn\\tsp_generation.py:241\u001b[0m, in \u001b[0;36mget_omp_coeff\u001b[1;34m(K0, Domp, col)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mCompute the coefficients using Orthogonal Matching Pursuit.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    np.ndarray: Coefficients.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m omp \u001b[38;5;241m=\u001b[39m OrthogonalMatchingPursuit(n_nonzero_coefs\u001b[38;5;241m=\u001b[39mK0)\n\u001b[1;32m--> 241\u001b[0m \u001b[43momp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omp\u001b[38;5;241m.\u001b[39mcoef_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:742\u001b[0m, in \u001b[0;36mOrthogonalMatchingPursuit.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    738\u001b[0m _normalize \u001b[38;5;241m=\u001b[39m _deprecate_normalize(\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize, estimator_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    740\u001b[0m )\n\u001b[1;32m--> 742\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    745\u001b[0m X, y, X_offset, y_offset, X_scale, Gram, Xy \u001b[38;5;241m=\u001b[39m _pre_fit(\n\u001b[0;32m    746\u001b[0m     X, y, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecompute, _normalize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    747\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:554\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    555\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:1104\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[1;32m-> 1104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1122\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:919\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    915\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    916\u001b[0m         )\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 919\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    927\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nOrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "e1, e2, hist = topoX2.topological_dictionary_learn_qp(lambda_=lambda_,\n",
    "                                        max_iter=max_iter,\n",
    "                                        patience=patience,\n",
    "                                        tol=tol,\n",
    "                                        step_h=step_h,\n",
    "                                        step_x=step_x,\n",
    "                                        solver='GUROBI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020213589758395246"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.020213589758395246"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def simulate_top_learning(X_train, \n",
    "                            X_test,\n",
    "                            Y_train,\n",
    "                            Y_test, \n",
    "                            c_true, \n",
    "                            epsilon_true, \n",
    "                            n_sim, \n",
    "                            topo_params, \n",
    "                            K0_coll,\n",
    "                            max_iter,\n",
    "                            patience,\n",
    "                            tol,\n",
    "                            step_h: int = 1,\n",
    "                            step_x: int = 1,\n",
    "                            verbose: bool = True,\n",
    "                            include_sdp: bool = True):\n",
    "\n",
    "\n",
    "    min_error_qp_test = np.zeros((n_sim, len(K0_coll)))\n",
    "    min_error_qp_comp_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "    algo_errors = {\n",
    "        \"qp\": min_error_qp_test,\n",
    "        \"qp_comp\": min_error_qp_comp_test,\n",
    "        }\n",
    "    \n",
    "    algo_types = {\n",
    "        \"qp\": (\"QP\",True,False),\n",
    "        \"qp_comp\": (\"QP complete\",True,True)\n",
    "        }\n",
    "\n",
    "    if include_sdp:\n",
    "        min_error_sdp_test = np.zeros((n_sim, len(K0_coll)))\n",
    "        algo_errors[\"sdp\"] = min_error_sdp_test\n",
    "        algo_types[\"sdp\"] = (\"SDP\",False,False)\n",
    "        # min_error_sdp_comp_test = np.zeros((n_sim, len(K0_coll)))\n",
    "        # algo_errors[\"sdp_comp\"] = min_error_sdp_comp_test\n",
    "        # algo_types[\"sdp_comp\"] = (\"SDP complete\",False,True)\n",
    "\n",
    "    for sim in range(n_sim):\n",
    "\n",
    "        for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "\n",
    "            for a in algo_types.items():\n",
    "\n",
    "                model = TspSolver(X_train=X_train[:, :, sim], \n",
    "                                    X_test=X_test[:, :, sim], \n",
    "                                    Y_train=Y_train[:, :, sim], \n",
    "                                    Y_test=Y_test[:, :, sim],\n",
    "                                    c=c_true[sim],\n",
    "                                    epsilon=epsilon_true[sim],\n",
    "                                    K0=k0,\n",
    "                                    dictionary_type=\"separated\",\n",
    "                                    **topo_params)\n",
    "\n",
    "                try:\n",
    "                    # Complete learning\n",
    "                    if a[1][2]:\n",
    "                        algo_errors[a[0]][sim,k0_index],  _, _, _ = model.learn_upper_laplacian(lambda_=lambda_, \n",
    "                                                                                                max_iter=max_iter,\n",
    "                                                                                                patience=patience, \n",
    "                                                                                                tol=tol,\n",
    "                                                                                                verbose=False,\n",
    "                                                                                                step_h=step_h,\n",
    "                                                                                                step_x=step_x,\n",
    "                                                                                                QP=a[1][1])\n",
    "                    # Learn only the dictionary (no topology)\n",
    "                    else:\n",
    "                        if a[1][1]:\n",
    "                            algo_errors[a[0]][sim,k0_index], _, _ = model.topological_dictionary_learn(lambda_=lambda_, \n",
    "                                                                                                        max_iter=max_iter,\n",
    "                                                                                                        patience=patience, \n",
    "                                                                                                        tol=tol,\n",
    "                                                                                                        step_h=step_h,\n",
    "                                                                                                        step_x=step_x)\n",
    "                        else:\n",
    "                            algo_errors[a[0]][sim,k0_index], _, _ = model.topological_dictionary_learn_qp(lambda_=lambda_, \n",
    "                                                                                                        max_iter=max_iter,\n",
    "                                                                                                        patience=patience, \n",
    "                                                                                                        tol=tol,\n",
    "                                                                                                        step_h=step_h,\n",
    "                                                                                                        step_x=step_x)\n",
    "\n",
    "                    if verbose:\n",
    "                        print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} learning with {a[1][0]}... Done! Test Error: {algo_errors[a[0]][sim,k0_index]}\")\n",
    "                except:\n",
    "                    print(f'Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {a[1][0]}... Diverged!')\n",
    "                    # If diverged, simply interpolate\n",
    "                    try:\n",
    "                        algo_errors[a[0]][sim,k0_index] = algo_errors[a[0]][sim-1,k0_index]\n",
    "                    except:\n",
    "                        algo_errors[a[0]][sim,k0_index] = algo_errors[a[0]][sim-1,k0_index]\n",
    "\n",
    "    return algo_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slepians + Wave + Fourier + Edege Laplacian + Separated QP + Separated QP complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "topo_params2 = {\"J\":J,\n",
    "                \"P\":P,\n",
    "                \"true_prob_T\":prob_T,\n",
    "                \"sub_size\":100,\n",
    "                \"seed\":0,\n",
    "                \"n\":40,\n",
    "                \"p_edges\":0.162\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harmonic Component is Present!\n",
      "Saving Directory not Valid!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:02, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulation: 1/10 Sparsity: 5 learning with Slepians... Done! Test Error: 0.16753769339234387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nOrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     algo_errors[a[\u001b[38;5;241m0\u001b[39m]][sim,k0_index], _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopological_dictionary_learn_qp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mtol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mstep_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                                                                                \u001b[49m\u001b[43mstep_x\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstep_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimulation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_sim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Sparsity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk0\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m learning with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m... Done! Test Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo_errors[a[\u001b[38;5;241m0\u001b[39m]][sim,k0_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[57], line 719\u001b[0m, in \u001b[0;36mTspSolver.topological_dictionary_learn_qp\u001b[1;34m(self, lambda_, max_iter, patience, tol, solver, step_h, step_x, verbose)\u001b[0m\n\u001b[0;32m    716\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_opt \u001b[38;5;241m=\u001b[39m SH\u001b[38;5;241m.\u001b[39matoms_flat\n\u001b[0;32m    718\u001b[0m \u001b[38;5;66;03m# OMP\u001b[39;00m\n\u001b[1;32m--> 719\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_opt_test, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_opt_train \u001b[38;5;241m=\u001b[39m \u001b[43msparse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mD_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[38;5;66;03m# Error Updating\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_error_train \u001b[38;5;241m=\u001b[39m nmse(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mD_opt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_opt_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_train, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm_train)\n",
      "Cell \u001b[1;32mIn[57], line 55\u001b[0m, in \u001b[0;36msparse_transform\u001b[1;34m(D, K0, Y_te, Y_tr)\u001b[0m\n\u001b[0;32m     53\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m dd)\n\u001b[0;32m     54\u001b[0m Domp \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m---> 55\u001b[0m X_te \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY_te\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Normalization\u001b[39;00m\n\u001b[0;32m     57\u001b[0m X_te \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m@\u001b[39m X_te\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind0], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "Cell \u001b[1;32mIn[57], line 55\u001b[0m, in \u001b[0;36msparse_transform.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     53\u001b[0m W \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiag(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m dd)\n\u001b[0;32m     54\u001b[0m Domp \u001b[38;5;241m=\u001b[39m D \u001b[38;5;241m@\u001b[39m W\n\u001b[1;32m---> 55\u001b[0m X_te \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mget_omp_coeff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDomp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, arr\u001b[38;5;241m=\u001b[39mY_te)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Normalization\u001b[39;00m\n\u001b[0;32m     57\u001b[0m X_te \u001b[38;5;241m=\u001b[39m W \u001b[38;5;241m@\u001b[39m X_te\n",
      "File \u001b[1;32mc:\\Users\\engri\\Desktop\\tesi\\TSP-DictionaryLearning\\tsplearn\\tsp_generation.py:241\u001b[0m, in \u001b[0;36mget_omp_coeff\u001b[1;34m(K0, Domp, col)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mCompute the coefficients using Orthogonal Matching Pursuit.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03m    np.ndarray: Coefficients.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m omp \u001b[38;5;241m=\u001b[39m OrthogonalMatchingPursuit(n_nonzero_coefs\u001b[38;5;241m=\u001b[39mK0)\n\u001b[1;32m--> 241\u001b[0m \u001b[43momp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDomp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m omp\u001b[38;5;241m.\u001b[39mcoef_\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\linear_model\\_omp.py:742\u001b[0m, in \u001b[0;36mOrthogonalMatchingPursuit.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m    738\u001b[0m _normalize \u001b[38;5;241m=\u001b[39m _deprecate_normalize(\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize, estimator_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[0;32m    740\u001b[0m )\n\u001b[1;32m--> 742\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    743\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    745\u001b[0m X, y, X_offset, y_offset, X_scale, Gram, Xy \u001b[38;5;241m=\u001b[39m _pre_fit(\n\u001b[0;32m    746\u001b[0m     X, y, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecompute, _normalize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    747\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\base.py:554\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    552\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 554\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    555\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:1104\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1099\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1102\u001b[0m     )\n\u001b[1;32m-> 1104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1120\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1122\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:919\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    914\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    915\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m    916\u001b[0m         )\n\u001b[0;32m    918\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m--> 919\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    927\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\sklearn\\utils\\validation.py:161\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m     )\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nOrthogonalMatchingPursuit does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "min_error_slep_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_wave_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_fou_test = np.zeros((n_sim, len(K0_coll)))\n",
    "min_error_edge_test = np.zeros((n_sim, len(K0_coll)))\n",
    "\n",
    "algo_errors = {\n",
    "    \"slep\": min_error_slep_test,\n",
    "    \"wave\": min_error_wave_test,\n",
    "    \"fou\": min_error_fou_test\n",
    "    # \"edge\": min_error_edge_test\n",
    "    }\n",
    "\n",
    "algo_types = {\n",
    "    \"slep\": (\"slepians\", \"Slepians\", False),\n",
    "    \"wave\": (\"wavelet\", \"Slepians\", False),\n",
    "    \"fou\": (\"fourier\", \"Fourier\", False)\n",
    "    # \"edge\": (\"edge_laplacian\", \"Edge Laplacian\", False)\n",
    "    }\n",
    "\n",
    "for sim in range(n_sim):\n",
    "\n",
    "    for k0_index, k0 in tqdm(enumerate(K0_coll)):\n",
    "\n",
    "        for a in algo_types.items():\n",
    "\n",
    "            model = TspSolver(X_train=X_train[:, :, sim], \n",
    "                                X_test=X_test[:, :, sim], \n",
    "                                Y_train=Y_train[:, :, sim], \n",
    "                                Y_test=Y_test[:, :, sim],\n",
    "                                c=c_true[sim],\n",
    "                                epsilon=epsilon_true[sim],\n",
    "                                K0=k0,\n",
    "                                dictionary_type=a[1][0],\n",
    "                                **topo_params2)\n",
    "            \n",
    "            if a[1][2]:\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                algo_errors[a[0]][sim,k0_index], _, _ = model.topological_dictionary_learn_qp(lambda_=lambda_, \n",
    "                                                                                            max_iter=max_iter,\n",
    "                                                                                            patience=patience, \n",
    "                                                                                            tol=tol,\n",
    "                                                                                            step_h=step_h,\n",
    "                                                                                            step_x=step_x)\n",
    "\n",
    "            if verbose:\n",
    "                print(f\"Simulation: {sim+1}/{n_sim} Sparsity: {k0} learning with {a[1][1]}... Done! Test Error: {algo_errors[a[0]][sim,k0_index]}\")\n",
    "            \n",
    "            # except:\n",
    "            #     print(f'Simulation: {sim+1}/{n_sim} Sparsity: {k0} Testing {a[1][0]}... Diverged!')\n",
    "            #     # If diverged, simply interpolate\n",
    "            #     try:\n",
    "            #         algo_errors[a[0]][sim,k0_index] = algo_errors[a[0]][sim-1,k0_index]\n",
    "            #     except:\n",
    "            #         algo_errors[a[0]][sim,k0_index] = algo_errors[a[0]][sim-1,k0_index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
